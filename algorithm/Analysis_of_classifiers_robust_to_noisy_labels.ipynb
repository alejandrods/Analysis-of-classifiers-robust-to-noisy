{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuQkyncqbzQu"
   },
   "source": [
    "# Analysis of classifiers robust to noisy labels\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "**1.-** Load and Preprocessing Data<br>\n",
    "**2.-** Utils Functions<br>\n",
    "**3.-** Classifiers<br>\n",
    "**3.1.-** T-Revision<br>\n",
    "**3.1.1.-** First Training Stage<br>\n",
    "**3.1.2.-** Generate transition matrix - CIFAR10<br>\n",
    "**3.1.3.-** Validating the effectiveness of our estimator<br>\n",
    "**3.1.4.-** Second Training Stage<br>\n",
    "**3.2.-** Forward Method<br>\n",
    "**4.-** Experiments<br>\n",
    "**4.1.-** Classifier using Forward Method<br>\n",
    "**4.1.1-** Training Function<br>\n",
    "**4.1.2-** Run Experiment<br>\n",
    "**4.1.3-** Results<br>\n",
    "**4.2.-** Classifier using T-Revision Method<br>\n",
    "**4.2.1-** ResNet Architecture<br>\n",
    "**4.2.2-** Loss Function<br>\n",
    "**4.2.3-** Training Function<br>\n",
    "**4.2.4-** Run Experiment<br>\n",
    "**4.2.5-** Results<br>\n",
    "**4.3.-** Classifier using Importance Reweighting Method<br>\n",
    "**4.3.1-** ResNet Architecture<br>\n",
    "**4.3.2-** Loss Function<br>\n",
    "**4.3.3-** Training Function<br>\n",
    "**4.3.4-** Run Experiment<br>\n",
    "**4.3.5-** Results<br>\n",
    "**4.4.-** Baseline Results<br>\n",
    "**4.4.1-** Generate Baseline Results<br>\n",
    "**4.4.2-** Combining Results<br>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We explore contemporary robust classification algorithms for overcoming class-dependant  labelling  noise:   Forward,  Importance  Re-weighting  and  T-revision.The classifiers was trained and evaluated on class-conditional random label noisedata while the test data is clean.  We also estimated the transition matrix in orderto obtain more insight on the classifiers’ performance on noisy data.  The effec-tiveness and robustness of the classifiers were also analysed with experiments aswell as multiple evaluation metric comparisons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7OIqeNmkkDJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNgkJ3SUIXfy"
   },
   "source": [
    "## 1.- Load and Preprocessing Data\n",
    "\n",
    "###FashionMINIST0.5.npz\n",
    "Number of the training and validation examples n = 18000\n",
    "\n",
    "Number of the test examples m = 3000\n",
    "\n",
    "The shape of each example image shape = (28 × 28)\n",
    "\n",
    "The transition matrix:\n",
    "\n",
    "$$ T=\\begin{bmatrix}\n",
    "0.5 & 0.2 & 0.3\\\\\n",
    "0.3 & 0.5 & 0.2\\\\\n",
    "0.2 & 0.3 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "###FashionMINIST0.6.npz\n",
    "Number of the training and validation examples n = 18000.\n",
    "\n",
    "Number of the test examples m = 3000.\n",
    "\n",
    "The shape of each example image shape = (28 × 28).\n",
    "\n",
    "The transition matrix:\n",
    "\n",
    "$$ T=\\begin{bmatrix}\n",
    "0.4 & 0.3 & 0.3\\\\\n",
    "0.3 & 0.4 & 0.3\\\\\n",
    "0.3 & 0.3 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "###CIFAR.npz\n",
    "Number of the training and validation examples n = 30000.\n",
    "\n",
    "Number of the test examples m = 3000.\n",
    "\n",
    "The shape of each example image shape = (32 × 32 × 3).\n",
    "\n",
    "The transition matrix T is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljmCVwuIM5PA"
   },
   "source": [
    "\n",
    "**Note:** In this section you need to add the path to the file that contains the datasets. Additionally, if you use Google Colab you just need to mount your Drive Folder using the lines above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpR8HGnOIwIR"
   },
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "  \"\"\"\n",
    "  Function to load numpy datasets\n",
    "\n",
    "  :param file: Dir\n",
    "  :return X_train: Training images with shape (num_images, channels, width, height)\n",
    "  :return y_train: Training labels with shape (num_labels)\n",
    "  :return X_test: Testing images with shape (num_images, channels, width, height)\n",
    "  :return y_test: Testing labels with shape (num_labels)\n",
    "  \"\"\" \n",
    "\n",
    "  dataset = np.load(file)\n",
    "  X_train = dataset['Xtr']/255\n",
    "  y_train = dataset['Str']\n",
    "  X_test = dataset['Xts']/255\n",
    "  y_test = dataset['Yts']\n",
    "\n",
    "  if X_train.ndim == 4:\n",
    "    X_train = np.rollaxis(X_train, 3, 1)\n",
    "    X_test = np.rollaxis(X_test, 3, 1)\n",
    "\n",
    "  elif X_train.ndim == 3:\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_train = np.rollaxis(X_train, 3, 1)\n",
    "    X_test = np.rollaxis(X_test, 3, 1)\n",
    "\n",
    "  return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ99j0gK7BW6"
   },
   "source": [
    "As we are running the Notebook using Google Colab, we need to mount our drive folder to read the data. If you run the code in your local machine, this line is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2596,
     "status": "ok",
     "timestamp": 1605607252941,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "0Ckm23tFIoBk",
    "outputId": "6522d393-c47b-4682-cadb-d5c1174ec54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKCXwB7pIyqm"
   },
   "outputs": [],
   "source": [
    "# Reading data - You need to add the path to the folder with the datasets in the next cell.\n",
    "dataset = \"cifar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14226,
     "status": "ok",
     "timestamp": 1605607264587,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "Nje0y6Ul_uTR",
    "outputId": "a48fd81a-f753-41b3-f9c3-dd4fff1dc9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cifar loaded.\n",
      "\n",
      "Shape Xtr: (15000, 3, 32, 32)\n",
      "Shape Str: (15000,)\n",
      "Shape Xts: (3000, 3, 32, 32)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "T_matrices = {'cifar': None,\n",
    "              'fashionmnist0.5': torch.Tensor([[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]]).cuda(),\n",
    "              'fashionmnist0.6': torch.Tensor([[0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4]]).cuda()}\n",
    "\n",
    "if dataset.lower() in available_datasets:\n",
    "  dir = available_datasets[dataset.lower()]\n",
    "  T_matrix = T_matrices[dataset.lower()]\n",
    "  X_train, y_train, X_test, y_test = load_data(dir)\n",
    "  print(f\"Dataset {dataset} loaded.\\n\")\n",
    "\n",
    "  print(f\"Shape Xtr: {X_train.shape}\")\n",
    "  print(f\"Shape Str: {y_train.shape}\")\n",
    "  print(f\"Shape Xts: {X_test.shape}\")\n",
    "  print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "  print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "else:\n",
    "  print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-ia7ruAHf11"
   },
   "source": [
    "## 2.- Utils Functions\n",
    "\n",
    "In this section, you will find utility functions that will be used in the implementation of the classifiers & transition matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu5N2fyH__Pd"
   },
   "source": [
    "### 2.1.- Display Images\n",
    "\n",
    "This section contains functions to display images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAhFFHAr0QWj"
   },
   "outputs": [],
   "source": [
    "def transform_img(img):\n",
    "  \"\"\"\n",
    "  Function to transform the image shape to display it\n",
    "\n",
    "  :param img: Image\n",
    "  :return Reshaped image\n",
    "  \"\"\"\n",
    "  return np.squeeze(np.transpose(img, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 14210,
     "status": "ok",
     "timestamp": 1605607264589,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "vXiB39pZ1B1E",
    "outputId": "6c31cbc7-4183-4d8a-ba67-1174121e8419"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA78AAAFICAYAAACY6EkSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAASdAAAEnQB3mYfeAAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dW4wt6Xne96dqnVefe5/PcyBnqCFFSpRMUqREiqSlOIADy4qE3EQJkOTCSGIhRk5AnMBRLgJfWU6kxA5iJA5sBzACQRFl0RYVS2IkkhLJITnkkHOe2bOPvXv37vM6r1WViz0jkJTX+3w705zNLv5/ADHSvNVv1ar66qv6VpP9ZGVZCgAAAACAKssf9gEAAAAAAPC9xuIXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+8T2RZdkvZFn2a1mW/VGWZftZlpVZlv3jh31cAPCwZVl2Isuy/yDLst/MsuzlLMsGWZbtZVn2x1mW/ftZlvFsBvADLcuyi1mW/W9Zlt3KsmyUZdnVLMv+bpZlaw/72HC8ZWVZPuxjQAVlWfY1Se+TdCjphqR3SfonZVn+2w/1wADgIcuy7K9J+nuSbkv6A0nXJJ2R9POSViT9hqRfLHlAA/gBlGXZ45I+L+m0pN+S9LykD0j6uKQXJH2kLMt7D+8IcZyx+MX3RJZlH9f9Re/Lkj6m+y94LH4B/MDLsuwTkhYk/U5ZlsW3/fuzkr4o6ZKkXyjL8jce0iECwEOTZdnvSvpZSb9cluWvfdu//zuS/oak/6Usy7/2sI4Pxxv/1Sp8T5Rl+QdlWb7Eby4A4DuVZfn7ZVn+9rcvfN/49xuS/v4b/+9Pv+0HBgAP2Ru/9f1ZSVcl/U/fVf5bknqSfinLsoW3+dBQESx+AQD4/jF545/Th3oUAPBwfPyNf37mX/EF4YGkz0nqSvrQ231gqAYWvwAAfB/Isqwu6d954//9Fw/zWADgIXnyjX++OKf+0hv/fOJtOBZUEItfAAC+P/xtSe+R9OmyLH/3YR8MADwEK2/8c29O/c1/v/o2HAsqiMUvAAAPWZZlvyzpP9X9v2r6Sw/5cAAAqCQWvwAAPERZlv3Hkv4HSd+S9PGyLLcf8iEBwMPy5m92V+bU3/z3u2/DsaCCWPwCAPCQZFn2n0j6NUnP6v7Cd+MhHxIAPEwvvPHPef+b3ne+8c95/5tgIMTiFwCAhyDLsv9S0q9K+pruL3w3H/IhAcDD9gdv/PNnsyz7jnVKlmVLkj4iqS/pT97uA0M1sPgFAOBtlmXZf6P7f+DqaUmfLMty6yEfEgA8dGVZviLpM5IekfQffVf5VyQtSPpHZVn23uZDQ0VkZVk+7GNABWVZ9nOSfu6N//espH9N0quS/uiNf7dVluV/9jCODQAepizL/l1J/1DSTPf/K8//qr9qerUsy3/4Nh4WAHxfyLLscUmfl3Ra0m9Jek7SB3U/A/hFSR8uy/LewztCHGcsfvE9kWXZfyvpbwWbvF6W5SNvz9EAwPePhPlRkj5bluVPf++PBgC+/2RZdknSfyfpL0k6Iem2pN+U9CtlWe48zGPD8cbiFwAAAABQefxvfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQefWjaJJl2Yqkj0m6Lml8FD0B/EBpSrok6bNlWe497IM5SsyPAI5AJedI5kcAR+CB5scjWfzq/sT1W0fUC8APrr8i6VMP+yCOGPMjgKNStTmS+RHAUUmaH49q8Xtdkv7zv/m3dfb8xbkbFWURNmk3mnZHjVYrrBe57zFTFtZrqtke+SyuN+KP+obSb1GL/5vp0yyhh6lnM99DZTxUZlPfo8gTTkp8aZKUZXwsrp5yHEWR8HlNk4SjSDpWt81sZgZrynEkbDMzW0XHuXn7mn79v/9l6Y25pGKuS9Jf/5mf0ZmV5bkbDQeTsEnNzAWSlJ07Fdb32/H8KUlPLDbC+saLz9sef/hMvM3+eGp71HI/GWRZvE296T/vyvpqWF9o+fN+5dx6WP8LP/oe22M29edk56Af1usL88fXm169djus/+mXvmZ7yIzFZsOfs8V6/Exp1Py8NUk4Z7OpGUfmvUSSGua9Yqj43pWk/VE8P+bmo/QGQ335hZek6s2R1yXpF3/xr2h9fW3uRodbd8Mmo4H/pXG92Yk3SJhzLl2+FNYvXrpse8g8szfvbNgWrzz7rN3m5vV4qBQJ/8PHzNynzXbb9lhaWAzri4txPXWb5ZWVuMdSXJekTjeeQ7vms0hSuxufk2bbjENJrVa8Ta3hn23uHfT+NrHyKP7HsQnvy6XZJjP35vXr1/Rf/I2/LiXOj0e1+B1L0tnzF3XpymNzNyqK+DR3zMJW8jdaUfM9puZq1hMWv+65nLT4TVnY1ONjnRzB4jdPWvzGL8TTie8xc98YSJVa/LovWY5q8evuq7dr8Tu15z3pG6Eq/tfexpJ0ZmVZF9bnL5IGvfij18xcIEnZqXjx2+36h+4jK/GLfr4VL54kackssmeZ/yxJi32z+G20/IvZinmpWu744zh7In6pevzSOdtjOvELqK29w7DeWJq/eHjTYBiPs6WFru0hMxZbCYvflUb8TGnW/bw1Tjhn08lbX/y2zHtFv/TTVlk3i1//Ud5UtTlyLEnr62s6ffrk3I1aRXyChr2h3VG9tRBvkCfc62fOhPXLl+b/AujPmGdlLeG9buf6NbvN/r14vp8lLGzyRrxMaHX8fLG6tBTW3aI1dZu14MsTSVpZib+klKTuYtxjKWUBvRifk3bXn7N2Jx6rNfdFjr6PFr8Jawz3HpsnvA+8IWl+5A9eAQAAAAAqj8UvAAAAAKDyWPwCAAAAACqPxS8AAAAAoPJY/AIAAAAAKu+o/tqzJKnI7v9n7s5a8V93HBf+rzv29g7CemPB/3WzWsP8lbQy5S+kxdukRBDNhv7POw73BmG9mRBdMjN/z+1wEP8FUUnKs3g/iwv+L+CV9u/KSYX5y8Tur7ve34+pJ/wFZXf5jiTqKOFPKLu/gHe/z1uPOnLntUj4e8+FOY7os7jrXgX7mzfVHczPXq/P4mvdMH8tVpJulqOw/pKJU5Kk9/7Q/L/YL0nFON6HJJ05Of+vtkpSJ+E4Uv7GuBu3/ZE/1r3tnbB+mPmxORrG8/T73v9B22PS93+tdutefKxnEiI0ivF+WO+0Uua2eKyeXvJxIO957B1h/e7mTdtjMIjfByTp8NA83/L4vUSSWvU4h+j8Wf/8mzRPh/WXv3U1/vky/c9BH0cr66e1durs3PqpE+avLF+8Yvexth7PS+PMj4WsHv81/JT3i6GZL548+4jt8fi73mu3efXFF8P63s627bG7HW9z7fXXbI/r1+Jt6gkpH52mvzazcRwF16j7JJd2O/5rz/WEBIH2UvyXmjsJ8+PqiTi5YXX9vO2xsur/+v9iEL8oSUumLkmdxfivedda/q9b10ykVr0WX7u89mBRMfzmFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeUea89sb9LXfm5+pN5nEOXVbd+/Zfdy4uRnWa+04X0uSFpfi7KtW7rNzXRTweOoz+YpJnB0oSf2DOKOw0/DHqjzOZDwY+6zE8Tj+wI89+k7b4x2P+xy+TjvOUEvJvbXbJMSBlWajIiHH2cWUpuQBpmxzFFxeap5w0lz25w+6G6OmBsH92g8ygCWpmfkcWM3ivNE8izMqJWnr9Tth/elbN2yP5zfjPNpy5Oe+lEzvtpkvJtOE/Og8/g643fFz7O4gHvtf/MZLtse5Ez4rdjR158TPFy3z1G80EiZIc6s/+fjjtsUjl+PnweqSz4bcuH3VblNM4vtmce2c7TFrxPnJ3ZbJEpZ0/mSc7Xm9Fn/epnmOH3dXHnlMVx6ZPyZeeiG+h7b2/HtMdym+x1odnyU7HMbXutn0c2wxjnN+e6M4r1aSTp324/YnLjwS1m9eu2p79Pd243185Cdtj9t34szuZsJ77KrJkpWkZ7/+pbD+2X/5adtjtvlqWM9zPz+W5tlVa/kx4sZRrfDH0UgYi/VWfO67Cz47fsVkcC+tX7Q91tbWw/qJEyfC+s3bt+w+vh2/+QUAAAAAVB6LXwAAAABA5bH4BQAAAABUHotfAAAAAEDlsfgFAAAAAFQei18AAAAAQOWx+AUAAAAAVB6LXwAAAABA5Zm4+wfz1Wee0dWbt+fWD3txOHguHzA+GJVhfTi7Z3s0mvE2tcJ/JzAz+dLDcprQI/4skrTQbIf1TuYvYbtVi48jH9sevd4krH/561+1PTa3fAj1Y48+GtZPnjxpe3S63bBeFv68z2azsF6Uhe2RuXFU+uN4u5RF/HlcaLsklebzFME+iu+jc/G9MqplGtTmn8ftPB5z2Wxk93GiHs8Hi8trtsewtxfWdw/8cewP4/miNJ9V8vegJNXMfuop3+9O4rHXG/vPu2jG7xef+brt8cQ73mG3edfjl8N6vRnPfZL0yCOPh/Ve4Z/Dd27fDev7BwPbQ+2FsPzjH32vbfG1L33WbjOYxs/ig4k/Z/d68X2zPhjaHhdqB2F9eBjPsaO+3cWxtrzQ1drS0tz6Y+94Z/jzN66/bvexvX0nPoalFduj1e6E9WYt5b0unpcGQ/9OVrqXUElm6GtlxT8PxqP4Xp7O/LFeejyeczrtVdtjseu3OXkpfn/sJ7xnfOY3/2lYr019j2YtnkMbhT9nxSDeJp/Fzz5JGuZ+jBTm3e6u/Ltu+fJL8QY1P8fW8nid0mq1wvpBwhz87fjNLwAAAACg8lj8AgAAAAAqj8UvAAAAAKDyWPwCAAAAACqPxS8AAAAAoPJY/AIAAAAAKo/FLwAAAACg8o405/egP1R2OD8TrCzjPKlMPj+r3ozzs7oJube1PN6mqabtMVScQTlN+F7hoN+z2wx68TatLM7GkqTFMs7HqiWMgkYrzrYbHvqMrVeu37TbvH57I6yvLvscvksXL4b1UydP2B6ra3H+Xd1kkklSzWQBu1zcVDPTptBbz+gtE3KNXVZvEeQrR7WqaGR7amXzs/nOdeNQxtWEHPT1tfg+fa2Ms0YlaaETX+tWQj65m4cnC/GcJEmTaUKO4SjO4J0lzMMuF7zZ8uf97KVzYf38xUu2x1bCHLqxH2dufvCDH7A9tu/Ec+zP/5sfsT0+/c9+N6x/4fN/Yntcfs/7w/on3vtjtscrN1+127z2uS+F9b3x/GzZNx1O43vih/5C/FkkaTDZCesnT7bDet6Mf/64e/WlFzTqzZ+flk+cDn++U/f3+s69zbA+MNmqknT67IV4g4QM80kZH+s4IUs2S3hm5mabRsO//K2tLYf1z33uD2yPpU483z/1bj9vjRKyYsfm1C+fOmt7TOrxM3Rnx9+H3Xo8X3RNDrAkterxtcnq/hma8lblhpFZtt3fxr0fjv17h3sHPejH9cHYhFp/F37zCwAAAACoPBa/AAAAAIDKY/ELAAAAAKg8Fr8AAAAAgMpj8QsAAAAAqDwWvwAAAACAymPxCwAAAACoPBa/AAAAAIDK8wnXD2A0KTUYzw879oHaPk25nE3iuuK6JGW1OAk7S0iGHk+GYX2ScGaXuot2m4P9fljfHw9sj1ERB1A3m03bY6kZn5RazffoTUd2m1oRfx8z2tqzPXZ3D8P6wmIcYi5J586dD+uPP/qY7bHYjEPIWwnnfTLx43li8sVL1WyPwoSUuwDy+9vE9VlQL0p/jMddo1NTc2H+xPDY0unw5x8t/aSy0mzHG+zdsD26q/G47TXjOUmSikY8x/74j7zf9jhzOj4fkvTqyy+H9evXbtoeea0R1stpPNdLUjuPP+9PfNB/3rv+tOqLn/3DsP7CC5dtj9nA7GhhzfbY7cVz+eHEf6/+8u17Yb1X+DmhN/X72dyNj3XU9s/hd16J5/vVM/HzQpLu3os/7yc+8e6wfv32Tf3T3/8Nu5/jam9/R53u/Hvx2a/9afjzjal5EEo6++iVsD5O6NFdXIjr3XO2R2l+75RwGOoPDuw2ubmFJmP/Tvb8M0+H9a/84Wdsj4WF+JydO+XP2ZlL/r2tadYYP/zU+2yP+i/9h2H95vXXbY+93a2wfrC/bXsc7u+G9V6vZ3sMBn594N4xS/l3vyyLx3OznnLt4udwt9sN6/lgKN2Mz9l3bJ+8JQAAAAAAxxSLXwAAAABA5bH4BQAAAABUHotfAAAAAEDlsfgFAAAAAFQei18AAAAAQOWx+AUAAAAAVN6R5vwOJyPVRvMzEUcm+y/LfM5vux3nWCZE9Ko0uykSgn7dNr1enDUrSe2O/7ytRhzUNpv4HsNRnPU1zXyoXGk+b9MFykmJX7XE+6nX/X7csR70/bXZe+m5sL51L85xk6Sl9kpYv3jhou2xtuYzN5stl6Hmx0gxnYb1lNzBqbnAs3J+Fuqo9JnHx11/UtfheH6W3UotzkKcbO3YfVzfjXNtf/J977I9BuM4P/BCwlhod+N78EOr8WeVpKdOnbTb9It4P1utOLNYkvp78XmdjW0L1cdx5uaVa6/ZHp3d+B6UpPVTq2F98uxXbQ+Xa/yFb8VznyS9cOtWWB8mZLrfvBZnTm/eu2t7fOBHP2S3ubJ6Kaz/j//n/217jAcbYf3pL/nnwZ07r4T1938yvjd3Rpt2H8fZ4tKillfmPzNfM8/trY07dh+DIs40XTrps8Xde2rHvKNK0olTcS50vR7fo5I0cnndkjqd+Ln60ov+Xv/CH/9RWM9ncca5JO1uxffHrRvXbY/W0gm7TbMbZ3avrvj3qZ/66U+E9Tz371ODYfwM7fd9RnPvYC+s37nh84avvuafOy+9/HJYdxnNknTxYjzHnjhxxvbodOL32PX19bB+49YtffGbv2L38yZ+8wsAAAAAqDwWvwAAAACAymPxCwAAAACoPBa/AAAAAIDKY/ELAAAAAKg8Fr8AAAAAgMpj8QsAAAAAqDwWvwAAAACAyqsfZbNJWWhcFnPr2Wx+TZKKIq5LUpEQMG214h5lzX8nUOTTsF5POLOT8cBu06zHgemLJsRckvrjYVifKv4skjQqTX1qNpDUyv1JqakW1suE72smRfx5pvKh7Hke72dje9P2uDW6F9Zffv2a7XHq1Em7zfnzccD44uKS7dFuxeOszOPrIkmTMj5ns9n8894bjWz/4249b+lUbf55vmDG/vKyv45f27kR1ndGe7bHlbPnwvovbD5qezT2e2H9xEvxcUpS65XbdptZMQnrjyQ8LhqzeKPczMGSNMsaYX30xa/YHitTfw8UJxfi45j6Z6j24/lvubZoW4x68fVd99OFumX8/NvfeN32uPBDT9htlhbi6/eBxy/YHpt747C+cdi3Pfr97bD+6ksvhfVb27t2H8darSnVW3PLq2vr4Y/fefWq3UV7EF+n/Rv+mXznzp2w/vRX/L3+1FPvC+vdhWXbYzyK3+skyb0uf/0rX7Q99vbjcTed+vepwrz7p7zVl6V/x5yM4+fBYRnPW5LU7cb1VqNje3TM9VtZO217tJvxM6WZx3VJ2t/z89InPvF4WD9z5oztsbgUf95625xU+fVfux3P42Pz/vnd+M0vAAAAAKDyWPwCAAAAACqPxS8AAAAAoPJY/AIAAAAAKo/FLwAAAACg8lj8AgAAAAAqj8UvAAAAAKDyjjTnd1aWmgY5v/bnTT6rJA0PD8J6PSFg18Q6qp7HmX6SVJoejYZPLqunnH6XfZz57LNFkxc2TfgKpDDbTBIymqczf17zLN5RmZBjOTM5vrOaP2cuCjghck6Zyf6cTvxn2b+1Y7d5/fbVsN5q+pzSrgm3cxlr9/cTZ043GvPPx86mz3097h5daOvi0vzzvHBvK/z5Wu7HyxMXL4b1gzt3bQ83uV1ImHO6zbhHzWSeSlJW+P24GWVk8rolSc352aKS1Ei42etmXmrkcf6kJE2WfDhu2Y+zgKcukF3SzKRqnkl4/n2iE+cNjzOfPz87H+dHtq9etT36fjeSycd+97veYVuc68fn5NzEv7s88fj5sP6Ok3G+cv3WLek3Pm33c1yNZqWGwX3UNFmhtYR3v+kkvo5l3d+DG7c2w/orr123Pb7whT8J63nNZ7jWa/7znlpfjTeY+KzguplCD/bjd3JJOrEUj+1my9/IWcJcPiviF7di7DOJG434WFZW12wPl2s8HPrz/uILz4X1z/3h79seV6++arc5fz7OOd/auWd7lOaZUm/HzwtJqgfvh5I0ncTP0M3N+L78bvzmFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACV51OyH8B4OlEehIhnWRyEXBSl3UdZxttMRwPbYzDqh/VG0weM17L4e4NW3fcoszgIW5KyMg5dLwrfo3TB3/60qz+bhvWx/HHkuQ+QH5sx0ijjuiSVeXwsk9wHnZthprzmP4uyOMg8IbNdCZdGRRE3Gg8ObY/9njkns/n39Z8ZxfuJ7v/Bwa7vf8ztbd5Qtz8/MH40jcf2oObvsf7KYljv9OOgeEkaPvdKWJ/V/P0zXYgfLXnNj6fWNGF+VDs+joT5Ymbm0LKRMJe/xbok1U8/ZrdZ2o3v9WF8OiRJ4ytrYX1t6ueLhWF8/aa78fNCkg4398J6/9bnbI/bX37GbrP87ifC+r2Nu7bHuLse1qf+tUP9ezthfb8Rn9PDw/h8HXfLa+taPXl6bv3OS8+FP19PeCYPB/G7n5r+lbhRj+eUTsv3OOyPwvp04ufpot602+zvboX12bBne6ysrob1ccIL5HAUf97DQz/n1GsJ53UY72d5adn2KCbx82Br447t0esdhPUXXozHsiR9+Ut/GtZfffUFfxwJ5/W11+PnfaPhz3thnrN5zY/Vmrl/p9P4mTIcxu/bf+6YHmhrAAAAAACOIRa/AAAAAIDKY/ELAAAAAKg8Fr8AAAAAgMpj8QsAAAAAqDwWvwAAAACAymPxCwAAAACoPBa/AAAAAIDK8+nFD2A4GqkIgobruVlrFwmHU8QB1IOeD6BuNuNQ7vUzF22Pziyu57M4kFmSah0f/Fzmcdj53s4922NwuB/Wrzz6pO1xMFkI6zs7e7ZHq9W120wm47CeyZx4SUVpQtf9pbE9Zj7XXU3F1y6v+QOZTuLwcEmaFea+yvx3XOUoDrsvdq/bHvduvmp2Mv84JuP4ulfBbv9ATY3m1q/34pD2aeHHSzM7G9a7aydtj3uDg7B+ttayPTrDeMzN9uN7Q5JGY7+NTsafZ+GJd9gWw+lhWD/ciudPSWoV8X1aG82/7m8a3Y3P+/0drYXlbHXRtqhn8eRV7MfjUJI6734s3qDpj6O7OQjrvZs3bY/d51+22xTX4neCpfUl22N7NX7vuLcRjyFJur15I6w/2jwX1nu723Yfx9nZs+d16dIjc+svfunz4c/f2/PvIIOd+D68+Mhl2yPP4ns9d++5kkwLlWU83iSpKBPeH8bx+9JCp2177B/E89JBz89tHXNOnv7KV2yPq5v++i6txPPjQjd+j5WkZtYI6y+++LztsbN7N6xfvfpSQo/43X5W+nfhskh4UTVjcTZL2Y+r+/fY0rxzu/tqMvXH+R39HmhrAAAAAACOIRa/AAAAAIDKY/ELAAAAAKg8Fr8AAAAAgMpj8QsAAAAAqDwWvwAAAACAymPxCwAAAACovCPN+Z3NZppNg+wxEzm11urYfSwvxFmxg27CR8riTNHGYZw/KEntafy9wenTp22PYULG2ngaZ1122j47t9aNz2t3edn2WF2IMwjPnvRZb4XJaJakocn66if02Lgb5zpOeru2R6OMz3t96rMwa0U8ziYTn+tZr/nrWygeR0WecE+YbNf9W1dti9FOfN4PD+ePEZvNXAH745Ea+fzxu9GPs0In+3EWsySdPHMqrJeX/LzUWotzT1v7Pl+yfivOORwf9m2PQ/l7fbYYz22NKz63s56ZLMxVf6yTF6/F9YTM4qHJdJekpY8+Fdb7u1u2h14wOZXm2SZJuh3vZ1QkzLFnz4f1sx/7kO3R6tTsNtsvvhLWV/u+x8qVONv62kY890lSpxbPcY1GM67X4/zR466dt9StzX+WnQsygCVp0vH549NRfI+Nxv45tGtysCelzzRtmHe/bJYw9w39O9c0j8dMmZDZXm/FPeojn686KuM55dmXfO7tvae/ZrfpduJ88WbdvwuV5voNBv55UJgM3pQc51rN3e9+3lLux7PN160lvD/WbHD1Wz4OF0icJ9x337H9A20NAAAAAMAxxOIXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5CenFD2A6kabjueWV7lL446tdfzg3b18L64OmD+0ezaZhPdt43fZ49MTpsH760gXb4/lbt+w2ZREHN3d7A9tjZSEOVP/G9Wdsj8WzvbhugtAl6bUXv2W3mS2shfXVd77X9lg8/46w3nv9Odujdrgf1pfLQ9ujf7gb1w82bY9mIw5tl6T9YRx23lk9ZXuc6MTj7FAT28NkkCvL53/XlhWlpDgY/rg7e/aMzi5159bz126GP9/xt7pm4zgovpX5+3SnF4/9z1+/YXucHx6E9XfJf5jR2I+5wc34nI2/4uecgeJzll3wc/nwibNhvT+df93f9N7Hn7Lb9PJ4Phjcump7NPeGYX263LQ9xtfi5/DkTvy8kKTG6Xj+65+Jn7GS1FhfsdusffL9YX33+m3bY/VkPMe+f/GK7fF7f7wT1ltmnm4MC7uP42zUG2p40J9bv3D+Uvjzi6vrdh+DO/G8s72zZ3v0+qOwPp3G75eSpDx+WBYzP/cV5j1WksZmbtvZj+d6SWo242dGZj6LJA1G89cFknQ4iuckSRpN4vMuSdNp/A5RS/h9X/kW3mPelGfm+hb+Xi7iS6c8M93NBb8AACAASURBVBskms2OYl5568dSluY5bK5L8YDHwG9+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVd6Q5v3kxVR7kk51djDMK7+z43NPJUhz2VF+Ks4QlKc/izL7pJM7jk6Qr7393WN+Rz84ar/nsx1oWX6J8Oc7wlaTd/Thz82DoMzeLfpxZOxr6zLmVhGO9fhjn5/bu3rM9rqyuhvXzT/qs4N1vxblzvZs+C3rnTrzNfs9/ltnUfz+1N4jvic6az/lduhRvM+37PMDhIM7hy/P4vqu6k6dO6Oz68tz6wc2t8Oe7az5PUVmcc95IyGS8vRWPy3/wzDdtjydPxHP9L7cXbI9uwlezZS+eL7a/4XN+t0/FWbGvjnxmrcvTPP/Eedvj8prPrB3fvhPWFxMya7MiztzUgR8jrbwT1vcH8/Na3zR79dWwXt7asD12luLxLkkLT14M6+cffdz2GG7E5/1U14/nH31PnD9/6dH4OMf1hPv/GBuPhxoF7yL1WvwutLa8Zvcxde86CVGh/UHco1n3z7nBMH6/KCY+57de8+PBZaPmuf/Aw2F8L+dZwkRtDmQ8NnNSIpcVW5RxDrAklTZQ1r/b+7149rMkDNY84Xmfuc97BNxneWOjuPwWf/678ZtfAAAAAEDlsfgFAAAAAFQei18AAAAAQOWx+AUAAAAAVB6LXwAAAABA5bH4BQAAAABUHotfAAAAAEDlsfgFAAAAAFRenBr+gFYWu+osLc2tn1ycX5Ok3e04SF6S1tuNsN5q+MDm6WQa1k8//qTt8di5S2H9m9detT1WW027zXQSh3+fPrtqe+QnF8N6r+6/A8mX4mPdubthe1w5fdFu02/Gn3dn1rM9tnfuhvX83GXb4+JTHwrrN288b3sMB3E4fCMhpL6c+eDuWjEJ66PdTdvjrg7C+rQffxZJymvxOJodRfL7MXZY7GsvOAn1ci/8+UbdT9fjWjxedqcD22N7EPeYlv449hudsH6z0bU9Vst4npakcR5vU5Yj22OviMf2jU0/5yzn7bC+E58OSdKnbn7KbvPkhQth/fH1+Dgk6UTrbFjvXb1pe8wG8TkpZ/7a7Zh5OmXuG7dbdpvJ3lbc4+sv2R5dxccyMu8lknTlqXeH9cmt18P6dNPP48fZcLCvfn9nbv31q/F16rT9+9TqcvwOOprEz1JJynfj+qkT67bHeBy/5wwSnrfjhGMdj+Nt6vWa7VEzz/WJeZ+WpOk0fvjPisL2UOnng9Lcpym7UWb2kyW8t5ljzVJ6mIPNct/j+4U7H5LMlZO9/gm7+A785hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHlHmvN7/sSKltfnZ5z9/L/+ifDnX3/1EbuPg+FhWB8N4/w0SZqO4lyyR877HNiyMJlTJ+MsRUnaMxm+ktTrx5/34snTtse0jPPCDntD26M0eYqL5ZrtUSt80OuZlTgQs7cZZ0NK0uHNOCNvMvJhbwtn4kzi8+/+KdujmMS5rZu3XrE9+odx/u79HcXndXnBZ/nVFee/JkS7atKPj6PU/Fy64kFD2o6hRlmqGdyLdZPXfDL3WaLjWjy31RPmnP4wvj8unDple1x8NM5Bv3no84ZTgvuaJl81myZkIxdxFvC5Eydtj7qJutxPyEEvt3225617cb7uXtdnnV4exeMs3/I5vxrEHzif+u/VB9P4s/RnfqyWJl9ZkrqDOA/z9s0bvofJ5exNfdbpqnnvOPneJ8J6ubVt93GcfePZL2vjzmtz6zevza9JUqPu54veYRzSW2/7QO7FxcWwfvHcOdtjbzs+jp0gD/5NnY7PuN7ZjfeTJ/z6azqLnwcDk/ktSTWZeeltev4nxOv6jY4g5zfF2/VG5DKHkzJ634br5/bxoMfAb34BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHn1o2y2UBtruTacW/+J918Of/4D775g93HQH4X1SenX85NpHIY87Q9sj8EwPo5Hx/6z9Ec+yPywFx9Lo+Ev4c7+flhvP2oCyCUNRvHnLVdP2h43N27bbV567VpYf2rttO1x7e52vEFRsz1m7aWwvnjl/bbHTz3+SFjfvv6K7fHCV56222xuvBDWF7Id20OjOKh+OPPnLCuKsF5vzO8xKwqNZ1O7j+OsNWqrM+jOrd+aroQ/fzqfP7e+aW2wG9brm/4enB7E4+WHnnrU9rj85DvD+vYz8ZiVpHOZH3NqxHN5I+F50DmMx35d8T4kqdvthPUXX7lqe5zs+WN97JH1sH6jObE97rwcj4HOgZk/JWXmGZolzBfDWny/j3N/PsY9P2dszw7Cere7bHscjOPnX2/kx8j2zTthvX75bFjfPzi0+zjOrl99Sfvb88fm9tZW+POPPXbF7qPVaYf14diPp/E4nocbdT9uM8XvfrUssz0OEt5Tyzy+D1vteN6SpGkvvn/KmX+PHRfxeS387SPJnxMnZTeZOfeunrpNlZRl0gV8S3LzPMjz+P3zz23/Vg4GAAAAAIDjgMUvAAAAAKDyWPwCAAAAACqPxS8AAAAAoPJY/AIAAAAAKo/FLwAAAACg8lj8AgAAAAAqj8UvAAAAAKDy6kfZrL+3r0ZQv/Has+HPX7zwqN3HhXNnwnq9u2R7FFn8sfdNmLok7e7uhPUT6ydsj95gYrfpD8Zxj8Oe7XFwuBLWn3z8Mduj14v3Mxz4wPVTnZbdpjGKz8mPffDDtsd2P+5xdWPP9hjn7bA+G8RB95KktVNh+fx7/Xg/9d6fsdtMd+6E9e3n/tT2eO3ZL4X1rVdetD3yZjxG8vr8EPJyNpMmU7uP4+xwMNVeff7Y/MO9+PNP/ZSijxTxfNHZ3LA92pN+WP/RH/uE7XH+0jvC+m9/8Ru2x97I32Oz4HxK0iSr2R6dMgvrwxv+nNXW18P6Y2snbY/hzM9L9YVmWH/vT37A9tgemfrTm7bHqCjDelH3c/3AnPeFhYQB31nw+2nGY6A4sWZ7DBX32Li7bXvs7cbvFTvPvxTW7/b8M/Y4272zqcn+/HugmMXjRYV/ne10V8P65t0btsdiZzGsHxzG74aS1GiaOWfo5z7zaihJ6nSXw/renj/WchrPsd2Ee3B/MAvrxTSeTyQpz8z1lyTF25Ty+3F7yZKO460rS3+sTp7755/bz1EcRwp3Xu15f8Drwm9+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVd6Q5v0utjlaDzK+De3Fe4u1ifg7om06ejbOcVmr+Iy0sxVlvWvFZwbUszj5b6tgWWln0+ynzONdxOvFhb8996/mwfupUnEcrSd3u5bDeT8gbft8jF+w2H/vx94f1QUIeXN/Exb7zUpw5J0l37sWZirc2fK7jxmvXw/q1mf8sw4Tc6s7qxbC++p6/ZHv8yJM/EdYvvPZ12+Prn/90WL+78drcWplNJfkxdJxNDjc1Lufnn758L85rHkziuUCSVi/GebLva/hs8aV6fAM9eumS7bG8GOfejmZ+3hr1/TbNRnwvD8uEHmaObY59/vRgO54P8rp/LhU1Px/cMc/Qnee+ZXt023H240E7zjGVpINON6yPEp5tLju+ezIeQ5K0PfZ5qAfTeIzkE5+fe3vjMO7RTsg6Nc/qhSDjVpJ2Biag+Zg7HI40C3JYu412+PP7u7t2H/VO3KNr6pLUMLfyaOiv02I3Hi/DoR+T5cjP5RMz/5VTPz+6mNdZQg7sdObe7X1Ga5b539W9HZm0b1furZNyHLXcn7PC9JnN/Pvy26Ew68PSZM9/N37zCwAAAACoPBa/AAAAAIDKY/ELAAAAAKg8Fr8AAAAAgMpj8QsAAAAAqDwWvwAAAACAymPxCwAAAACoPBa/AAAAAIDKM3HdD+bUyqJOr6/MrWfjOJR7+86m3cczX385rH/12RdsjzMXLoX1n/rYR22PC6fmf05JGu70bY9avWO3Ud4My/W6v4SXz6+F9U67YXu0mvH3JMvNru2hpfizSNJkFh/rwcAHuw9mcWD6cy9dtT12RnfD+vsfO2V7HJ6Or81rtzdsj+def95u88yr8T1x0Fq1PU4ux9fvqTMXbI8f/+jPhPWvfuH35tZGw756B9t2H8fZh8939I7V+ff83e3F8Oe/9JqfU37v6l5Y7zy2YHt0F1thfanm7/XJwTCsz7KZ7dEbxT0kqV2L77FZLeH73Szepsh9j+3eYVgvh1Pbo9nzn3eyO47388o126NrvvMed5dtj29MR2H96pZ/lreLuN4sBrZHo+2ff9kkfh4Md/280yuXwnp90T9DZ434OK6sxfN0u+Hv/+NsOJlK+fxzVFN8D21v3bL7OHXmbFi/cP607dFuxe8x2/e2bI+tu/fCejHz7znd3G/TzGth/fT5+HxI0sZW/EzZ2Y/nPkmazszNrvjekKQs89scRQ+3TVmWb/k4jkLKccwKd96l3DzfUs7ZbBY/z90+UrjjeNDhwW9+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVd6Q5vy8+/6w2FuZnQJb3Xg9/fuWEz059+ptx7unzCRmuH/n4J8P6P/4n/8j2+Dc++ZNhfa3tM7janTg7UJLqjThTczD02X+nTsTZdUXLZ3/ujOJcxxRZQubmxHwfkzXatsfLr98I67/6d37V9tjajLMfP/ih+PpL0l/+xV8K66fP+vG+MPVZl+enccDZN3d91luRxxmKm9fie1eS3nn5TFh/7Mmn5tYO9nb0+kvfsPs4zh45VdcTp+ZPuf9e93L485daN+0+fv+FOHPxX1712ZA/cuV8WD985TXbY9fcx7WE/MHdccLc1o3n0FkZ51xK0qSIz8nd0h/rVjfOaB7Wfc7vUuYfxwsr8ectxn4/urcfllsJz4Mbw3heujfzz7+zjTgbt7sQn1NJWlrwx1oO4vzkrbGfY+u1eCzWtv1YfU8Z58MuHsTj8KCfcG2PsdnoQNNi/v1auN/VzPz7RVbG57Be9/f62XNxNu7pk/FzUJL++SufDuvnz8VzsCR1fLS0+sM4F7w38Xnr0yK+l+11kZTncY+jis71WbBvPSu4SHh2uQzetOMw5yyhQ8qxHkUGr+uRsg93zo46X5nf/AIAAAAAKo/FLwAAAACg8lj8AgAAAAAqj8UvAAAAAKDyWPwCAAAAACqPxS8AAAAAoPJY/AIAAAAAKo/FLwAAAACg8upH2Wxnf6hRkNP+fONu+PO1zXt2H9du3w7rH/3kT9se/9V//TfD+q/9+v9se/zOb38qrL/rwgnbo9GcH+j+poWl5bA+m/mQ8vWV9bB+at2Hstfr8VBpNpu2R5754XY4i0Pox3X/fc3f+/v/e1j/1vPfsD1ajfjz/Oan/i/b4+KTPxzWf/idT9genVbbbrNcxufs/KJtoak5r72ZD2Uvx6OwfuXC5bm17bb/nMfdeDLQaDw/qH29HZ/jn3jipN3HVi8OtX/65p7t8dydnbD+zuHA9hg343u9LPx9fDCMx5MklaP4Pm20/ZxTFvOviSTJ1eXv04NyaHvsX/bz8Il3vyus1+LLL0n6xu9+NqxfSjjvF9dOxRuMxrZHux4f7N7Ej7Pevb7d5mw3ngDPn/TP6mYej6PGtr+vrhwchvVLq6thvTTP4OPu3GpHi535n/HEejf8+dU1f/80uvH71HDmx+3drc2wfuXC47bHpeBZKEmnTsZjQZKms+Bl+w23vvlcWN/aPbA9xmZOyXI/l2eZm0P9HHsUyvKt7yfL/LuQFG+T1ML1UMJkb3rcP5akgwnlZgzUan6tM53G77FHjd/8AgAAAAAqj8UvAAAAAKDyWPwCAAAAACqPxS8AAAAAoPJY/AIAAAAAKo/FLwAAAACg8lj8AgAAAAAq70iD405fuKyV5fk5ajPFmWKTic9CbC7EmX3nLl2wPUqTOXbp/EXb4//5rd8I6wcba7ZHt9Oy27Q6HbOFz+hq1RthfdHkIEpStxNn7DVNLq4ktZvus0hlOz4ndwc+l+6bz30rrP/Fv/hJ2+N9P/K+sP6//oM4S1iSvvD//vOw/thZn+XX7Pp8tK2NjbD+zEsv2h6NhfjanFn2xzobxJnTneb879pqWUpm3fGW1erKavOn3Gwa56ueW/VZyB9+dCWs74/9HHt1N85O7df8nHP60qWwXmvG84kkDac+k3F4EM8H9YnPQW824rEfn9H7pnfiDPtlk18uSaN9n1m7PYnvk9U1/9xZzeLvvBtDfxwXFhbCejPhe/VsIZ7rs0a8D0nKD30u65l6PNZMvPb9/YzicdQ341CSVmrxeX38cnx/l9t+LB9nl86uan15/jnoLsXvKY0F/4x6/dZWWL93sG979HvxmLt7edv2OHvhXNzjbvxMl6RXr16329zciOclZf79ojTb2Jx0HU2W7NvFZQHnuf8spcstLvz7jj1lCee0KP2cUZZurk7JRjbHchSX356PB2vHb34BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHn1o2w200xTzQ9Vnpkw7GYrDqOXpIXluL5/GAfJS9KdzTj4e2t7x/a4sXEvrJfTie3RbnXsNpNJHFKdEj/dasSXeaHVsD1q9TjovNOeH07/pnbbX9+iFidVX7t7x/ZQGff4ub/6V22LD3/4w2H9+vUbtsdvfuq3w/pXn7lie8yGY7vNzp29sD6+d9P2qM+Wwnp/emh7vLpzPax3W825tcP9fdv/uCtLqQzGZlnE93qzGNl9PLUe3+t3zy3aHr1RvJ/pYGh7nDxxKqy3F1dsj13zvJCkyTieZ6emLkmjWvx58iye+yRp2XyN7GdHabwf38eSpGF8rOXGpm1xUfH82KhNbY+lQXysp2v+2bazGz+rW0trtkcx8d/fT/u7YX1/5N8ZRvGtqWLUsz3OPXU6rD96Ob5nBq2W3cdx1lnqamFl/rjJW6vhz/dnfiwUtXibejb/GfWmTiueDw56/j7uTeIx9+rV12yP7W3/zJzaOTSeC+5vEW9TlilvofF5T+mRsk2W+c/jm8T7Ma+XkqR6Hm9UJLy5l0VheiRcu8zfE5NZPN/Pyvg4JMl8XOUJS01/Tsx1sXv4TvzmFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeUea87tzsKtxMT9XcTKNMwrruV+Ll9M4cO+rX3/W9vjh9/2Y6fEN22NivjcY133O4Xji8yNv394K68ORz9xs1uPL3PCHYRPFGk2fFdwwecOSzxQ7HA5sj/WTZ8L6yRMnbI8Dkzt79txZ22N7J86T/sxnPm17DA99fuS9e3EGby8h663eiTMkawnhdmtn4pzK02fmn7N+z3/O467IchXBtZjJ3IgJ2eEr9fg6/eilk7bHvYPtsD6+c9v2mJjr2Vzw8+MwJaOwjLfJg+fRm2YmSz2b+bE/Ncc6bqTkT/p83cw8/2Y1n1PqQhlnU38cpckbbs/886CcxBnmG+04n1eSJkF2+JsKE4/bWPDH2u/Hx9pMyMI8dTl+ZrTr8Wdp1f1xHmdLqye1cnJ+3vy12wfhz79+O37eStLM5MCOB37sDwfxWNjt+XeyzLwLjcycJEkJMeiqm3e/ImFuK1zerB/6UpayUewosoBTYoDrJgu6SLjXS7O0yho+s7ucxfupJXyYYubH0XRm8nMT3v0y8xzOMv/un7kxkpnn9AOOMX7zCwAAAACoPBa/AAAAAIDKY/ELAAAAAKg8Fr8AAAAAgMpj8QsAAAAAqDwWvwAAAACAymPxCwAAAACoPBa/AAAAAIDK88nDD6DISs2CoOGsFoe4H/b7dh+Dw8OwvnH3nu3xd3/t18P66y+/bnscjuPA5Zdv+sD1MiGlfGZCqicmCFuSstkorNcSvgPJFAddZwMfpl1mPkDexmknBJ13FuLPe++eHyOtZjxW9/f2bY/RKP68V6/esD2yqT+vEzMEynbX9nBntdmIz4ckLbQWw3q/N/+zDBPGz3HXaHXU7CzMrdfMdRrvxnOfJM0m8Xk8v+rHwg/vDcP6c7t3bI+NW9fC+v7A3z+HhZ/bhnk8dzUS5thpGZ+zvPSPyV4Wz1z90s5sqifMw8UoPifFKL52kpTl5lgSztmwHp+zYurn+p7Zz7AVz+OSpNzPG+1GK6wXs7HtsVDEx/KOM0u2x1oz/rz9e7thfbh7YPdxnI1nUvTIvHFrM/z5Gxv+nWvsxnbh78HpOB7b3YX5c/yb6tP4Pp5N/D2Y8v6YN+LPU/opVoWZh/1RSJmZ23Izj6cqzDkx0/T9bdwnSngHde/ttbzmj8Ock2bC86Ks+Q9cms/jrr8kFWYdUoz9XJ6bwZjX4uMsk0bit/V7oK0BAAAAADiGWPwCAAAAACqPxS8AAAAAoPJY/AIAAAAAKo/FLwAAAACg8lj8AgAAAAAqj8UvAAAAAKDyWPwCAAAAACqvfpTNVlZWtLQShb3Hwc6Dw57dx2hhMaznmV/P7+7EYfInTp22PVbWT4X1aUIAeVGO7TbTSRwOPZvGgeuSNJnEgdtFQqC6C+0ejfxnKRLCwV3qep7wfc3u/n5Y/9znP2d7fPzjHw/r3/zWc7aHOWUaJ4yRmrlnJKkwY35iAsglaTaaxBuM/bFef/16WK+15s8Ns6kPQT/28tr9/8yRZY3wx+sdv4thHl/HRtNfx8vnumH9tRv+Xh+P4rl8Vvgeu1O/zVYWP8KWav7+ycy8lGWZ7bFnbrGNsZkMlPbsqpX+WOx+TL2RMOfcKeJxtif/eQ/NObuQ+8+6ap5tklTbPgjrZ+pt2+PHLp0N649f8jdnd3AY1kezeLyPD/p2H8fZcDDQoDd/7E0m8ZhLuX9mEzen+GdlvRbvp5bwnlM3mzTlx37RatltxlN3f6TMJ+ZgE17r3BSaJ9zrhb80Vsp+MjMGaglzW25OSj4b2h41c6ydul++1esJz78s3mZq7jtJmtp3TN/D3Xu1Wnw+Rmbt8N34zS8AAAAAoPJY/AIAAAAAKo/FLwAAAACg8lj8AgAAAAAqj8UvAAAAAKDyWPwCAAAAACqPxS8AAAAAoPKONOd3pkKzIKupMEFd9VbT7qPVijMo6wnZV2trJ+MNbDaaVJiM1jwhX3I69rl9hcn+myVkuLrznhK/O53EecKHvTjDUJJGI5/lOjE5fLOEa+P2889+53dsj2e/9a2w/uWnv2J7ZHmc2zpLyNibJlycmck3K6cJY2QWX1+fJi3lQYatJLXL+Vlvbv+VUGRSMf/7xtEgng9SMl4zkw1Yjn3e3uLCQlg/uezzd7fvbob1g424Lkl7Jk9Tkj5v8mbXEua2ZZOvvJCQ8zvJ4x3tT/2BDBPyI92R1HJ/zprm2dRNyv6Me9QzP+d0zTkrzDNHksYzf6wdc15XFhPmnkmcHX+44z/v/nI8zrJpPJYPB9WeI8f9noaN+WNiOhiEP58lvAvVTP7qLOE55HJRy4l/z6m7vNmEW7Bs+XzqaRkfy3jqP2+ZNB/EZuYdtEh4z0l5T/U9EvKEzedN+Y1htx5/3m7DH8dyN85x7nb99U9Zh7g1U57wTCnNO2jCI9TmZzeacX3vcKTXNvw65E385hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlRenGz+gLKuFAeCNRrzWzmoJScgm1L7RiIPkJcnknKtMSGRuufDohB7NhLOfKQ6ynk58SLkLGE9JD3dh2SdOrtsek4RjdWHZs4Qg+6KYhfVer297bNy5E9YfeeRR2+OgNwnr/cHA9rCDVdLUXL+ZOaeSVJoxkhKW7sLQ83z+PTEZjzXs7dl9HGdFKc2K+deqDGqSlJkQeElq1pthvRzEY/L+RnH59EK8D0n6yjeeDev3bt21PaaZnyDvKp5n96dj26Nr5pRuwmOpZa5N2fTnzN0/kpSZ50q97p9/bj7Yn/kxMp3Gc6ybxyWp6T5uwvOiSLgn8no8oAv5z7t7uBvWa6U/1la+FNazIh7vvZEfy8dZMRurmM6/nuvL8diu1/2YG5nLVBb+Pm3U4uNoJtyDzTx+ns4K32Nv6sdcuxGPqWnbT27jcXxepxP/juJeQe07qqQy4T01M8+DWs33aNbjuW1lIX4nl6Qz6ytxj45/trWb8RjJ62/9eSFJtVp8LCnPFLefLPfnvWbeMWtmrt/a7Uu6avfzJn7zCwAAAACoPBa/AAAAAIDKY/ELAAAAAKg8Fr8AAAAAgMpj8QsAAAAAqDwWvwAAAACAymPxCwAAAACovCPN+S3LXGU5P6upLEwWlMnoknx8bpGQF2azgOs+09TlWuUJ+Vop+6mZ7MeGyQaVpMkkzjGczeJcM0lyl8ZllEpSLfN5YdNZnF2XEDerhjlnnaVV2+PC5Tjvr0j4vINxfF5Tco9TxrPLf03Jx3P7cRlskh9Ho9Fobq3XO9Tm7Zt2H8dZVq8rD+aehrlMmb+MykxmnxLu9VnvMKyfW+raHica8X4aQ59xvWyeF5I0zEy2tKlL0tTkg/YS7sGBuzYJ2bm16VvPscwTco3dfFBmCXOOqTcyP180zFjtJFy7xYSv7xcyMxYTHn+SmdsGPdvB3Fbq5vF9Na54zm+mqbIgc/nUevxMPnXCv18URXwdc7Vsj1r+1l+b3fM25bm/3PfjodFaCOt57ufY0TA+Z+P5j/U/43J8r43c4QAABiFJREFUU95RUrbJTX5ys+EnjE4znqsXu36MdDvxvewyayX/7p/XEjJ8E8Zqnrv7xh9rKXNtkn7Nat5jzTt3M20iT9wbAAAAAAAVwOIXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeSx+AQAAAACVx+IXAAAAAFB5bz2t+9tMxoXGQSB2lsWhzAm5z2qY4OeUcPBaPf7YWT0OypZ8qHPhQp8lZZn/wHkWB1A3Oj7YvazFod2tlBNv+cDtlJDy6XQa1idjH+xelPEYcPuQpP447jGb+UDt4TQ+7+5+kCQlBJmX5ljKhHui2WyG9bq5Z1J0u/OD34uZvybHXV6rKQ/OY60096EZ1/ebmOs08/NFPY/H3GLm78GPvvt8WN/r+x5fvbZlt9kaxeNmWPg5Z2TmrsKdU0mF+R55lnAceZbyzDA9ct/DqSU8l+pmN53cn7NuHo/Fpbqf+5Zyf0+cMIfSTZiHG4rHWTPhvJdmjhsOB2F9NI6fJ8deWd7/zxz1ejwuXV2SGo12XK+1bA/3rpPynuPeH8Zj/zysJdxjS8vzn7mSVJR+Hs7k3of9+3KWx583S5j7Ut4x3TtVnnCvu1GU8trmjiPl3d+ul2r+WV7L/bXJzTycZX6c5eadITN1SSrdmS/jHofDB3v28ZtfAAAAAEDlsfgFAAAAAFQei18AAAAAQOWx+AUAAAAAVB6LXwAAAABA5bH4BQAAAABUHotfAAAAAEDlHWnO7/2YtiiLKc5pmk19dqqyeJtWy+e0TSZxXt5s5vP0Gs04Gyslb7gun9M1m8R5b9OEaCuXO5eSSWxzvBLCzzKT0SxJjVacS1ZrxHm0KceSktHrrt/EZPhKUl7E165IOI5pwjY1k39WJOQauzGSkl3o5MH1Txkbx16jJTWjnMn4OmUp18DkMU8Txm1hHgsur1SSzsXxkvrL77tge5xp+Dn05Tv7Yf1Oz3/enWl8/wwLn5U4MpdmmiXcgym57zUzP5q65NMyGwmZxHVzaRYSspFb5vO2Mn/9l2t+flwzecELCTn37Ub8eer+tNv3jr55txmaTOvjLsuz8DngxnbTvJNJUrsdb1NPyE7NXC54wrufewcpE3p0Gx27TaMWH2vK+0VmsrQTbp+EHNiE3NuEnF+7SdL7stlFwmHkJl83JW9Y7nmQlOGbsE1mjjVpP/Gx1sw4lCSVcY/M/K62mfDM+XY/AG+cAAAAAIAfdCx+AQAAAACVx+IXAAAAAFB5LH4BAAAAAJXH4hcAAAAAUHksfgEAAAAAlcfiFwAAAABQeUeV89uUpH6vF27k8lez0meb5VkcwjUajWwPdxxFwlcC9cZbz/mtpZx+k+FaT8h1dBmtLqNLSrh2KTm/SdvE9SIhg9Ln/PprU5qxOE3IOp0UcYZeSs7vLGEc5S7nN+FYXbhdLSFDbWb2E2X59XqHb/6fPsj5+GlK0itbh+FGhcn0TskodIGjs0Hftij2BvFhlH48FeYqbh74Hjf7PqP37ji+P3YSgtD3zTZjc3/d3yauTxPmrbQYS5O5mXCsbot6wrHW3Oct/dzWNM/yZkKu9ThhP4U5Jz0/xaplnhltfxhqmEd1exrv49bwz3ZStTmyKUl7h+NwI5dx3Wz6OaXVjOeUlOecz/n147Yw7wbDgX+PrZv7R5IaJuN6WgxtD/dOVsv9eXeRtUnvhm9Xzu9b3IXks3FTYn79SfNjNS3nN+6TJfWIjzVPyvk1awzzu9q7Ow/2DnlUi99LkvSNp790RO0A/IC6JOmrD/sgjtglSfq3/o8/edjHgf9fEr48At4+VZsjL0nSv/jCjYd9HACOv6T5MXO/FUyRZdmKpI9Jui4p/voOAP68pu5PWp8ty3LvYR/MUWJ+BHAEKjlHMj8COAIPND8eyeIXAAAAAIDvZ/zBKwAAAABA5bH4BQAAAABUHotfAAAAAEDlsfgFAAAAAFQei18AAAAAQOWx+AUAAAAAVB6LXwAAAABA5bH4BQAAAABUHotfAAAAAEDlsfgFAAAAAFQei18AAAAAQOWx+AUAAAAAVB6LXwAAAABA5bH4BQAAAABUHotfAAAAAEDl/X8DtqIrq+vybwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1200x1200 with 3 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example images\n",
    "plt.figure(figsize=(10,10), dpi=120)\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "      \n",
    "    img_reshaped = transform_img(X_train[i])\n",
    "    label = y_train[i]\n",
    "\n",
    "    # Set title\n",
    "    plt.title(f'{label}', fontsize=12)\n",
    "\n",
    "    # Get label for this image\n",
    "    plt.imshow(img_reshaped, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNNeeRDpAL7j"
   },
   "source": [
    "### 2.2.- Training and Evaluation\n",
    "\n",
    "This section contains functions that help us train and evaluate our model, as well as to estimate the transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGzDeGiTG4LY"
   },
   "outputs": [],
   "source": [
    "# When all random seeds are fixed, the python runtime environment becomes deterministic.\n",
    "def seed_torch(seed=1029):\n",
    "    \"\"\"\n",
    "    Function to set seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # If multi-GPUs are used. \n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhgAAoAiAiQQ"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiAWCsSO2DX4"
   },
   "outputs": [],
   "source": [
    "class DatasetArray(Dataset):\n",
    "  \"\"\"This is a child class of the pytorch Dataset object.\"\"\"\n",
    "  def __init__(self, data, labels=None, transform=None):\n",
    "      self.data_arr = np.asarray(data).astype(np.float32)\n",
    "      self.label_arr = np.asarray(labels).astype(np.long)\n",
    "      self.transform = transform\n",
    "      \n",
    "  def __len__(self):\n",
    "      return len(self.data_arr)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "      data = self.data_arr[index]\n",
    "      label = self.label_arr[index]\n",
    "      \n",
    "      if self.transform is not None:\n",
    "        data = self.transform(data)\n",
    "          \n",
    "      return (data, label)\n",
    "\n",
    "\n",
    "def get_loader(batch_size=128, num_workers = 1, val_split=0.2, X_train=None, y_train=None, X_test=None, y_test=None, transform=False):\n",
    "  \"\"\"\n",
    "  Function to perform the train test split and return the DataLoader objects for each set.\n",
    "  \n",
    "  :param batch_size = Number of batch size\n",
    "  :param num_workers = Number of workers for DataLoader\n",
    "  :param val_split = Percentage to split the training set into training and validation set\n",
    "  :param X_train = X_train data\n",
    "  :param y_train = y_train data\n",
    "  :param X_test = X_test data\n",
    "  :param y_test = y_test data\n",
    "  :return X_train DataLoader\n",
    "  :return X_val DataLoader\n",
    "  :return X_test DataLoader\n",
    "  \"\"\"\n",
    "\n",
    "  # Split data\n",
    "  if val_split != 0.0:\n",
    "    print(f\"Validation Split: {val_split}\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                      y_train,\n",
    "                                                      test_size=val_split, \n",
    "                                                      random_state=42)\n",
    "    \n",
    "    train_data = DatasetArray(data=X_train, labels=y_train)\n",
    "    val_data = DatasetArray(data=X_val, labels=y_val)\n",
    "    test_data = DatasetArray(data=X_test, labels=y_test)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader\n",
    "  \n",
    "  else:\n",
    "    print(f\"No Validation Split Inserted - Returning the entire dataset\")\n",
    "    train_data = DatasetArray(data=X_train, labels=y_train)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return train_loader, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6n0zTKoxA8oW"
   },
   "outputs": [],
   "source": [
    "def prediction(model, test_loader, device, revision=False):\n",
    "    \"\"\"\n",
    "    Function to generate predictions\n",
    "\n",
    "    :param model: Model\n",
    "    :param test_loader: Test DataLoader\n",
    "    :param device: GPU or CPU\n",
    "    :param revision: True or False - T-revision method\n",
    "    :return y_true: True labels\n",
    "    :return y_pred: Predicted labels\n",
    "    :return acc: Accuracy\n",
    "    :return outputs: Output from the softmax function\n",
    "    :return predictions: Output from the argmax function applied to the softmax output\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the model to be evaluated\n",
    "    model.eval()\n",
    "    \n",
    "    # Create empty tensors to store the results\n",
    "    y_true = torch.tensor([], dtype=torch.long, device=device)\n",
    "    all_outputs = torch.tensor([], device=device)\n",
    "    \n",
    "    # Deactivate autograd engine to reduce memory usage\n",
    "    with torch.no_grad():\n",
    "        #for data in test_loader:\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if revision:\n",
    "              outputs = model(inputs, revision=revision)\n",
    "              outputs = outputs[0]\n",
    "            else:\n",
    "              outputs = model(inputs)\n",
    "            y_true = torch.cat((y_true, labels), 0)\n",
    "            all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "    \n",
    "    # Get the output from the softmax layer\n",
    "    outputs = F.softmax(all_outputs, dim=1)\n",
    "\n",
    "    # Convert the true labels to numpy\n",
    "    y_true = y_true.cpu().numpy()\n",
    "\n",
    "    # Get pred values from the model\n",
    "    _, y_pred = torch.max(all_outputs, 1)\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = sum(y_true == y_pred)/len(y_pred)\n",
    "    return y_true, y_pred, acc, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7u75NTEbCew_"
   },
   "outputs": [],
   "source": [
    "def estimate_tmatrix(num_classes, outputs, y_pred, average=True):\n",
    "  \"\"\"\n",
    "  Function to estimate the transition matrix based on the noisy class posterior\n",
    "\n",
    "  :param num_classes: Number of classes\n",
    "  :param outputs: Outputs from the softmax function\n",
    "  :param y_pred: Prediction values\n",
    "  :param average: Average the probabilities for each layer P(Y^{~}=i | x)\n",
    "  :return T: Estimated transition matrix\n",
    "  \"\"\"\n",
    "\n",
    "  T = np.empty((num_classes, num_classes))\n",
    "  # Iterate over the number of classes\n",
    "  for i in range(num_classes):\n",
    "    # Average the probability per column\n",
    "    if average:\n",
    "      # Get the samples with this predicted class\n",
    "      # Get the samples of these indexes P(Y^=i | X=Xi)\n",
    "      Xi_outputs = outputs[(y_pred == i)]\n",
    "\n",
    "      T[i, :] = np.mean(Xi_outputs, axis=0)\n",
    "    \n",
    "    else:\n",
    "      # Get the argmax\n",
    "      idx = np.argmax(outputs[:, i])\n",
    "      # Iterave over each label to fill the matrix\n",
    "      for j in np.arange(num_classes):\n",
    "        T[i, j] = outputs[idx, j]\n",
    "  return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNH2HhvCNcRX"
   },
   "source": [
    "## 3.- Classifiers robust to label noise\n",
    "\n",
    "In this section, we present the implementation of two classifiers which are robust to label noise. A detailed formulation of each one are provided in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8c9ozH1k0xL"
   },
   "source": [
    "### 3.1- T-Revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrTQtTQcxTI4"
   },
   "source": [
    "We first initialize the transition matrix by exploiting examples that are similar to anchor points, namely, those having high estimated noisy class\n",
    "posterior probabilities. Then, we modify the initial matrix by adding a slack variable, which will be learned and validated together with the classifier by using noisy data only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thg_P-u678qY"
   },
   "source": [
    "#### 3.1.1.- First Training Stage\n",
    "\n",
    "In this stage we should learn $\\widehat{T}$ using the noisy training data. We will train the neural network to learn the noisy class posterior and estimate the transition matrix using this neural network. In this case, we will use the ResNet as the paper does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRerdZW478IK"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, T=None):\n",
    "  \"\"\"\n",
    "  Function to train the model\n",
    "  \n",
    "  :param model: Model\n",
    "  :param T: Transition Matrix\n",
    "  :param dataloaders: DataLoader\n",
    "  :param criterion: Loss function\n",
    "  :param optimizer: Optimizer\n",
    "  :param num_epochs: Number of epochs\n",
    "  :return model: Trained model\n",
    "  \"\"\"\n",
    "\n",
    "  val_acc_history = []\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_acc = 0.0\n",
    "\n",
    "  print(f\"Transition Matrix: \\n{T}\\n\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "      print('-' * 10)\n",
    "\n",
    "      # Each epoch has a training and validation phase\n",
    "      for phase in ['train', 'val']:\n",
    "          if phase == 'train':\n",
    "              model.train()  # Set model to training mode\n",
    "          else:\n",
    "              model.eval()   # Set model to evaluate mode\n",
    "\n",
    "          running_loss = 0.0\n",
    "          running_corrects = 0\n",
    "\n",
    "          # Iterate over data.\n",
    "          for inputs, labels in dataloaders[phase]:\n",
    "              inputs = inputs.to(device)\n",
    "              labels = labels.to(device)\n",
    "\n",
    "              # zero the parameter gradients\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "              # forward\n",
    "              # track history if only in train\n",
    "              with torch.set_grad_enabled(phase == 'train'):\n",
    "                  outputs = model(inputs)\n",
    "                  loss = criterion(outputs, labels)\n",
    "                  \n",
    "                  if torch.is_tensor(T):\n",
    "                    prob = F.softmax(outputs, dim=1)\n",
    "                    prob = prob.t()\n",
    "                    out_forward = torch.matmul(T.t(), prob)\n",
    "                    out_forward = out_forward.t()\n",
    "\n",
    "                    _, preds = torch.max(out_forward, 1)\n",
    "                  else:\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                  # backward + optimize only if in training phase\n",
    "                  if phase == 'train':\n",
    "                      loss.backward()\n",
    "                      optimizer.step()\n",
    "\n",
    "              # statistics\n",
    "              running_loss += loss.item() * inputs.size(0)\n",
    "              running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "          epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "          epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "          # deep copy the model\n",
    "          if phase == 'val' and epoch_acc > best_acc:\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "          if phase == 'val':\n",
    "              val_acc_history.append(epoch_acc)\n",
    "\n",
    "      print()\n",
    "\n",
    "  print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "  # load best model weights\n",
    "  model.load_state_dict(best_model_wts)\n",
    "  return model, val_acc_history\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "  \"\"\"\n",
    "  Function to define if feature extracting\n",
    "  \n",
    "  :param model: Model\n",
    "  :param feature_extracting: Boolean to indicate if perform feature extraction\n",
    "  \"\"\"\n",
    "  if feature_extracting:\n",
    "      for param in model.parameters():\n",
    "          param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScIHf9mJN4UJ"
   },
   "outputs": [],
   "source": [
    "# Clean cache\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Define parameters\n",
    "num_classes = 3\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "use_pretrained = False\n",
    "feature_extract = False\n",
    "\n",
    "# Initialize ResNet model\n",
    "model_resnet = models.resnet18(pretrained=use_pretrained)\n",
    "\n",
    "# Replace 1st conv layer with 1 channel if fashionMNIST\n",
    "if X_train.shape[1] == 1:\n",
    "  model_resnet.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=True)\n",
    "      \n",
    "set_parameter_requires_grad(model_resnet, feature_extract)\n",
    "num_ftrs = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "input_size = 224\n",
    "\n",
    "# Detect if GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFQ3mXns9rCj"
   },
   "source": [
    "Now we need to generate our Training, Validation and Testing DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14808,
     "status": "ok",
     "timestamp": 1605607265242,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "473z2PFC9wvX",
    "outputId": "415825db-4145-4fd8-e6a1-183615ef9912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Split: 0.2\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                   y_train=y_train, \n",
    "                                                   X_test=X_test, \n",
    "                                                   y_test=y_test, \n",
    "                                                   batch_size=batch_size,\n",
    "                                                   transform=True)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTfCruzD91Vo"
   },
   "source": [
    "Training the model to estimate the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56414,
     "status": "ok",
     "timestamp": 1605607306859,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "u2LwjqCk933a",
    "outputId": "f36ece63-2687-495a-a82b-be44ed699fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1623 Acc: 0.3444\n",
      "val Loss: 1.1376 Acc: 0.3370\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1048 Acc: 0.3844\n",
      "val Loss: 1.1601 Acc: 0.3417\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0652 Acc: 0.4267\n",
      "val Loss: 1.1241 Acc: 0.3657\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0163 Acc: 0.4882\n",
      "val Loss: 1.1306 Acc: 0.3590\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9340 Acc: 0.5653\n",
      "val Loss: 1.2366 Acc: 0.3457\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.8585 Acc: 0.6171\n",
      "val Loss: 1.2499 Acc: 0.3567\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.7566 Acc: 0.6763\n",
      "val Loss: 1.3596 Acc: 0.3527\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.6421 Acc: 0.7361\n",
      "val Loss: 1.5339 Acc: 0.3417\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.5523 Acc: 0.7807\n",
      "val Loss: 1.5994 Acc: 0.3620\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.4485 Acc: 0.8262\n",
      "val Loss: 1.8324 Acc: 0.3400\n",
      "\n",
      "Best val Acc: 0.365667\n"
     ]
    }
   ],
   "source": [
    "# If GPU: send the model to GPU\n",
    "model_resnet = model_resnet.to(device)\n",
    "\n",
    "# Generate a list of parameters to learn based on feature_extraction or pretrained model\n",
    "params_to_update = model_resnet.parameters()\n",
    "if feature_extract:\n",
    "  params_to_update = []\n",
    "  for name,param in model_resnet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "      params_to_update.append(param)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and Evaluate model\n",
    "model_resnet, hist = train_model(model_resnet, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqkL75q7Bv-g"
   },
   "source": [
    "Now, we need to evaluate our entire training set using the previous model. We set the `val_split=0.0` to return the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57670,
     "status": "ok",
     "timestamp": 1605607308125,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "krqGLTVVBrzK",
    "outputId": "dbeb2c00-5181-4092-8628-de626771b3eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Validation Split Inserted - Returning the entire dataset\n",
      "\n",
      "Accuracy: 0.4577333333333333\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                   y_train=y_train, \n",
    "                                                   X_test=X_test, \n",
    "                                                   y_test=y_test, \n",
    "                                                   val_split=0.0,\n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "y_true, y_pred, acc, outputs = prediction(model_resnet, train_loader, device)\n",
    "\n",
    "print(f\"\\nAccuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtlDM3-WCYKn"
   },
   "source": [
    "#### 3.1.2.- Generate transition matrix - CIFAR10\n",
    "\n",
    "The function `estimate_tmatrix` contains the code to estimate the transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57661,
     "status": "ok",
     "timestamp": 1605607308126,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "441kEKR23wG1",
    "outputId": "c7c34c08-6824-40ec-9c91-898a3ffa1cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Transition Matrix: \n",
      "[[0.4395 0.3019 0.2586]\n",
      " [0.283  0.4672 0.2498]\n",
      " [0.2788 0.2901 0.4311]]\n"
     ]
    }
   ],
   "source": [
    "# Using average\n",
    "estimated_T = estimate_tmatrix(num_classes=num_classes, \n",
    "                               outputs=outputs.cpu().numpy(), \n",
    "                               y_pred=y_pred)\n",
    "\n",
    "print(f\"Estimated Transition Matrix: \\n{np.round(estimated_T, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgK8d222__LJ"
   },
   "source": [
    "Now, we can update the value of the transition matrix for the CIFAR dataset in the `T_matrices` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdot_FjjAIUr"
   },
   "outputs": [],
   "source": [
    "T_matrices['cifar'] = torch.from_numpy(estimated_T).float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQccNnhpMSQV"
   },
   "source": [
    "#### 3.1.3.- Validating the effectiveness of our estimator - FashionMNIST0.6\n",
    "\n",
    "We will validate the effectiveness of our estimator using the dataset `FashionMNIST0.6`, where the transition matrix is already provided:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "0.4 & 0.3 & 0.3\\\\\n",
    "0.3 & 0.4 & 0.3\\\\\n",
    "0.3 & 0.3 & 0.4\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The ResNet will be trained using the `FashionMNIST0.6 dataset` and we compare the estimated transition matrix with the provided one. Firstly, we load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58139,
     "status": "ok",
     "timestamp": 1605607308620,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "WkEmTfMHNu9E",
    "outputId": "ac5a7dc3-e862-41bb-cad8-75679051525d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset fashionmnist0.6 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Reading data - You need to add the path to the folder with the datasets here.\n",
    "dataset = \"fashionmnist0.6\"\n",
    "\n",
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "if dataset.lower() in available_datasets:\n",
    "  dir = available_datasets[dataset.lower()]\n",
    "  T_matrix = T_matrices[dataset.lower()]\n",
    "  X_train, y_train, X_test, y_test = load_data(dir)\n",
    "  print(f\"Dataset {dataset} loaded.\\n\")\n",
    "\n",
    "  print(f\"Shape Xtr: {X_train.shape}\")\n",
    "  print(f\"Shape Str: {y_train.shape}\")\n",
    "  print(f\"Shape Xts: {X_test.shape}\")\n",
    "  print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "  print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "else:\n",
    "  print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWKtaHpKPTaq"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed_torch()\n",
    "\n",
    "# Clean cache\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Define parameters\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "use_pretrained = False\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b7dFhp8OOZq"
   },
   "source": [
    "ResNet model accept as input colorizer images (3 channels) but the images in the `FashionMNIST0.6` dataset are in greyscale (1 channel). Hence, we need to modify the first layer of our ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcMj__zdN8a6"
   },
   "outputs": [],
   "source": [
    "class greyscaleResNet(nn.Module):\n",
    "    def __init__(self, pretrained, num_classes=3, in_channels=1):\n",
    "      super(greyscaleResNet, self).__init__()\n",
    "\n",
    "      # Load ResNet\n",
    "      self.model = models.resnet18(pretrained=pretrained)\n",
    "      self.num_ftrs = self.model.fc.in_features\n",
    "      \n",
    "      # Modify first layer and last layer\n",
    "      self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "      self.model.fc = nn.Linear(self.num_ftrs, num_classes)\n",
    "\n",
    "      self.revision = nn.Linear(num_classes, num_classes, False)\n",
    "\n",
    "    def forward(self, x, revision=False):\n",
    "      T_delta = self.revision.weight\n",
    "      if revision==True:\n",
    "        return self.model(x), T_delta\n",
    "      return self.model(x)\n",
    "\n",
    "model_greyscaleResNet = greyscaleResNet(pretrained=use_pretrained)\n",
    "\n",
    "# Initialize ResNet model\n",
    "set_parameter_requires_grad(model_greyscaleResNet, feature_extract)\n",
    "input_size = 224\n",
    "\n",
    "# Detect if GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvL-uEZcOBmf"
   },
   "source": [
    "Generate the training, validation and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58118,
     "status": "ok",
     "timestamp": 1605607308622,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "7elLEGZeOF6K",
    "outputId": "c48e4f77-f97f-4091-a454-9bfba99e3f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Split: 0.2\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                   y_train=y_train, \n",
    "                                                   X_test=X_test, \n",
    "                                                   y_test=y_test, \n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272977,
     "status": "ok",
     "timestamp": 1605607523493,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "l-cwjfnKOP6c",
    "outputId": "de55dad3-79df-44aa-f023-fdb99d219f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1757 Acc: 0.3442\n",
      "val Loss: 1.1333 Acc: 0.3669\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.1383 Acc: 0.3582\n",
      "val Loss: 1.1434 Acc: 0.3414\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.1288 Acc: 0.3647\n",
      "val Loss: 1.1184 Acc: 0.3703\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.1260 Acc: 0.3627\n",
      "val Loss: 1.1137 Acc: 0.3794\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.1142 Acc: 0.3697\n",
      "val Loss: 1.1119 Acc: 0.3600\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.1020 Acc: 0.3906\n",
      "val Loss: 1.1243 Acc: 0.3583\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0966 Acc: 0.3955\n",
      "val Loss: 1.1323 Acc: 0.3683\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0862 Acc: 0.4155\n",
      "val Loss: 1.1420 Acc: 0.3647\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0706 Acc: 0.4333\n",
      "val Loss: 1.1491 Acc: 0.3536\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0680 Acc: 0.4322\n",
      "val Loss: 1.1533 Acc: 0.3519\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0421 Acc: 0.4644\n",
      "val Loss: 1.1833 Acc: 0.3547\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0250 Acc: 0.4754\n",
      "val Loss: 1.1897 Acc: 0.3514\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0037 Acc: 0.4935\n",
      "val Loss: 1.2173 Acc: 0.3422\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9710 Acc: 0.5157\n",
      "val Loss: 1.2345 Acc: 0.3456\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9531 Acc: 0.5250\n",
      "val Loss: 1.2641 Acc: 0.3461\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9134 Acc: 0.5515\n",
      "val Loss: 1.3499 Acc: 0.3506\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8933 Acc: 0.5655\n",
      "val Loss: 1.3549 Acc: 0.3456\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.8577 Acc: 0.5883\n",
      "val Loss: 1.4916 Acc: 0.3458\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.8374 Acc: 0.6058\n",
      "val Loss: 1.3814 Acc: 0.3497\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.8087 Acc: 0.6197\n",
      "val Loss: 1.4657 Acc: 0.3447\n",
      "\n",
      "Best val Acc: 0.379444\n"
     ]
    }
   ],
   "source": [
    "# If GPU: send the model to GPU\n",
    "model_greyscaleResNet = model_greyscaleResNet.to(device)\n",
    "\n",
    "# Generate a list of parameters to learn based on feature_extraction or pretrained model\n",
    "params_to_update = model_greyscaleResNet.parameters()\n",
    "if feature_extract:\n",
    "  params_to_update = []\n",
    "  for name,param in model_greyscaleResNet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "      params_to_update.append(param)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and Evaluate model\n",
    "model_greyscaleResNet, hist = train_model(model_greyscaleResNet, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_ehoP2ORB3j"
   },
   "source": [
    "As we did previously, we need to evaluate our entire training set using the previous model. We set the `val_split=0.0` to return the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276457,
     "status": "ok",
     "timestamp": 1605607526983,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "ao5PC4GQRHjl",
    "outputId": "f8deb0b3-ec02-471f-d6df-1623711d0195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Validation Split Inserted - Returning the entire dataset\n",
      "\n",
      "Accuracy: 0.4066666666666667\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                   y_train=y_train, \n",
    "                                                   X_test=X_test, \n",
    "                                                   y_test=y_test, \n",
    "                                                   val_split=0.0,\n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "y_true, y_pred, acc, outputs = prediction(model_greyscaleResNet, train_loader, device)\n",
    "\n",
    "print(f\"\\nAccuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdAhGchVRMOo"
   },
   "source": [
    "As we did before, we estimate the transition matrix using the function `estimate_tmatrix()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276447,
     "status": "ok",
     "timestamp": 1605607526984,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "3cLXpZlFRLyl",
    "outputId": "c5647c41-18a8-4f39-cee8-83480e190e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Transition Matrix: \n",
      "[[0.47519803 0.25008431 0.27471787]\n",
      " [0.2734361  0.43387389 0.29269025]\n",
      " [0.28925034 0.28128991 0.42946056]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate transition matrix\n",
    "estimated_T_fashionmnist06 = estimate_tmatrix(num_classes=num_classes, \n",
    "                                              outputs=outputs.cpu().numpy(),\n",
    "                                              y_pred=y_pred)\n",
    "\n",
    "print(f\"Estimated Transition Matrix: \\n{estimated_T_fashionmnist06}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VavC-HpHRdls"
   },
   "source": [
    "The transition matrices estimated using our estimator are very closed to the true transition matrix. It is possible to calculate the difference using the sum average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276435,
     "status": "ok",
     "timestamp": 1605607526984,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "8dETwfxxRnyn",
    "outputId": "d28579b3-84bc-42df-f0d5-19471fb9be2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.09235457838737342\n"
     ]
    }
   ],
   "source": [
    "T_true = T_matrix.cpu().numpy()\n",
    "\n",
    "# Calculate differences between estimated_T_argmax and T_true\n",
    "error = np.sum(np.abs(estimated_T_fashionmnist06-T_true)) / np.sum(np.abs(T_true))\n",
    "\n",
    "print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Epz9v1JpOg07"
   },
   "source": [
    "#### 3.1.4.- Validating the effectiveness of our estimator - FashionMNIST0.5\n",
    "\n",
    "In this section, we will validate the effectiveness of our estimator using the dataset `FashionMNIST0.5`, where the transition matrix is already provided:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "0.5 & 0.2 & 0.3\\\\\n",
    "0.3 & 0.5 & 0.2\\\\\n",
    "0.2 & 0.3 & 0.5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The ResNet will be trained using the `FashionMNIST0.5 dataset` and we compare the estimated transition matrix with the provided one. Firstly, we load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276927,
     "status": "ok",
     "timestamp": 1605607527488,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "x932T75LOg08",
    "outputId": "45a8c2ad-5a40-4705-dee7-ba97b7d14a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset fashionmnist0.5 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Reading data - You need to add the path to the folder with the datasets here.\n",
    "dataset = \"fashionmnist0.5\"\n",
    "\n",
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "if dataset.lower() in available_datasets:\n",
    "  dir = available_datasets[dataset.lower()]\n",
    "  T_matrix = T_matrices[dataset.lower()]\n",
    "  X_train, y_train, X_test, y_test = load_data(dir)\n",
    "  print(f\"Dataset {dataset} loaded.\\n\")\n",
    "\n",
    "  print(f\"Shape Xtr: {X_train.shape}\")\n",
    "  print(f\"Shape Str: {y_train.shape}\")\n",
    "  print(f\"Shape Xts: {X_test.shape}\")\n",
    "  print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "  print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "else:\n",
    "  print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBwG6kxlOg1B"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed_torch()\n",
    "\n",
    "# Clean cache\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Define parameters\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "use_pretrained = False\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276912,
     "status": "ok",
     "timestamp": 1605607527490,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "aZzHvs7COg1G",
    "outputId": "f3946118-5311-4f90-9f64-fd76b30b8a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Split: 0.2\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                   y_train=y_train, \n",
    "                                                   X_test=X_test, \n",
    "                                                   y_test=y_test, \n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 490929,
     "status": "ok",
     "timestamp": 1605607741518,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "4zCmO5aTOg1K",
    "outputId": "cce26e5b-d845-42ce-9f7d-27b65f34d98c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0928 Acc: 0.4232\n",
      "val Loss: 1.0554 Acc: 0.4553\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0721 Acc: 0.4503\n",
      "val Loss: 1.0974 Acc: 0.4300\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0614 Acc: 0.4579\n",
      "val Loss: 1.0779 Acc: 0.4608\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0517 Acc: 0.4674\n",
      "val Loss: 1.0752 Acc: 0.4411\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0416 Acc: 0.4734\n",
      "val Loss: 1.0834 Acc: 0.4397\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0261 Acc: 0.4854\n",
      "val Loss: 1.1318 Acc: 0.4397\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0131 Acc: 0.4953\n",
      "val Loss: 1.0924 Acc: 0.4494\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.9917 Acc: 0.5123\n",
      "val Loss: 1.1078 Acc: 0.4306\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.9739 Acc: 0.5284\n",
      "val Loss: 1.1275 Acc: 0.4381\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.9516 Acc: 0.5417\n",
      "val Loss: 1.1512 Acc: 0.4283\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.9277 Acc: 0.5552\n",
      "val Loss: 1.1842 Acc: 0.4258\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9013 Acc: 0.5687\n",
      "val Loss: 1.2382 Acc: 0.4125\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.8759 Acc: 0.5882\n",
      "val Loss: 1.2341 Acc: 0.4086\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.8572 Acc: 0.5978\n",
      "val Loss: 1.2276 Acc: 0.4100\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8106 Acc: 0.6233\n",
      "val Loss: 1.4326 Acc: 0.3994\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.7947 Acc: 0.6314\n",
      "val Loss: 1.3700 Acc: 0.4203\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7757 Acc: 0.6428\n",
      "val Loss: 1.4163 Acc: 0.4128\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7408 Acc: 0.6592\n",
      "val Loss: 1.5447 Acc: 0.4072\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.7170 Acc: 0.6776\n",
      "val Loss: 1.5017 Acc: 0.4117\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6924 Acc: 0.6908\n",
      "val Loss: 1.4148 Acc: 0.4253\n",
      "\n",
      "Best val Acc: 0.460833\n"
     ]
    }
   ],
   "source": [
    "# If GPU: send the model to GPU\n",
    "model_greyscaleResNet = model_greyscaleResNet.to(device)\n",
    "\n",
    "# Generate a list of parameters to learn based on feature_extraction or pretrained model\n",
    "params_to_update = model_greyscaleResNet.parameters()\n",
    "if feature_extract:\n",
    "  params_to_update = []\n",
    "  for name,param in model_greyscaleResNet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "      params_to_update.append(param)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and Evaluate model\n",
    "model_greyscaleResNet, hist = train_model(model_greyscaleResNet, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXcw5ZAyOg1N"
   },
   "source": [
    "As we did previously, we need to evaluate our entire training set using the previous model. We set the `val_split=0.0` to return the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494583,
     "status": "ok",
     "timestamp": 1605607745183,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "SLhmUdOROg1O",
    "outputId": "e468461c-9324-41bc-b224-3522a04ccec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Validation Split Inserted - Returning the entire dataset\n",
      "\n",
      "Accuracy: 0.4845\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                   y_train=y_train, \n",
    "                                                   X_test=X_test, \n",
    "                                                   y_test=y_test, \n",
    "                                                   val_split=0.0,\n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "y_true, y_pred, acc, outputs = prediction(model_greyscaleResNet, train_loader, device)\n",
    "\n",
    "print(f\"\\nAccuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoXORq4ZOg1T"
   },
   "source": [
    "As we did before, we estimate the transition matrix using the function `estimate_tmatrix()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494574,
     "status": "ok",
     "timestamp": 1605607745184,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "65pUaiHXOg1U",
    "outputId": "6d82cf32-e99d-42b9-978e-ef9a6c2e7f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Transition Matrix: \n",
      "[[0.54578483 0.22476467 0.2294507 ]\n",
      " [0.23077437 0.48880059 0.28042436]\n",
      " [0.28578123 0.21305765 0.50116193]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate transition matrix\n",
    "estimated_T_fashionmnist05 = estimate_tmatrix(num_classes=num_classes, \n",
    "                                              outputs=outputs.cpu().numpy(),\n",
    "                                              y_pred=y_pred)\n",
    "\n",
    "print(f\"Estimated Transition Matrix: \\n{estimated_T_fashionmnist05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1OsT2OIOg1b"
   },
   "source": [
    "The transition matrices estimated using our estimator are very closed to the true transition matrix. It is possible to calculate the difference using the sum average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494565,
     "status": "ok",
     "timestamp": 1605607745185,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "zhv4N2teOg1c",
    "outputId": "1d5e40d9-a890-4131-e949-4855bccc34b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.15861124793688455\n"
     ]
    }
   ],
   "source": [
    "T_true = T_matrix.cpu().numpy()\n",
    "\n",
    "# Calculate differences between estimated_T_argmax and T_true\n",
    "error = np.sum(np.abs(estimated_T_fashionmnist05-T_true)) / np.sum(np.abs(T_true))\n",
    "\n",
    "print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_ajs0kGG8Q5"
   },
   "source": [
    "#### 3.1.5.- Second Training Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qzwbODuW2m8"
   },
   "source": [
    "This stage contains 2 steps:\n",
    "\n",
    "- **Revision=False:** In this step we train the neural network and we initialize the transition matrix as we did before.\n",
    "\n",
    "- **Revision=True:** Then, we propagate and update the weights to learn the correction layer.\n",
    "\n",
    "You will find a detailed explanation of how this works in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494554,
     "status": "ok",
     "timestamp": 1605607745185,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "Bkk_aeJtGO-l",
    "outputId": "0b5f5e13-4043-489c-d46c-9f8298b2386b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cifar loaded.\n",
      "\n",
      "Shape Xtr: (15000, 3, 32, 32)\n",
      "Shape Str: (15000,)\n",
      "Shape Xts: (3000, 3, 32, 32)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Reading data - You need to add the path to the folder with the datasets here.\n",
    "dataset = \"cifar\"\n",
    "\n",
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "if dataset.lower() in available_datasets:\n",
    "  dir = available_datasets[dataset.lower()]\n",
    "  T_matrix = T_matrices[dataset.lower()]\n",
    "  X_train, y_train, X_test, y_test = load_data(dir)\n",
    "  print(f\"Dataset {dataset} loaded.\\n\")\n",
    "\n",
    "  print(f\"Shape Xtr: {X_train.shape}\")\n",
    "  print(f\"Shape Str: {y_train.shape}\")\n",
    "  print(f\"Shape Xts: {X_test.shape}\")\n",
    "  print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "  print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "else:\n",
    "  print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TScXyyNvoCgK"
   },
   "source": [
    "In this case, we need to define the architecture of the ResNet model from scratch because we need to initialize an correction layer $\\Delta T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkV89Pikjuod"
   },
   "outputs": [],
   "source": [
    "# Convolution Block\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Main Block ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        self.delta_T = nn.Linear(num_classes, num_classes, False)\n",
    "        nn.init.zeros_(self.delta_T.weight)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, revision=False, plot_deltaT=False):\n",
    "        correction = self.delta_T.weight\n",
    "        if plot_deltaT:\n",
    "          print(\"Corr\", correction)\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        if revision:\n",
    "            return out, correction\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIWUZzDopsr2"
   },
   "source": [
    "Now, we define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8q6qnHDWil6"
   },
   "outputs": [],
   "source": [
    "class ReweightingLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super(ReweightingLoss, self).__init__()\n",
    "      \n",
    "    def forward(self, out, T, target, revision=None):\n",
    "      loss = 0.0\n",
    "      out_softmax = F.softmax(out, dim=1) \n",
    "      # If standard mode (revision false)\n",
    "      if revision is None:\n",
    "        g_Y = out_softmax.gather(1, target.view(-1,1))\n",
    "        Tg = torch.mm(T.t(), out_softmax.t())\n",
    "        Tg_Y = Tg.t().gather(1, target.view(-1,1))\n",
    "        # requires_grad=True indicates that we want to compute gradients with\n",
    "        # respect to these Variables during the backward pass.\n",
    "        beta = torch.nn.Parameter(g_Y / Tg_Y)\n",
    "        loss = F.cross_entropy(out, target, reduction='none')\n",
    "        loss = beta.view(1,-1) * loss\n",
    "        return torch.mean(loss)\n",
    "\n",
    "      # If revision mode\n",
    "      else:\n",
    "        len_t = len(target)\n",
    "        delta_T = revision\n",
    "        for i in range(len_t):\n",
    "            # Add new dimension\n",
    "            soft_iter = out_softmax[i].unsqueeze(0)\n",
    "            # Adding correction deltaT to transition matrix\n",
    "            T = T + delta_T\n",
    "            # Matrix product\n",
    "            out_T_transpose = torch.mm(T.t(), soft_iter.t()).t()\n",
    "            g_Y = soft_iter[:, target[i]]\n",
    "            Tg_Y = out_T_transpose[:, target[i]]\n",
    "            # Update beta\n",
    "            beta = (g_Y/Tg_Y)\n",
    "            cross_loss = F.cross_entropy(out[i].unsqueeze(0), target[i].unsqueeze(0))\n",
    "            # Sum loss\n",
    "            loss += beta * cross_loss\n",
    "        return loss / len_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-zvnut8Ia6F"
   },
   "outputs": [],
   "source": [
    "def train_model_forward(model, dataloaders, criterion, optimizer, T_matrix, with_revision=False, num_epochs=25):\n",
    "  \"\"\"\n",
    "  Function to train the model\n",
    "  \n",
    "  :param model: Model\n",
    "  :param dataloaders: DataLoader\n",
    "  :param criterion: Loss function\n",
    "  :param optimizer: Optimizer\n",
    "  :param with_revision: True or False\n",
    "  :param num_epochs: Number of epochs\n",
    "  :return model: Trained model\n",
    "  \"\"\"\n",
    "\n",
    "  val_acc_history = []\n",
    "  revision_evl = []\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_acc = 0.0\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "      print('-' * 10)\n",
    "\n",
    "      # Each epoch has a training and validation phase\n",
    "      for phase in ['train', 'val']:\n",
    "          if phase == 'train':\n",
    "              model.train()  # Set model to training mode\n",
    "          else:\n",
    "              model.eval()   # Set model to evaluate mode\n",
    "\n",
    "          running_loss = 0.0\n",
    "          running_corrects = 0\n",
    "\n",
    "          # Iterate over data.\n",
    "          for b, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "              inputs = inputs.to(device)\n",
    "              labels = labels.to(device)\n",
    "\n",
    "              # zero the parameter gradients\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "              # forward\n",
    "              if with_revision==True:\n",
    "                out, revision = model(inputs, revision=with_revision)\n",
    "                loss = criterion(out, T_matrix, labels, revision)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                out = torch.matmul((T_matrix+revision).t(), prob.t())\n",
    "              else:\n",
    "                out = model(inputs)\n",
    "                loss = criterion(out, T_matrix, labels)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                out = torch.matmul(T_matrix.t(), prob.t())\n",
    "                \n",
    "              out = out.t()\n",
    "\n",
    "              # track history if only in train\n",
    "              if with_revision==False:\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                  \n",
    "                  _, preds = torch.max(out, 1)\n",
    "                  \n",
    "                  # backward + optimize only if in training phase\n",
    "                  if phase == 'train':\n",
    "                      loss.backward()\n",
    "                      optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "              else:\n",
    "                _, preds = torch.max(out, 1)\n",
    "\n",
    "                if phase == 'train':\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if abs(loss.item()* inputs.size(0)) < 0.02:\n",
    "                  break\n",
    "\n",
    "          epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "          epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "          if with_revision==True:\n",
    "            revision_evl.append(revision)\n",
    "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "          # deep copy the model\n",
    "          if phase == 'val' and epoch_acc > best_acc:\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "          if phase == 'val':\n",
    "              val_acc_history.append(epoch_acc)\n",
    "\n",
    "      print()\n",
    "\n",
    "  print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "  # load best model weights\n",
    "  model.load_state_dict(best_model_wts)\n",
    "  return model, val_acc_history, revision_evl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz2xUkpdI28k"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed_torch()\n",
    "\n",
    "# Clean cache\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Define parameters\n",
    "num_classes = 3\n",
    "batch_size = 100\n",
    "num_epochs = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAY6YSAUCFhU"
   },
   "source": [
    "We define our `train_loader`, `val_loader` and `test_loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494810,
     "status": "ok",
     "timestamp": 1605607745477,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "VuxFiTjoI3f0",
    "outputId": "2619c1ad-962c-4ae5-d058-cdf276af74f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Split: 0.2\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                   y_train=y_train, \n",
    "                                                   X_test=X_test, \n",
    "                                                   y_test=y_test, \n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MScT13KtCKPr"
   },
   "source": [
    "Initialize and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 663291,
     "status": "ok",
     "timestamp": 1605607913968,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "LYTNznlNEQ9B",
    "outputId": "7327b7fe-5e35-4ffe-8316-3167e92e4e2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.8614 Acc: 0.3600\n",
      "val Loss: 0.7707 Acc: 0.3833\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.7962 Acc: 0.3762\n",
      "val Loss: 0.7684 Acc: 0.3787\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.8228 Acc: 0.3765\n",
      "val Loss: 0.7864 Acc: 0.3733\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.7887 Acc: 0.3782\n",
      "val Loss: 0.7844 Acc: 0.3603\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.8108 Acc: 0.3840\n",
      "val Loss: 0.7609 Acc: 0.3657\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.7786 Acc: 0.3905\n",
      "val Loss: 0.7498 Acc: 0.3743\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.7603 Acc: 0.3888\n",
      "val Loss: 0.7352 Acc: 0.3723\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.7235 Acc: 0.3964\n",
      "val Loss: 0.7992 Acc: 0.3743\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.7543 Acc: 0.3913\n",
      "val Loss: 0.7684 Acc: 0.3773\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.7224 Acc: 0.3908\n",
      "val Loss: 0.7463 Acc: 0.3767\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.7331 Acc: 0.3933\n",
      "val Loss: 0.7583 Acc: 0.3747\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.7226 Acc: 0.4017\n",
      "val Loss: 0.7426 Acc: 0.3833\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.7086 Acc: 0.4028\n",
      "val Loss: 0.7126 Acc: 0.3843\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.6804 Acc: 0.4045\n",
      "val Loss: 0.6505 Acc: 0.3730\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.6681 Acc: 0.4113\n",
      "val Loss: 0.6658 Acc: 0.3797\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.6492 Acc: 0.4133\n",
      "val Loss: 0.6678 Acc: 0.3830\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.6300 Acc: 0.4162\n",
      "val Loss: 0.6069 Acc: 0.3813\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.6030 Acc: 0.4286\n",
      "val Loss: 0.5428 Acc: 0.3793\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.5722 Acc: 0.4305\n",
      "val Loss: 0.5609 Acc: 0.3810\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.5491 Acc: 0.4414\n",
      "val Loss: 0.5408 Acc: 0.3693\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.5258 Acc: 0.4447\n",
      "val Loss: 0.5257 Acc: 0.3717\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.5231 Acc: 0.4457\n",
      "val Loss: 0.4929 Acc: 0.3947\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.5048 Acc: 0.4566\n",
      "val Loss: 0.4795 Acc: 0.3827\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.4651 Acc: 0.4702\n",
      "val Loss: 0.4446 Acc: 0.3797\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.4488 Acc: 0.4793\n",
      "val Loss: 0.4538 Acc: 0.3767\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.4187 Acc: 0.4925\n",
      "val Loss: 0.4696 Acc: 0.3833\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.3934 Acc: 0.5055\n",
      "val Loss: 0.3447 Acc: 0.3660\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.3776 Acc: 0.5118\n",
      "val Loss: 0.3374 Acc: 0.3883\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.3441 Acc: 0.5285\n",
      "val Loss: 0.3564 Acc: 0.3800\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.3345 Acc: 0.5446\n",
      "val Loss: 0.3313 Acc: 0.3730\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.3160 Acc: 0.5536\n",
      "val Loss: 0.3462 Acc: 0.3817\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.2911 Acc: 0.5742\n",
      "val Loss: 0.2792 Acc: 0.3830\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.2727 Acc: 0.5867\n",
      "val Loss: 0.2757 Acc: 0.3583\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.2625 Acc: 0.5996\n",
      "val Loss: 0.3377 Acc: 0.3660\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.2407 Acc: 0.6159\n",
      "val Loss: 0.2375 Acc: 0.3513\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.2295 Acc: 0.6296\n",
      "val Loss: 0.2512 Acc: 0.3647\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.2077 Acc: 0.6512\n",
      "val Loss: 0.2542 Acc: 0.3633\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.2021 Acc: 0.6563\n",
      "val Loss: 0.2583 Acc: 0.3600\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1899 Acc: 0.6727\n",
      "val Loss: 0.2886 Acc: 0.3623\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.6893\n",
      "val Loss: 0.2364 Acc: 0.3690\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1576 Acc: 0.6969\n",
      "val Loss: 0.2409 Acc: 0.3533\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1508 Acc: 0.7147\n",
      "val Loss: 0.2346 Acc: 0.3577\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1407 Acc: 0.7210\n",
      "val Loss: 0.2240 Acc: 0.3600\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1397 Acc: 0.7253\n",
      "val Loss: 0.2229 Acc: 0.3620\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1263 Acc: 0.7382\n",
      "val Loss: 0.2165 Acc: 0.3643\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1253 Acc: 0.7414\n",
      "val Loss: 0.2313 Acc: 0.3487\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1145 Acc: 0.7525\n",
      "val Loss: 0.2317 Acc: 0.3613\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1080 Acc: 0.7616\n",
      "val Loss: 0.2253 Acc: 0.3497\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1037 Acc: 0.7706\n",
      "val Loss: 0.1921 Acc: 0.3710\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1071 Acc: 0.7737\n",
      "val Loss: 0.2003 Acc: 0.3547\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1008 Acc: 0.7789\n",
      "val Loss: 0.1961 Acc: 0.3520\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0974 Acc: 0.7823\n",
      "val Loss: 0.1952 Acc: 0.3677\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.8003\n",
      "val Loss: 0.1834 Acc: 0.3423\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.7961\n",
      "val Loss: 0.1728 Acc: 0.3507\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0837 Acc: 0.8037\n",
      "val Loss: 0.1857 Acc: 0.3543\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0805 Acc: 0.8116\n",
      "val Loss: 0.1811 Acc: 0.3603\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0762 Acc: 0.8105\n",
      "val Loss: 0.1797 Acc: 0.3570\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0759 Acc: 0.8156\n",
      "val Loss: 0.1634 Acc: 0.3347\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0687 Acc: 0.8210\n",
      "val Loss: 0.1675 Acc: 0.3497\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0710 Acc: 0.8221\n",
      "val Loss: 0.1667 Acc: 0.3640\n",
      "\n",
      "Best val Acc: 0.394667\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model_modified_resnet = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "\n",
    "# If GPU: send the model to GPU\n",
    "model_modified_resnet = model_modified_resnet.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer_ft = optim.Adam(model_modified_resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = ReweightingLoss()\n",
    "\n",
    "# Train and Evaluate model\n",
    "model_modified_resnet, hist, revision_ls = train_model_forward(model_modified_resnet, \n",
    "                                                               dataloaders_dict, \n",
    "                                                               criterion, \n",
    "                                                               optimizer_ft, \n",
    "                                                               T_matrix, \n",
    "                                                               with_revision=False, \n",
    "                                                               num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720669,
     "status": "ok",
     "timestamp": 1605607971356,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "tl7d4BJvDC7Z",
    "outputId": "5c29cd78-cdd3-486e-df9c-dd69c47529b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.1080 Acc: 0.4399\n",
      "val Loss: 0.0447 Acc: 0.3863\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0340 Acc: 0.4132\n",
      "val Loss: 0.0275 Acc: 0.3873\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0232 Acc: 0.4041\n",
      "val Loss: 0.0211 Acc: 0.3863\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0174 Acc: 0.3997\n",
      "val Loss: 0.0173 Acc: 0.3850\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0153 Acc: 0.3935\n",
      "val Loss: 0.0153 Acc: 0.3780\n",
      "\n",
      "Best val Acc: 0.387333\n"
     ]
    }
   ],
   "source": [
    "# Number epochs for revision\n",
    "num_epochs = 5\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = ReweightingLoss()\n",
    "\n",
    "# Train and Evaluate model\n",
    "model_modified_resnet, hist, revision_ls = train_model_forward(model_modified_resnet, \n",
    "                                                               dataloaders_dict, \n",
    "                                                               criterion, \n",
    "                                                               optimizer_ft, \n",
    "                                                               T_matrix, \n",
    "                                                               with_revision=True, \n",
    "                                                               num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6CyeAA07C0n"
   },
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720659,
     "status": "ok",
     "timestamp": 1605607971357,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "LJxaI7Yt7bLc",
    "outputId": "0adf0953-f698-42f4-8f17-95a4ac5e5e9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on test set: 0.6263333333333333\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred, acc, outputs = prediction(model_modified_resnet, test_loader, device, revision=True)\n",
    "\n",
    "print(f\"\\nAccuracy on test set: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJtE1DgSf45g"
   },
   "source": [
    "The variable `revision_ls` contains the correction $\\Delta T$ thus, now we can show the transition matrix using $T + \\Delta T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720650,
     "status": "ok",
     "timestamp": 1605607971358,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "PM-cHcYzf0Ov",
    "outputId": "8eff28f1-5802-4dc4-bd4d-6274784e6fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated T_matrix for Cifar10: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "\n",
      "T_matrix + DeltaT for Cifar10: \n",
      "tensor([[0.4756, 0.3420, 0.2904],\n",
      "        [0.3357, 0.5141, 0.2964],\n",
      "        [0.3133, 0.3354, 0.4695]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Estimated T_matrix for Cifar10: \\n{T_matrix}\\n\\n\")\n",
    "\n",
    "print(f\"T_matrix + DeltaT for Cifar10: \\n{T_matrix + revision_ls[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnZozW1LnLww"
   },
   "source": [
    "### 3.2 Forward\n",
    "\n",
    "In this section, we will implement a classification using the Forward Method to make it robust to noisy label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720641,
     "status": "ok",
     "timestamp": 1605607971359,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "0q25eAWcnlo5",
    "outputId": "b347fe56-574d-48b2-ca81-f396e2146606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset fashionmnist0.6 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Reading data - You need to add the path to the folder with the datasets here.\n",
    "dataset = \"fashionmnist0.6\"\n",
    "\n",
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "if dataset.lower() in available_datasets:\n",
    "  dir = available_datasets[dataset.lower()]\n",
    "  T_matrix = T_matrices[dataset.lower()]\n",
    "  X_train, y_train, X_test, y_test = load_data(dir)\n",
    "  print(f\"Dataset {dataset} loaded.\\n\")\n",
    "\n",
    "  print(f\"Shape Xtr: {X_train.shape}\")\n",
    "  print(f\"Shape Str: {y_train.shape}\")\n",
    "  print(f\"Shape Xts: {X_test.shape}\")\n",
    "  print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "  print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "else:\n",
    "  print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCsjJs7pn5pw"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, T=None):\n",
    "  \"\"\"\n",
    "  Function to train the model\n",
    "  \n",
    "  :param model: Model\n",
    "  :param T: Transition Matrix\n",
    "  :param dataloaders: DataLoader\n",
    "  :param criterion: Loss function\n",
    "  :param optimizer: Optimizer\n",
    "  :param num_epochs: Number of epochs\n",
    "  :return model: Trained model\n",
    "  \"\"\"\n",
    "\n",
    "  val_acc_history = []\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_acc = 0.0\n",
    "\n",
    "  print(f\"Transition Matrix: \\n{T}\\n\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "      print('-' * 10)\n",
    "\n",
    "      # Each epoch has a training and validation phase\n",
    "      for phase in ['train', 'val']:\n",
    "          if phase == 'train':\n",
    "              model.train()  # Set model to training mode\n",
    "          else:\n",
    "              model.eval()   # Set model to evaluate mode\n",
    "\n",
    "          running_loss = 0.0\n",
    "          running_corrects = 0\n",
    "    \n",
    "          # Iterate over data.\n",
    "          for inputs, labels in dataloaders[phase]:\n",
    "              inputs = inputs.to(device)\n",
    "              labels = labels.to(device)\n",
    "\n",
    "              # zero the parameter gradients\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "              # forward\n",
    "              # track history if only in train\n",
    "              with torch.set_grad_enabled(phase == 'train'):\n",
    "                  outputs = model(inputs)\n",
    "                  loss = criterion(outputs, labels)\n",
    "                  \n",
    "                  if torch.is_tensor(T):\n",
    "                    prob = F.softmax(outputs, dim=1)\n",
    "                    prob = prob.t()\n",
    "                    out_forward = torch.matmul(T.t(), prob)\n",
    "                    out_forward = out_forward.t()\n",
    "\n",
    "                    _, preds = torch.max(out_forward, 1)\n",
    "                  else:\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                  # backward + optimize only if in training phase\n",
    "                  if phase == 'train':\n",
    "                      loss.backward()\n",
    "                      optimizer.step()\n",
    "\n",
    "              # statistics\n",
    "              running_loss += loss.item() * inputs.size(0)\n",
    "              running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "          epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "          epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "          # deep copy the model\n",
    "          if phase == 'val' and epoch_acc > best_acc:\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "          if phase == 'val':\n",
    "              val_acc_history.append(epoch_acc)\n",
    "\n",
    "      print()\n",
    "\n",
    "  print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "  # load best model weights\n",
    "  model.load_state_dict(best_model_wts)\n",
    "  return model, val_acc_history\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "  \"\"\"\n",
    "  Function to define if feature extracting\n",
    "  \n",
    "  :param model: Model\n",
    "  :param feature_extracting: Boolean to indicate if perform feature extraction\n",
    "  \"\"\"\n",
    "  if feature_extracting:\n",
    "      for param in model.parameters():\n",
    "          param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqnACAxroWcb"
   },
   "source": [
    "In this case, we implemented the following Convolutional Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Zxuh5IQV4lx"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes, n_channels, n_filters):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_channels, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64*n_filters*n_filters, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ9DYL95Yoda"
   },
   "source": [
    "Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBhBWichrGT7"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed_torch()\n",
    "\n",
    "# Clean cache\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Define parameters\n",
    "if dataset.lower() == 'cifar':\n",
    "  n_channels = 3\n",
    "  n_filters = 7\n",
    "else:\n",
    "  n_channels = 1\n",
    "  n_filters = 6\n",
    "\n",
    "num_classes = 3\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize ResNet model\n",
    "model_cnn_fashion06 = CNN(num_classes=num_classes, n_channels=n_channels, n_filters=n_filters).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_cnn_fashion06.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Detect if GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU1ooh8BoPfO"
   },
   "source": [
    "Now we need to generate our Training, Validation and Testing DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720948,
     "status": "ok",
     "timestamp": 1605607971696,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "N_WTScuZoDR1",
    "outputId": "45328ae1-7caa-4e47-cc41-f3bef189f761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Split: 0.2\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                   y_train=y_train, \n",
    "                                                   X_test=X_test, \n",
    "                                                   y_test=y_test, \n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHVFbxhkpsMj"
   },
   "source": [
    "We train the neural network using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740399,
     "status": "ok",
     "timestamp": 1605607991158,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "EZW-6IuKoc4h",
    "outputId": "7afbbddb-dd21-40f0-e0a7-a6a67dc65909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1028 Acc: 0.3576\n",
      "val Loss: 1.0951 Acc: 0.3594\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0947 Acc: 0.3765\n",
      "val Loss: 1.0975 Acc: 0.3561\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0926 Acc: 0.3867\n",
      "val Loss: 1.0911 Acc: 0.3894\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0913 Acc: 0.3866\n",
      "val Loss: 1.0926 Acc: 0.3983\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0896 Acc: 0.3875\n",
      "val Loss: 1.0925 Acc: 0.3869\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0879 Acc: 0.3886\n",
      "val Loss: 1.0924 Acc: 0.3978\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0875 Acc: 0.3940\n",
      "val Loss: 1.0913 Acc: 0.3897\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0839 Acc: 0.3990\n",
      "val Loss: 1.0940 Acc: 0.4008\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0831 Acc: 0.4040\n",
      "val Loss: 1.0926 Acc: 0.3975\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0827 Acc: 0.3992\n",
      "val Loss: 1.0941 Acc: 0.3878\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0804 Acc: 0.4033\n",
      "val Loss: 1.0926 Acc: 0.3969\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0803 Acc: 0.4046\n",
      "val Loss: 1.0938 Acc: 0.3928\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0792 Acc: 0.4110\n",
      "val Loss: 1.0961 Acc: 0.3883\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0783 Acc: 0.4119\n",
      "val Loss: 1.0960 Acc: 0.3894\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0751 Acc: 0.4126\n",
      "val Loss: 1.0944 Acc: 0.3878\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0743 Acc: 0.4149\n",
      "val Loss: 1.0943 Acc: 0.3869\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0705 Acc: 0.4244\n",
      "val Loss: 1.0974 Acc: 0.3906\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0719 Acc: 0.4195\n",
      "val Loss: 1.0980 Acc: 0.3856\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0661 Acc: 0.4278\n",
      "val Loss: 1.0999 Acc: 0.3847\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0683 Acc: 0.4281\n",
      "val Loss: 1.1017 Acc: 0.3792\n",
      "\n",
      "Best val Acc: 0.400833\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate model\n",
    "model_cnn_fashion06, hist = train_model(model_cnn_fashion06, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs, T=T_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 741011,
     "status": "ok",
     "timestamp": 1605607991782,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "e-zKjW8_puzt",
    "outputId": "6efba22b-f1b1-466d-8469-6951b1cf56e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using Forward Method: 0.852\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred, acc, outputs = prediction(model_cnn_fashion06, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy on test set using Forward Method: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4MyVnZ-xQkg"
   },
   "source": [
    "## 4.- Experiments\n",
    "\n",
    "This section contains the experiments performed using the different classifier implemented before. In this case, we will test our algorithms in 3 datasets:\n",
    "\n",
    "#### **FashionMINIST0.5:**\n",
    "\n",
    "Number of the training and validation examples n = 18000.\n",
    "\n",
    "Number of the test examples m = 3000.\n",
    "\n",
    "The shape of each example image shape = (28 × 28).\n",
    "\n",
    "$$ T=\\begin{bmatrix}\n",
    "0.5 & 0.2 & 0.3\\\\\n",
    "0.3 & 0.5 & 0.2\\\\\n",
    "0.2 & 0.3 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **FashionMINIST0.6**\n",
    "\n",
    "Number of the training and validation examples n = 18000.\n",
    "\n",
    "Number of the test examples m = 3000.\n",
    "\n",
    "The shape of each example image shape = (28 × 28).\n",
    "\n",
    "The transition matrix:\n",
    "\n",
    "$$ T=\\begin{bmatrix}\n",
    "0.4 & 0.3 & 0.3\\\\\n",
    "0.3 & 0.4 & 0.3\\\\\n",
    "0.3 & 0.3 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **CIFAR**\n",
    "\n",
    "Number of the training and validation examples n = 30000.\n",
    "\n",
    "Number of the test examples m = 3000.\n",
    "\n",
    "The shape of each example image shape = (32 × 32 × 3).\n",
    "\n",
    "The transition matrix T was not provided but we have estimated it using T-Revision method.\n",
    "\n",
    "To correctly measure the performance of our classifiers, **we will train each classifier 10 times with the different training and validation sets generated by random sampling.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6LiDQxbEXx2"
   },
   "source": [
    "### 4.1.- Classifier using Forward Method\n",
    "\n",
    "Next cell is responsible to train the different classifiers and store the results in the variable called `test_acc_forward_results`. We iterate over the number of times we want to train the models (in this case 10 times) and then we iterave over the different datasets + transition matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b7djkgtHpFt"
   },
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrkUVcXBcVxb"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, T=None):\n",
    "  \"\"\"\n",
    "  Function to train the model\n",
    "  \n",
    "  :param model: Model\n",
    "  :param T: Transition Matrix\n",
    "  :param dataloaders: DataLoader\n",
    "  :param criterion: Loss function\n",
    "  :param optimizer: Optimizer\n",
    "  :param num_epochs: Number of epochs\n",
    "  :return model: Trained model\n",
    "  \"\"\"\n",
    "\n",
    "  val_acc_history = []\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_acc = 0.0\n",
    "\n",
    "  print(f\"Transition Matrix: \\n{T}\\n\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "      print('-' * 10)\n",
    "\n",
    "      # Each epoch has a training and validation phase\n",
    "      for phase in ['train', 'val']:\n",
    "          if phase == 'train':\n",
    "              model.train()  # Set model to training mode\n",
    "          else:\n",
    "              model.eval()   # Set model to evaluate mode\n",
    "\n",
    "          running_loss = 0.0\n",
    "          running_corrects = 0\n",
    "    \n",
    "          # Iterate over data.\n",
    "          for inputs, labels in dataloaders[phase]:\n",
    "              inputs = inputs.to(device)\n",
    "              labels = labels.to(device)\n",
    "\n",
    "              # zero the parameter gradients\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "              # forward\n",
    "              # track history if only in train\n",
    "              with torch.set_grad_enabled(phase == 'train'):\n",
    "                  outputs = model(inputs)\n",
    "                  loss = criterion(outputs, labels)\n",
    "                  \n",
    "                  if torch.is_tensor(T):\n",
    "                    prob = F.softmax(outputs, dim=1)\n",
    "                    prob = prob.t()\n",
    "                    out_forward = torch.matmul(T.t(), prob)\n",
    "                    out_forward = out_forward.t()\n",
    "\n",
    "                    _, preds = torch.max(out_forward, 1)\n",
    "                  else:\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                  # backward + optimize only if in training phase\n",
    "                  if phase == 'train':\n",
    "                      loss.backward()\n",
    "                      optimizer.step()\n",
    "\n",
    "              # statistics\n",
    "              running_loss += loss.item() * inputs.size(0)\n",
    "              running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "          epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "          epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "          # deep copy the model\n",
    "          if phase == 'val' and epoch_acc > best_acc:\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "          if phase == 'val':\n",
    "              val_acc_history.append(epoch_acc)\n",
    "\n",
    "      print()\n",
    "\n",
    "  print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "  # load best model weights\n",
    "  model.load_state_dict(best_model_wts)\n",
    "  return model, val_acc_history\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "  \"\"\"\n",
    "  Function to define if feature extracting\n",
    "  \n",
    "  :param model: Model\n",
    "  :param feature_extracting: Boolean to indicate if perform feature extraction\n",
    "  \"\"\"\n",
    "  if feature_extracting:\n",
    "      for param in model.parameters():\n",
    "          param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phbsTYisLFBm"
   },
   "source": [
    "#### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1342780,
     "status": "ok",
     "timestamp": 1605608593570,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "AKTmOuxgErA1",
    "outputId": "b8c455e5-6597-49e7-e179-60005d8870fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset cifar loaded.\n",
      "\n",
      "Shape Xtr: (15000, 3, 32, 32)\n",
      "Shape Str: (15000,)\n",
      "Shape Xts: (3000, 3, 32, 32)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "..............................\n",
      "\n",
      "Training iteration: 0\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1015 Acc: 0.3364\n",
      "val Loss: 1.0985 Acc: 0.3560\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0944 Acc: 0.3438\n",
      "val Loss: 1.0948 Acc: 0.3573\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0899 Acc: 0.3513\n",
      "val Loss: 1.0929 Acc: 0.3587\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0840 Acc: 0.3534\n",
      "val Loss: 1.0979 Acc: 0.3603\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0823 Acc: 0.3546\n",
      "val Loss: 1.0952 Acc: 0.3553\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0772 Acc: 0.3598\n",
      "val Loss: 1.0963 Acc: 0.3543\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0750 Acc: 0.3661\n",
      "val Loss: 1.0976 Acc: 0.3640\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0732 Acc: 0.3659\n",
      "val Loss: 1.0961 Acc: 0.3583\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0672 Acc: 0.3770\n",
      "val Loss: 1.1010 Acc: 0.3600\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0670 Acc: 0.3756\n",
      "val Loss: 1.1062 Acc: 0.3523\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0619 Acc: 0.3843\n",
      "val Loss: 1.1057 Acc: 0.3570\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0565 Acc: 0.3937\n",
      "val Loss: 1.1089 Acc: 0.3643\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0551 Acc: 0.3917\n",
      "val Loss: 1.1073 Acc: 0.3550\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0495 Acc: 0.4009\n",
      "val Loss: 1.1079 Acc: 0.3527\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0445 Acc: 0.4066\n",
      "val Loss: 1.1165 Acc: 0.3513\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0421 Acc: 0.4218\n",
      "val Loss: 1.1243 Acc: 0.3587\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0349 Acc: 0.4210\n",
      "val Loss: 1.1301 Acc: 0.3647\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0294 Acc: 0.4341\n",
      "val Loss: 1.1230 Acc: 0.3610\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0246 Acc: 0.4374\n",
      "val Loss: 1.1326 Acc: 0.3583\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0187 Acc: 0.4494\n",
      "val Loss: 1.1284 Acc: 0.3647\n",
      "\n",
      "Best val Acc: 0.364667\n",
      "Accuracy on test set using Forward Method: 0.4693333333333333\n",
      "\n",
      "\n",
      "Training iteration: 1\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1015 Acc: 0.3354\n",
      "val Loss: 1.0971 Acc: 0.3620\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0916 Acc: 0.3453\n",
      "val Loss: 1.0911 Acc: 0.3620\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0868 Acc: 0.3467\n",
      "val Loss: 1.0957 Acc: 0.3667\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0835 Acc: 0.3563\n",
      "val Loss: 1.0931 Acc: 0.3620\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0780 Acc: 0.3594\n",
      "val Loss: 1.0921 Acc: 0.3590\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0753 Acc: 0.3625\n",
      "val Loss: 1.0953 Acc: 0.3663\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0711 Acc: 0.3733\n",
      "val Loss: 1.0976 Acc: 0.3667\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0668 Acc: 0.3719\n",
      "val Loss: 1.1017 Acc: 0.3617\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0634 Acc: 0.3878\n",
      "val Loss: 1.0994 Acc: 0.3643\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0616 Acc: 0.3847\n",
      "val Loss: 1.1047 Acc: 0.3583\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0539 Acc: 0.3950\n",
      "val Loss: 1.1085 Acc: 0.3593\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0473 Acc: 0.3988\n",
      "val Loss: 1.1162 Acc: 0.3597\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0431 Acc: 0.4163\n",
      "val Loss: 1.1118 Acc: 0.3570\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0361 Acc: 0.4216\n",
      "val Loss: 1.1247 Acc: 0.3530\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0360 Acc: 0.4233\n",
      "val Loss: 1.1120 Acc: 0.3667\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0302 Acc: 0.4274\n",
      "val Loss: 1.1160 Acc: 0.3677\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0214 Acc: 0.4470\n",
      "val Loss: 1.1218 Acc: 0.3627\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0143 Acc: 0.4481\n",
      "val Loss: 1.1385 Acc: 0.3640\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0042 Acc: 0.4581\n",
      "val Loss: 1.2213 Acc: 0.3413\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0017 Acc: 0.4669\n",
      "val Loss: 1.1357 Acc: 0.3630\n",
      "\n",
      "Best val Acc: 0.367667\n",
      "Accuracy on test set using Forward Method: 0.497\n",
      "\n",
      "\n",
      "Training iteration: 2\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1080 Acc: 0.3330\n",
      "val Loss: 1.0931 Acc: 0.3620\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0936 Acc: 0.3421\n",
      "val Loss: 1.0912 Acc: 0.3637\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0886 Acc: 0.3455\n",
      "val Loss: 1.1054 Acc: 0.3737\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0854 Acc: 0.3543\n",
      "val Loss: 1.0975 Acc: 0.3700\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0800 Acc: 0.3593\n",
      "val Loss: 1.0987 Acc: 0.3650\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0755 Acc: 0.3619\n",
      "val Loss: 1.0956 Acc: 0.3653\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0730 Acc: 0.3690\n",
      "val Loss: 1.0927 Acc: 0.3590\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0706 Acc: 0.3742\n",
      "val Loss: 1.0933 Acc: 0.3590\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0656 Acc: 0.3775\n",
      "val Loss: 1.1005 Acc: 0.3583\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0601 Acc: 0.3807\n",
      "val Loss: 1.1053 Acc: 0.3650\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0594 Acc: 0.3922\n",
      "val Loss: 1.1008 Acc: 0.3567\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0523 Acc: 0.3971\n",
      "val Loss: 1.1041 Acc: 0.3660\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0478 Acc: 0.4076\n",
      "val Loss: 1.1078 Acc: 0.3563\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0398 Acc: 0.4124\n",
      "val Loss: 1.1161 Acc: 0.3673\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0346 Acc: 0.4192\n",
      "val Loss: 1.1094 Acc: 0.3703\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0265 Acc: 0.4347\n",
      "val Loss: 1.1286 Acc: 0.3707\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0221 Acc: 0.4322\n",
      "val Loss: 1.1257 Acc: 0.3597\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0165 Acc: 0.4489\n",
      "val Loss: 1.1365 Acc: 0.3650\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0033 Acc: 0.4604\n",
      "val Loss: 1.1417 Acc: 0.3597\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.9979 Acc: 0.4672\n",
      "val Loss: 1.1505 Acc: 0.3670\n",
      "\n",
      "Best val Acc: 0.373667\n",
      "Accuracy on test set using Forward Method: 0.49833333333333335\n",
      "\n",
      "\n",
      "Training iteration: 3\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1055 Acc: 0.3366\n",
      "val Loss: 1.0982 Acc: 0.3667\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0961 Acc: 0.3453\n",
      "val Loss: 1.0994 Acc: 0.3613\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0888 Acc: 0.3507\n",
      "val Loss: 1.0940 Acc: 0.3647\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0845 Acc: 0.3533\n",
      "val Loss: 1.0918 Acc: 0.3670\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0795 Acc: 0.3585\n",
      "val Loss: 1.0994 Acc: 0.3650\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0771 Acc: 0.3722\n",
      "val Loss: 1.0931 Acc: 0.3520\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0741 Acc: 0.3669\n",
      "val Loss: 1.1022 Acc: 0.3630\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0704 Acc: 0.3775\n",
      "val Loss: 1.0964 Acc: 0.3583\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0661 Acc: 0.3807\n",
      "val Loss: 1.1035 Acc: 0.3633\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0610 Acc: 0.3849\n",
      "val Loss: 1.0968 Acc: 0.3587\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0601 Acc: 0.3882\n",
      "val Loss: 1.0993 Acc: 0.3613\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0548 Acc: 0.3939\n",
      "val Loss: 1.1046 Acc: 0.3700\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0522 Acc: 0.4044\n",
      "val Loss: 1.1067 Acc: 0.3623\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0433 Acc: 0.4094\n",
      "val Loss: 1.1087 Acc: 0.3660\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0386 Acc: 0.4163\n",
      "val Loss: 1.1112 Acc: 0.3623\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0381 Acc: 0.4230\n",
      "val Loss: 1.1180 Acc: 0.3537\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0334 Acc: 0.4284\n",
      "val Loss: 1.1158 Acc: 0.3563\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0228 Acc: 0.4377\n",
      "val Loss: 1.1247 Acc: 0.3647\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0175 Acc: 0.4459\n",
      "val Loss: 1.1278 Acc: 0.3583\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0133 Acc: 0.4545\n",
      "val Loss: 1.1390 Acc: 0.3570\n",
      "\n",
      "Best val Acc: 0.370000\n",
      "Accuracy on test set using Forward Method: 0.495\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1022 Acc: 0.3367\n",
      "val Loss: 1.0945 Acc: 0.3700\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0953 Acc: 0.3463\n",
      "val Loss: 1.0909 Acc: 0.3683\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0879 Acc: 0.3503\n",
      "val Loss: 1.1023 Acc: 0.3780\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0854 Acc: 0.3546\n",
      "val Loss: 1.0927 Acc: 0.3603\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0790 Acc: 0.3617\n",
      "val Loss: 1.0902 Acc: 0.3587\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0768 Acc: 0.3585\n",
      "val Loss: 1.0938 Acc: 0.3653\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0735 Acc: 0.3688\n",
      "val Loss: 1.0939 Acc: 0.3633\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0707 Acc: 0.3696\n",
      "val Loss: 1.0951 Acc: 0.3657\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0667 Acc: 0.3830\n",
      "val Loss: 1.0974 Acc: 0.3597\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0644 Acc: 0.3788\n",
      "val Loss: 1.1018 Acc: 0.3710\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0606 Acc: 0.3887\n",
      "val Loss: 1.1026 Acc: 0.3580\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0567 Acc: 0.3902\n",
      "val Loss: 1.1011 Acc: 0.3647\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0529 Acc: 0.3976\n",
      "val Loss: 1.1039 Acc: 0.3703\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0489 Acc: 0.4007\n",
      "val Loss: 1.1051 Acc: 0.3623\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0437 Acc: 0.4149\n",
      "val Loss: 1.1138 Acc: 0.3627\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0406 Acc: 0.4202\n",
      "val Loss: 1.1205 Acc: 0.3550\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0358 Acc: 0.4210\n",
      "val Loss: 1.1155 Acc: 0.3643\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0324 Acc: 0.4264\n",
      "val Loss: 1.1252 Acc: 0.3503\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0244 Acc: 0.4331\n",
      "val Loss: 1.1386 Acc: 0.3497\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0180 Acc: 0.4480\n",
      "val Loss: 1.1422 Acc: 0.3440\n",
      "\n",
      "Best val Acc: 0.378000\n",
      "Accuracy on test set using Forward Method: 0.49266666666666664\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1053 Acc: 0.3355\n",
      "val Loss: 1.0926 Acc: 0.3510\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0943 Acc: 0.3407\n",
      "val Loss: 1.0928 Acc: 0.3550\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0890 Acc: 0.3488\n",
      "val Loss: 1.1037 Acc: 0.3523\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0859 Acc: 0.3515\n",
      "val Loss: 1.1002 Acc: 0.3580\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0816 Acc: 0.3533\n",
      "val Loss: 1.0937 Acc: 0.3530\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0773 Acc: 0.3623\n",
      "val Loss: 1.1003 Acc: 0.3543\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0760 Acc: 0.3632\n",
      "val Loss: 1.0939 Acc: 0.3580\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0714 Acc: 0.3695\n",
      "val Loss: 1.1011 Acc: 0.3623\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0677 Acc: 0.3766\n",
      "val Loss: 1.1008 Acc: 0.3603\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0652 Acc: 0.3809\n",
      "val Loss: 1.1005 Acc: 0.3543\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0629 Acc: 0.3867\n",
      "val Loss: 1.1105 Acc: 0.3633\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0595 Acc: 0.3913\n",
      "val Loss: 1.1044 Acc: 0.3660\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0536 Acc: 0.4006\n",
      "val Loss: 1.1051 Acc: 0.3627\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0511 Acc: 0.3998\n",
      "val Loss: 1.1225 Acc: 0.3560\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0489 Acc: 0.4064\n",
      "val Loss: 1.1283 Acc: 0.3527\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0488 Acc: 0.4144\n",
      "val Loss: 1.1127 Acc: 0.3563\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0383 Acc: 0.4138\n",
      "val Loss: 1.1230 Acc: 0.3547\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0322 Acc: 0.4268\n",
      "val Loss: 1.1381 Acc: 0.3503\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0269 Acc: 0.4310\n",
      "val Loss: 1.1212 Acc: 0.3640\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0215 Acc: 0.4492\n",
      "val Loss: 1.1271 Acc: 0.3567\n",
      "\n",
      "Best val Acc: 0.366000\n",
      "Accuracy on test set using Forward Method: 0.5443333333333333\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1035 Acc: 0.3336\n",
      "val Loss: 1.0934 Acc: 0.3547\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0947 Acc: 0.3405\n",
      "val Loss: 1.0930 Acc: 0.3533\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0865 Acc: 0.3439\n",
      "val Loss: 1.0973 Acc: 0.3560\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0828 Acc: 0.3569\n",
      "val Loss: 1.0945 Acc: 0.3567\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0808 Acc: 0.3627\n",
      "val Loss: 1.0980 Acc: 0.3627\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0784 Acc: 0.3577\n",
      "val Loss: 1.0997 Acc: 0.3583\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0729 Acc: 0.3691\n",
      "val Loss: 1.0993 Acc: 0.3583\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0713 Acc: 0.3700\n",
      "val Loss: 1.0975 Acc: 0.3630\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0671 Acc: 0.3768\n",
      "val Loss: 1.0995 Acc: 0.3647\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0695 Acc: 0.3812\n",
      "val Loss: 1.1035 Acc: 0.3607\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0592 Acc: 0.3837\n",
      "val Loss: 1.1043 Acc: 0.3583\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0574 Acc: 0.3946\n",
      "val Loss: 1.1060 Acc: 0.3600\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0503 Acc: 0.3998\n",
      "val Loss: 1.1119 Acc: 0.3557\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0463 Acc: 0.4106\n",
      "val Loss: 1.1170 Acc: 0.3593\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0431 Acc: 0.4113\n",
      "val Loss: 1.1127 Acc: 0.3587\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0386 Acc: 0.4144\n",
      "val Loss: 1.1239 Acc: 0.3590\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0321 Acc: 0.4233\n",
      "val Loss: 1.1431 Acc: 0.3407\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0323 Acc: 0.4325\n",
      "val Loss: 1.1411 Acc: 0.3520\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0218 Acc: 0.4424\n",
      "val Loss: 1.1299 Acc: 0.3557\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0124 Acc: 0.4514\n",
      "val Loss: 1.1895 Acc: 0.3417\n",
      "\n",
      "Best val Acc: 0.364667\n",
      "Accuracy on test set using Forward Method: 0.5203333333333333\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1064 Acc: 0.3317\n",
      "val Loss: 1.0926 Acc: 0.3613\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0962 Acc: 0.3453\n",
      "val Loss: 1.0939 Acc: 0.3580\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0912 Acc: 0.3446\n",
      "val Loss: 1.0888 Acc: 0.3573\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0845 Acc: 0.3517\n",
      "val Loss: 1.0906 Acc: 0.3597\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0814 Acc: 0.3617\n",
      "val Loss: 1.0949 Acc: 0.3600\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0786 Acc: 0.3597\n",
      "val Loss: 1.0936 Acc: 0.3543\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0749 Acc: 0.3642\n",
      "val Loss: 1.0937 Acc: 0.3603\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0710 Acc: 0.3669\n",
      "val Loss: 1.1048 Acc: 0.3570\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0677 Acc: 0.3786\n",
      "val Loss: 1.1084 Acc: 0.3563\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0659 Acc: 0.3790\n",
      "val Loss: 1.1077 Acc: 0.3650\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0626 Acc: 0.3857\n",
      "val Loss: 1.1013 Acc: 0.3573\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0564 Acc: 0.3881\n",
      "val Loss: 1.1038 Acc: 0.3517\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0564 Acc: 0.3934\n",
      "val Loss: 1.1120 Acc: 0.3560\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0541 Acc: 0.4061\n",
      "val Loss: 1.1098 Acc: 0.3533\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0479 Acc: 0.4079\n",
      "val Loss: 1.1097 Acc: 0.3533\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0422 Acc: 0.4106\n",
      "val Loss: 1.1152 Acc: 0.3537\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0384 Acc: 0.4209\n",
      "val Loss: 1.1192 Acc: 0.3490\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0312 Acc: 0.4290\n",
      "val Loss: 1.1298 Acc: 0.3523\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0285 Acc: 0.4362\n",
      "val Loss: 1.1304 Acc: 0.3573\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0228 Acc: 0.4451\n",
      "val Loss: 1.1418 Acc: 0.3517\n",
      "\n",
      "Best val Acc: 0.365000\n",
      "Accuracy on test set using Forward Method: 0.5383333333333333\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1020 Acc: 0.3372\n",
      "val Loss: 1.0921 Acc: 0.3547\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0945 Acc: 0.3460\n",
      "val Loss: 1.0919 Acc: 0.3533\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0892 Acc: 0.3456\n",
      "val Loss: 1.0967 Acc: 0.3593\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0866 Acc: 0.3487\n",
      "val Loss: 1.0916 Acc: 0.3607\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0820 Acc: 0.3533\n",
      "val Loss: 1.0941 Acc: 0.3597\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0784 Acc: 0.3591\n",
      "val Loss: 1.0945 Acc: 0.3590\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0750 Acc: 0.3657\n",
      "val Loss: 1.0966 Acc: 0.3630\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0703 Acc: 0.3760\n",
      "val Loss: 1.0960 Acc: 0.3610\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0679 Acc: 0.3717\n",
      "val Loss: 1.0968 Acc: 0.3567\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0659 Acc: 0.3793\n",
      "val Loss: 1.1021 Acc: 0.3663\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0644 Acc: 0.3842\n",
      "val Loss: 1.0991 Acc: 0.3593\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0585 Acc: 0.3854\n",
      "val Loss: 1.1104 Acc: 0.3673\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0579 Acc: 0.3936\n",
      "val Loss: 1.1070 Acc: 0.3597\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0522 Acc: 0.3930\n",
      "val Loss: 1.1054 Acc: 0.3683\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0483 Acc: 0.4113\n",
      "val Loss: 1.1131 Acc: 0.3647\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0438 Acc: 0.4103\n",
      "val Loss: 1.1107 Acc: 0.3600\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0410 Acc: 0.4214\n",
      "val Loss: 1.1171 Acc: 0.3587\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0361 Acc: 0.4290\n",
      "val Loss: 1.1194 Acc: 0.3513\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0322 Acc: 0.4322\n",
      "val Loss: 1.1421 Acc: 0.3570\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0196 Acc: 0.4437\n",
      "val Loss: 1.1244 Acc: 0.3670\n",
      "\n",
      "Best val Acc: 0.368333\n",
      "Accuracy on test set using Forward Method: 0.5233333333333333\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1036 Acc: 0.3317\n",
      "val Loss: 1.0971 Acc: 0.3523\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0944 Acc: 0.3448\n",
      "val Loss: 1.0968 Acc: 0.3530\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0873 Acc: 0.3516\n",
      "val Loss: 1.0925 Acc: 0.3553\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0840 Acc: 0.3526\n",
      "val Loss: 1.0931 Acc: 0.3577\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0804 Acc: 0.3581\n",
      "val Loss: 1.0993 Acc: 0.3570\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0772 Acc: 0.3637\n",
      "val Loss: 1.1012 Acc: 0.3607\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0727 Acc: 0.3680\n",
      "val Loss: 1.0996 Acc: 0.3613\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0683 Acc: 0.3740\n",
      "val Loss: 1.0997 Acc: 0.3670\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0627 Acc: 0.3806\n",
      "val Loss: 1.0987 Acc: 0.3593\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0627 Acc: 0.3828\n",
      "val Loss: 1.1047 Acc: 0.3633\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0578 Acc: 0.3926\n",
      "val Loss: 1.1045 Acc: 0.3623\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0537 Acc: 0.4017\n",
      "val Loss: 1.1044 Acc: 0.3587\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0501 Acc: 0.4013\n",
      "val Loss: 1.1079 Acc: 0.3590\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0461 Acc: 0.4097\n",
      "val Loss: 1.1176 Acc: 0.3650\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0381 Acc: 0.4163\n",
      "val Loss: 1.1179 Acc: 0.3520\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0334 Acc: 0.4289\n",
      "val Loss: 1.1176 Acc: 0.3533\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0269 Acc: 0.4348\n",
      "val Loss: 1.1265 Acc: 0.3530\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0205 Acc: 0.4409\n",
      "val Loss: 1.1242 Acc: 0.3560\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0121 Acc: 0.4467\n",
      "val Loss: 1.1423 Acc: 0.3533\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0066 Acc: 0.4661\n",
      "val Loss: 1.1311 Acc: 0.3513\n",
      "\n",
      "Best val Acc: 0.367000\n",
      "Accuracy on test set using Forward Method: 0.49233333333333335\n",
      "\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset fashionmnist0.5 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "..............................\n",
      "\n",
      "Training iteration: 0\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0684 Acc: 0.4360\n",
      "val Loss: 1.0431 Acc: 0.4869\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0530 Acc: 0.4723\n",
      "val Loss: 1.0498 Acc: 0.4867\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0478 Acc: 0.4773\n",
      "val Loss: 1.0402 Acc: 0.4878\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0462 Acc: 0.4811\n",
      "val Loss: 1.0390 Acc: 0.4906\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0437 Acc: 0.4794\n",
      "val Loss: 1.0379 Acc: 0.4903\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0437 Acc: 0.4808\n",
      "val Loss: 1.0392 Acc: 0.4922\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0408 Acc: 0.4840\n",
      "val Loss: 1.0379 Acc: 0.4908\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0369 Acc: 0.4833\n",
      "val Loss: 1.0368 Acc: 0.4900\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0380 Acc: 0.4868\n",
      "val Loss: 1.0426 Acc: 0.4844\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0345 Acc: 0.4867\n",
      "val Loss: 1.0429 Acc: 0.4919\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0346 Acc: 0.4835\n",
      "val Loss: 1.0373 Acc: 0.4911\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0324 Acc: 0.4862\n",
      "val Loss: 1.0409 Acc: 0.4853\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0305 Acc: 0.4880\n",
      "val Loss: 1.0357 Acc: 0.4908\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0293 Acc: 0.4888\n",
      "val Loss: 1.0399 Acc: 0.4883\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0265 Acc: 0.4908\n",
      "val Loss: 1.0406 Acc: 0.4867\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0245 Acc: 0.4921\n",
      "val Loss: 1.0459 Acc: 0.4872\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0232 Acc: 0.4917\n",
      "val Loss: 1.0393 Acc: 0.4869\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0199 Acc: 0.4951\n",
      "val Loss: 1.0424 Acc: 0.4864\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0175 Acc: 0.4942\n",
      "val Loss: 1.0413 Acc: 0.4875\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0162 Acc: 0.4972\n",
      "val Loss: 1.0430 Acc: 0.4892\n",
      "\n",
      "Best val Acc: 0.492222\n",
      "Accuracy on test set using Forward Method: 0.915\n",
      "\n",
      "\n",
      "Training iteration: 1\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0680 Acc: 0.4410\n",
      "val Loss: 1.0528 Acc: 0.4744\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0533 Acc: 0.4748\n",
      "val Loss: 1.0440 Acc: 0.4853\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0486 Acc: 0.4731\n",
      "val Loss: 1.0414 Acc: 0.4842\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0451 Acc: 0.4801\n",
      "val Loss: 1.0388 Acc: 0.4908\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0434 Acc: 0.4810\n",
      "val Loss: 1.0428 Acc: 0.4889\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0418 Acc: 0.4772\n",
      "val Loss: 1.0404 Acc: 0.4903\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0391 Acc: 0.4839\n",
      "val Loss: 1.0422 Acc: 0.4886\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0382 Acc: 0.4840\n",
      "val Loss: 1.0374 Acc: 0.4908\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0336 Acc: 0.4860\n",
      "val Loss: 1.0385 Acc: 0.4903\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0334 Acc: 0.4867\n",
      "val Loss: 1.0375 Acc: 0.4900\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0319 Acc: 0.4847\n",
      "val Loss: 1.0395 Acc: 0.4861\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0317 Acc: 0.4890\n",
      "val Loss: 1.0427 Acc: 0.4875\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0278 Acc: 0.4882\n",
      "val Loss: 1.0401 Acc: 0.4889\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0263 Acc: 0.4889\n",
      "val Loss: 1.0413 Acc: 0.4861\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0213 Acc: 0.4918\n",
      "val Loss: 1.0513 Acc: 0.4861\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0211 Acc: 0.4917\n",
      "val Loss: 1.0413 Acc: 0.4881\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0217 Acc: 0.4931\n",
      "val Loss: 1.0416 Acc: 0.4869\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0172 Acc: 0.4965\n",
      "val Loss: 1.0544 Acc: 0.4681\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0145 Acc: 0.4967\n",
      "val Loss: 1.0510 Acc: 0.4756\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0120 Acc: 0.4977\n",
      "val Loss: 1.0468 Acc: 0.4794\n",
      "\n",
      "Best val Acc: 0.490833\n",
      "Accuracy on test set using Forward Method: 0.9093333333333333\n",
      "\n",
      "\n",
      "Training iteration: 2\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0681 Acc: 0.4432\n",
      "val Loss: 1.0448 Acc: 0.4789\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0523 Acc: 0.4719\n",
      "val Loss: 1.0421 Acc: 0.4842\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0471 Acc: 0.4760\n",
      "val Loss: 1.0406 Acc: 0.4856\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0450 Acc: 0.4801\n",
      "val Loss: 1.0393 Acc: 0.4858\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0436 Acc: 0.4787\n",
      "val Loss: 1.0450 Acc: 0.4847\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0420 Acc: 0.4784\n",
      "val Loss: 1.0402 Acc: 0.4892\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0406 Acc: 0.4820\n",
      "val Loss: 1.0396 Acc: 0.4892\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0356 Acc: 0.4869\n",
      "val Loss: 1.0404 Acc: 0.4869\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0338 Acc: 0.4860\n",
      "val Loss: 1.0469 Acc: 0.4836\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0318 Acc: 0.4860\n",
      "val Loss: 1.0391 Acc: 0.4897\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0288 Acc: 0.4890\n",
      "val Loss: 1.0436 Acc: 0.4850\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0289 Acc: 0.4881\n",
      "val Loss: 1.0449 Acc: 0.4883\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0261 Acc: 0.4897\n",
      "val Loss: 1.0414 Acc: 0.4872\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0273 Acc: 0.4922\n",
      "val Loss: 1.0527 Acc: 0.4811\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0204 Acc: 0.4959\n",
      "val Loss: 1.0473 Acc: 0.4850\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0169 Acc: 0.4960\n",
      "val Loss: 1.0455 Acc: 0.4875\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0196 Acc: 0.4920\n",
      "val Loss: 1.0435 Acc: 0.4869\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0133 Acc: 0.4997\n",
      "val Loss: 1.0475 Acc: 0.4886\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0109 Acc: 0.4994\n",
      "val Loss: 1.0545 Acc: 0.4817\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0064 Acc: 0.5035\n",
      "val Loss: 1.0487 Acc: 0.4833\n",
      "\n",
      "Best val Acc: 0.489722\n",
      "Accuracy on test set using Forward Method: 0.9123333333333333\n",
      "\n",
      "\n",
      "Training iteration: 3\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0673 Acc: 0.4395\n",
      "val Loss: 1.0428 Acc: 0.4822\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0519 Acc: 0.4713\n",
      "val Loss: 1.0416 Acc: 0.4844\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0488 Acc: 0.4758\n",
      "val Loss: 1.0397 Acc: 0.4878\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0455 Acc: 0.4808\n",
      "val Loss: 1.0398 Acc: 0.4872\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0434 Acc: 0.4808\n",
      "val Loss: 1.0428 Acc: 0.4847\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0424 Acc: 0.4815\n",
      "val Loss: 1.0394 Acc: 0.4867\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0414 Acc: 0.4828\n",
      "val Loss: 1.0401 Acc: 0.4903\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0393 Acc: 0.4847\n",
      "val Loss: 1.0392 Acc: 0.4867\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0366 Acc: 0.4847\n",
      "val Loss: 1.0385 Acc: 0.4872\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0342 Acc: 0.4860\n",
      "val Loss: 1.0502 Acc: 0.4764\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0325 Acc: 0.4848\n",
      "val Loss: 1.0506 Acc: 0.4775\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0320 Acc: 0.4842\n",
      "val Loss: 1.0412 Acc: 0.4864\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0299 Acc: 0.4887\n",
      "val Loss: 1.0375 Acc: 0.4878\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0283 Acc: 0.4902\n",
      "val Loss: 1.0404 Acc: 0.4889\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0261 Acc: 0.4892\n",
      "val Loss: 1.0395 Acc: 0.4858\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0243 Acc: 0.4948\n",
      "val Loss: 1.0404 Acc: 0.4881\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0227 Acc: 0.4910\n",
      "val Loss: 1.0395 Acc: 0.4864\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0193 Acc: 0.4922\n",
      "val Loss: 1.0443 Acc: 0.4878\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0175 Acc: 0.4943\n",
      "val Loss: 1.0426 Acc: 0.4883\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0151 Acc: 0.4959\n",
      "val Loss: 1.0441 Acc: 0.4817\n",
      "\n",
      "Best val Acc: 0.490278\n",
      "Accuracy on test set using Forward Method: 0.9\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0701 Acc: 0.4359\n",
      "val Loss: 1.0438 Acc: 0.4883\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0510 Acc: 0.4752\n",
      "val Loss: 1.0424 Acc: 0.4875\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0476 Acc: 0.4769\n",
      "val Loss: 1.0411 Acc: 0.4881\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0454 Acc: 0.4788\n",
      "val Loss: 1.0485 Acc: 0.4817\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0459 Acc: 0.4794\n",
      "val Loss: 1.0400 Acc: 0.4864\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0402 Acc: 0.4831\n",
      "val Loss: 1.0397 Acc: 0.4867\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0401 Acc: 0.4844\n",
      "val Loss: 1.0393 Acc: 0.4906\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0376 Acc: 0.4835\n",
      "val Loss: 1.0378 Acc: 0.4886\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0356 Acc: 0.4860\n",
      "val Loss: 1.0385 Acc: 0.4867\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0352 Acc: 0.4866\n",
      "val Loss: 1.0405 Acc: 0.4900\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0318 Acc: 0.4891\n",
      "val Loss: 1.0382 Acc: 0.4900\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0289 Acc: 0.4884\n",
      "val Loss: 1.0410 Acc: 0.4875\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0281 Acc: 0.4892\n",
      "val Loss: 1.0388 Acc: 0.4864\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0271 Acc: 0.4917\n",
      "val Loss: 1.0427 Acc: 0.4881\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0254 Acc: 0.4905\n",
      "val Loss: 1.0417 Acc: 0.4875\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0228 Acc: 0.4955\n",
      "val Loss: 1.0432 Acc: 0.4892\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0198 Acc: 0.4943\n",
      "val Loss: 1.0423 Acc: 0.4839\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0145 Acc: 0.4950\n",
      "val Loss: 1.0442 Acc: 0.4825\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0142 Acc: 0.4988\n",
      "val Loss: 1.0493 Acc: 0.4819\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0108 Acc: 0.5018\n",
      "val Loss: 1.0462 Acc: 0.4850\n",
      "\n",
      "Best val Acc: 0.490556\n",
      "Accuracy on test set using Forward Method: 0.904\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0664 Acc: 0.4488\n",
      "val Loss: 1.0417 Acc: 0.4831\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0527 Acc: 0.4736\n",
      "val Loss: 1.0391 Acc: 0.4869\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0509 Acc: 0.4756\n",
      "val Loss: 1.0403 Acc: 0.4869\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0479 Acc: 0.4797\n",
      "val Loss: 1.0370 Acc: 0.4919\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0424 Acc: 0.4851\n",
      "val Loss: 1.0354 Acc: 0.4911\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0431 Acc: 0.4840\n",
      "val Loss: 1.0410 Acc: 0.4908\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0405 Acc: 0.4854\n",
      "val Loss: 1.0339 Acc: 0.4914\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0397 Acc: 0.4838\n",
      "val Loss: 1.0374 Acc: 0.4928\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0369 Acc: 0.4858\n",
      "val Loss: 1.0379 Acc: 0.4906\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0366 Acc: 0.4853\n",
      "val Loss: 1.0374 Acc: 0.4900\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0335 Acc: 0.4885\n",
      "val Loss: 1.0356 Acc: 0.4897\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0311 Acc: 0.4868\n",
      "val Loss: 1.0362 Acc: 0.4922\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0282 Acc: 0.4885\n",
      "val Loss: 1.0462 Acc: 0.4831\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0298 Acc: 0.4880\n",
      "val Loss: 1.0394 Acc: 0.4908\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0269 Acc: 0.4896\n",
      "val Loss: 1.0378 Acc: 0.4900\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0233 Acc: 0.4888\n",
      "val Loss: 1.0361 Acc: 0.4908\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0217 Acc: 0.4940\n",
      "val Loss: 1.0430 Acc: 0.4900\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0219 Acc: 0.4924\n",
      "val Loss: 1.0477 Acc: 0.4903\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0169 Acc: 0.4956\n",
      "val Loss: 1.0399 Acc: 0.4844\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0158 Acc: 0.4943\n",
      "val Loss: 1.0412 Acc: 0.4875\n",
      "\n",
      "Best val Acc: 0.492778\n",
      "Accuracy on test set using Forward Method: 0.91\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0662 Acc: 0.4380\n",
      "val Loss: 1.0470 Acc: 0.4872\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0511 Acc: 0.4753\n",
      "val Loss: 1.0502 Acc: 0.4836\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0475 Acc: 0.4781\n",
      "val Loss: 1.0415 Acc: 0.4911\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0473 Acc: 0.4788\n",
      "val Loss: 1.0405 Acc: 0.4900\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0424 Acc: 0.4822\n",
      "val Loss: 1.0404 Acc: 0.4886\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0416 Acc: 0.4827\n",
      "val Loss: 1.0386 Acc: 0.4903\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0378 Acc: 0.4850\n",
      "val Loss: 1.0384 Acc: 0.4914\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0380 Acc: 0.4820\n",
      "val Loss: 1.0438 Acc: 0.4856\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0345 Acc: 0.4874\n",
      "val Loss: 1.0390 Acc: 0.4903\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0320 Acc: 0.4860\n",
      "val Loss: 1.0417 Acc: 0.4919\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0313 Acc: 0.4878\n",
      "val Loss: 1.0432 Acc: 0.4903\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0305 Acc: 0.4869\n",
      "val Loss: 1.0395 Acc: 0.4911\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0280 Acc: 0.4891\n",
      "val Loss: 1.0466 Acc: 0.4839\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0266 Acc: 0.4919\n",
      "val Loss: 1.0452 Acc: 0.4883\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0235 Acc: 0.4925\n",
      "val Loss: 1.0469 Acc: 0.4839\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0228 Acc: 0.4915\n",
      "val Loss: 1.0439 Acc: 0.4883\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0197 Acc: 0.4906\n",
      "val Loss: 1.0455 Acc: 0.4836\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0167 Acc: 0.4944\n",
      "val Loss: 1.0489 Acc: 0.4856\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0162 Acc: 0.4948\n",
      "val Loss: 1.0500 Acc: 0.4833\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0147 Acc: 0.4985\n",
      "val Loss: 1.0619 Acc: 0.4772\n",
      "\n",
      "Best val Acc: 0.491944\n",
      "Accuracy on test set using Forward Method: 0.9323333333333333\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0690 Acc: 0.4425\n",
      "val Loss: 1.0419 Acc: 0.4894\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0527 Acc: 0.4765\n",
      "val Loss: 1.0417 Acc: 0.4928\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0482 Acc: 0.4782\n",
      "val Loss: 1.0401 Acc: 0.4864\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0449 Acc: 0.4799\n",
      "val Loss: 1.0374 Acc: 0.4928\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0428 Acc: 0.4810\n",
      "val Loss: 1.0371 Acc: 0.4922\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0401 Acc: 0.4823\n",
      "val Loss: 1.0402 Acc: 0.4917\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0385 Acc: 0.4827\n",
      "val Loss: 1.0372 Acc: 0.4936\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0385 Acc: 0.4828\n",
      "val Loss: 1.0378 Acc: 0.4903\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0333 Acc: 0.4853\n",
      "val Loss: 1.0354 Acc: 0.4911\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0334 Acc: 0.4851\n",
      "val Loss: 1.0409 Acc: 0.4900\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0311 Acc: 0.4885\n",
      "val Loss: 1.0372 Acc: 0.4919\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0278 Acc: 0.4884\n",
      "val Loss: 1.0369 Acc: 0.4903\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0246 Acc: 0.4881\n",
      "val Loss: 1.0387 Acc: 0.4917\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0243 Acc: 0.4884\n",
      "val Loss: 1.0397 Acc: 0.4925\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0228 Acc: 0.4921\n",
      "val Loss: 1.0395 Acc: 0.4869\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0218 Acc: 0.4921\n",
      "val Loss: 1.0410 Acc: 0.4933\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0172 Acc: 0.4963\n",
      "val Loss: 1.0416 Acc: 0.4875\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0162 Acc: 0.4969\n",
      "val Loss: 1.0436 Acc: 0.4894\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0116 Acc: 0.4974\n",
      "val Loss: 1.0453 Acc: 0.4900\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0076 Acc: 0.4999\n",
      "val Loss: 1.0441 Acc: 0.4839\n",
      "\n",
      "Best val Acc: 0.493611\n",
      "Accuracy on test set using Forward Method: 0.911\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0667 Acc: 0.4483\n",
      "val Loss: 1.0441 Acc: 0.4811\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0527 Acc: 0.4735\n",
      "val Loss: 1.0416 Acc: 0.4889\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0485 Acc: 0.4766\n",
      "val Loss: 1.0385 Acc: 0.4858\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0467 Acc: 0.4801\n",
      "val Loss: 1.0397 Acc: 0.4919\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0430 Acc: 0.4819\n",
      "val Loss: 1.0434 Acc: 0.4908\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0402 Acc: 0.4838\n",
      "val Loss: 1.0383 Acc: 0.4889\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0397 Acc: 0.4818\n",
      "val Loss: 1.0375 Acc: 0.4861\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0377 Acc: 0.4822\n",
      "val Loss: 1.0390 Acc: 0.4889\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0364 Acc: 0.4856\n",
      "val Loss: 1.0370 Acc: 0.4900\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0358 Acc: 0.4853\n",
      "val Loss: 1.0400 Acc: 0.4897\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0311 Acc: 0.4851\n",
      "val Loss: 1.0372 Acc: 0.4903\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0325 Acc: 0.4882\n",
      "val Loss: 1.0366 Acc: 0.4911\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0297 Acc: 0.4888\n",
      "val Loss: 1.0369 Acc: 0.4867\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0261 Acc: 0.4894\n",
      "val Loss: 1.0446 Acc: 0.4875\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0257 Acc: 0.4908\n",
      "val Loss: 1.0400 Acc: 0.4917\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0232 Acc: 0.4924\n",
      "val Loss: 1.0402 Acc: 0.4886\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0210 Acc: 0.4959\n",
      "val Loss: 1.0478 Acc: 0.4817\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0178 Acc: 0.4933\n",
      "val Loss: 1.0413 Acc: 0.4875\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0174 Acc: 0.4955\n",
      "val Loss: 1.0477 Acc: 0.4844\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0146 Acc: 0.4942\n",
      "val Loss: 1.0455 Acc: 0.4878\n",
      "\n",
      "Best val Acc: 0.491944\n",
      "Accuracy on test set using Forward Method: 0.901\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0668 Acc: 0.4442\n",
      "val Loss: 1.0433 Acc: 0.4822\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0530 Acc: 0.4697\n",
      "val Loss: 1.0519 Acc: 0.4825\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0496 Acc: 0.4756\n",
      "val Loss: 1.0421 Acc: 0.4853\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0456 Acc: 0.4806\n",
      "val Loss: 1.0398 Acc: 0.4828\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0451 Acc: 0.4788\n",
      "val Loss: 1.0389 Acc: 0.4919\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0414 Acc: 0.4797\n",
      "val Loss: 1.0379 Acc: 0.4869\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0380 Acc: 0.4840\n",
      "val Loss: 1.0414 Acc: 0.4917\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0372 Acc: 0.4856\n",
      "val Loss: 1.0426 Acc: 0.4817\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0389 Acc: 0.4827\n",
      "val Loss: 1.0376 Acc: 0.4903\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0326 Acc: 0.4872\n",
      "val Loss: 1.0382 Acc: 0.4883\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0324 Acc: 0.4844\n",
      "val Loss: 1.0380 Acc: 0.4858\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0323 Acc: 0.4874\n",
      "val Loss: 1.0397 Acc: 0.4869\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0280 Acc: 0.4908\n",
      "val Loss: 1.0374 Acc: 0.4875\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0280 Acc: 0.4902\n",
      "val Loss: 1.0376 Acc: 0.4883\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0275 Acc: 0.4883\n",
      "val Loss: 1.0504 Acc: 0.4858\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0220 Acc: 0.4916\n",
      "val Loss: 1.0421 Acc: 0.4869\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0230 Acc: 0.4933\n",
      "val Loss: 1.0444 Acc: 0.4892\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0199 Acc: 0.4922\n",
      "val Loss: 1.0403 Acc: 0.4886\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0145 Acc: 0.4982\n",
      "val Loss: 1.0398 Acc: 0.4897\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0129 Acc: 0.4990\n",
      "val Loss: 1.0475 Acc: 0.4778\n",
      "\n",
      "Best val Acc: 0.491944\n",
      "Accuracy on test set using Forward Method: 0.9243333333333333\n",
      "\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset fashionmnist0.6 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "..............................\n",
      "\n",
      "Training iteration: 0\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1024 Acc: 0.3593\n",
      "val Loss: 1.0947 Acc: 0.3856\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0941 Acc: 0.3783\n",
      "val Loss: 1.0925 Acc: 0.3917\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0926 Acc: 0.3828\n",
      "val Loss: 1.0906 Acc: 0.3903\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0906 Acc: 0.3892\n",
      "val Loss: 1.0949 Acc: 0.3781\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0887 Acc: 0.3899\n",
      "val Loss: 1.0908 Acc: 0.3931\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0882 Acc: 0.3935\n",
      "val Loss: 1.0946 Acc: 0.3928\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0864 Acc: 0.3978\n",
      "val Loss: 1.0906 Acc: 0.3925\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0854 Acc: 0.3998\n",
      "val Loss: 1.0924 Acc: 0.3881\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0847 Acc: 0.3991\n",
      "val Loss: 1.0940 Acc: 0.3792\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0842 Acc: 0.3984\n",
      "val Loss: 1.0932 Acc: 0.3894\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0809 Acc: 0.4069\n",
      "val Loss: 1.0921 Acc: 0.3850\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0805 Acc: 0.4039\n",
      "val Loss: 1.0925 Acc: 0.3883\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0789 Acc: 0.4118\n",
      "val Loss: 1.0930 Acc: 0.3861\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0773 Acc: 0.4117\n",
      "val Loss: 1.0981 Acc: 0.3831\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0760 Acc: 0.4134\n",
      "val Loss: 1.0948 Acc: 0.3842\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0745 Acc: 0.4218\n",
      "val Loss: 1.0935 Acc: 0.3833\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0729 Acc: 0.4217\n",
      "val Loss: 1.0939 Acc: 0.3842\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0712 Acc: 0.4244\n",
      "val Loss: 1.0998 Acc: 0.3692\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0701 Acc: 0.4186\n",
      "val Loss: 1.1012 Acc: 0.3742\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0696 Acc: 0.4248\n",
      "val Loss: 1.1021 Acc: 0.3683\n",
      "\n",
      "Best val Acc: 0.393056\n",
      "Accuracy on test set using Forward Method: 0.8726666666666667\n",
      "\n",
      "\n",
      "Training iteration: 1\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1026 Acc: 0.3572\n",
      "val Loss: 1.0907 Acc: 0.3897\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0949 Acc: 0.3793\n",
      "val Loss: 1.0915 Acc: 0.3886\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0928 Acc: 0.3846\n",
      "val Loss: 1.0894 Acc: 0.3967\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0917 Acc: 0.3876\n",
      "val Loss: 1.0898 Acc: 0.3914\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0896 Acc: 0.3891\n",
      "val Loss: 1.0895 Acc: 0.3944\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0883 Acc: 0.3926\n",
      "val Loss: 1.0897 Acc: 0.3936\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0872 Acc: 0.3950\n",
      "val Loss: 1.0907 Acc: 0.3900\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0856 Acc: 0.3992\n",
      "val Loss: 1.0926 Acc: 0.3856\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0849 Acc: 0.3987\n",
      "val Loss: 1.0896 Acc: 0.3989\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0836 Acc: 0.4000\n",
      "val Loss: 1.0985 Acc: 0.3739\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0827 Acc: 0.4024\n",
      "val Loss: 1.0915 Acc: 0.3967\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0813 Acc: 0.4042\n",
      "val Loss: 1.0921 Acc: 0.3878\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0801 Acc: 0.4050\n",
      "val Loss: 1.0925 Acc: 0.3867\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0771 Acc: 0.4110\n",
      "val Loss: 1.0927 Acc: 0.3928\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0762 Acc: 0.4085\n",
      "val Loss: 1.0926 Acc: 0.3906\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0748 Acc: 0.4150\n",
      "val Loss: 1.1001 Acc: 0.3844\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0728 Acc: 0.4163\n",
      "val Loss: 1.1005 Acc: 0.3781\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0725 Acc: 0.4180\n",
      "val Loss: 1.0957 Acc: 0.3672\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0704 Acc: 0.4210\n",
      "val Loss: 1.0993 Acc: 0.3769\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0692 Acc: 0.4258\n",
      "val Loss: 1.1016 Acc: 0.3764\n",
      "\n",
      "Best val Acc: 0.398889\n",
      "Accuracy on test set using Forward Method: 0.8726666666666667\n",
      "\n",
      "\n",
      "Training iteration: 2\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1041 Acc: 0.3542\n",
      "val Loss: 1.0955 Acc: 0.3956\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0976 Acc: 0.3722\n",
      "val Loss: 1.0905 Acc: 0.4011\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0940 Acc: 0.3776\n",
      "val Loss: 1.0910 Acc: 0.3917\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0914 Acc: 0.3853\n",
      "val Loss: 1.0952 Acc: 0.3917\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0894 Acc: 0.3902\n",
      "val Loss: 1.0922 Acc: 0.4019\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0881 Acc: 0.3942\n",
      "val Loss: 1.0957 Acc: 0.3789\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0865 Acc: 0.3999\n",
      "val Loss: 1.0921 Acc: 0.3969\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0846 Acc: 0.3965\n",
      "val Loss: 1.0948 Acc: 0.3772\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0842 Acc: 0.3983\n",
      "val Loss: 1.0944 Acc: 0.3894\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0813 Acc: 0.4079\n",
      "val Loss: 1.0979 Acc: 0.3964\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0807 Acc: 0.4085\n",
      "val Loss: 1.0924 Acc: 0.3933\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0794 Acc: 0.4074\n",
      "val Loss: 1.0931 Acc: 0.3928\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0783 Acc: 0.4064\n",
      "val Loss: 1.0933 Acc: 0.3861\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0758 Acc: 0.4144\n",
      "val Loss: 1.0944 Acc: 0.3972\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0744 Acc: 0.4183\n",
      "val Loss: 1.0953 Acc: 0.3878\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0738 Acc: 0.4168\n",
      "val Loss: 1.0979 Acc: 0.3775\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0717 Acc: 0.4222\n",
      "val Loss: 1.1066 Acc: 0.3639\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0696 Acc: 0.4249\n",
      "val Loss: 1.0973 Acc: 0.3844\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0680 Acc: 0.4247\n",
      "val Loss: 1.0999 Acc: 0.3792\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0648 Acc: 0.4328\n",
      "val Loss: 1.1008 Acc: 0.3792\n",
      "\n",
      "Best val Acc: 0.401944\n",
      "Accuracy on test set using Forward Method: 0.8826666666666667\n",
      "\n",
      "\n",
      "Training iteration: 3\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1028 Acc: 0.3651\n",
      "val Loss: 1.0910 Acc: 0.3950\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0955 Acc: 0.3801\n",
      "val Loss: 1.0932 Acc: 0.3872\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0917 Acc: 0.3870\n",
      "val Loss: 1.0933 Acc: 0.3861\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0908 Acc: 0.3848\n",
      "val Loss: 1.0958 Acc: 0.3747\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0895 Acc: 0.3950\n",
      "val Loss: 1.1015 Acc: 0.3733\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0850 Acc: 0.4002\n",
      "val Loss: 1.0925 Acc: 0.3908\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0847 Acc: 0.3984\n",
      "val Loss: 1.0933 Acc: 0.3883\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0808 Acc: 0.4090\n",
      "val Loss: 1.0961 Acc: 0.3858\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0819 Acc: 0.4070\n",
      "val Loss: 1.0955 Acc: 0.3792\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0783 Acc: 0.4124\n",
      "val Loss: 1.0949 Acc: 0.3933\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0766 Acc: 0.4137\n",
      "val Loss: 1.0949 Acc: 0.3861\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0756 Acc: 0.4181\n",
      "val Loss: 1.0960 Acc: 0.3831\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0725 Acc: 0.4213\n",
      "val Loss: 1.0961 Acc: 0.3900\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0721 Acc: 0.4201\n",
      "val Loss: 1.0987 Acc: 0.3772\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0690 Acc: 0.4229\n",
      "val Loss: 1.1098 Acc: 0.3611\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0660 Acc: 0.4319\n",
      "val Loss: 1.1014 Acc: 0.3747\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0629 Acc: 0.4356\n",
      "val Loss: 1.1134 Acc: 0.3719\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0594 Acc: 0.4349\n",
      "val Loss: 1.1084 Acc: 0.3708\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0571 Acc: 0.4371\n",
      "val Loss: 1.1080 Acc: 0.3797\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0536 Acc: 0.4449\n",
      "val Loss: 1.1075 Acc: 0.3756\n",
      "\n",
      "Best val Acc: 0.395000\n",
      "Accuracy on test set using Forward Method: 0.8516666666666667\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1009 Acc: 0.3632\n",
      "val Loss: 1.0939 Acc: 0.3922\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0963 Acc: 0.3795\n",
      "val Loss: 1.0945 Acc: 0.3808\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0934 Acc: 0.3855\n",
      "val Loss: 1.0923 Acc: 0.3936\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0928 Acc: 0.3829\n",
      "val Loss: 1.0992 Acc: 0.3767\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0909 Acc: 0.3862\n",
      "val Loss: 1.0930 Acc: 0.3919\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0868 Acc: 0.3971\n",
      "val Loss: 1.0938 Acc: 0.3917\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0861 Acc: 0.4019\n",
      "val Loss: 1.0924 Acc: 0.3875\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0845 Acc: 0.4003\n",
      "val Loss: 1.0919 Acc: 0.3919\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0847 Acc: 0.3981\n",
      "val Loss: 1.0929 Acc: 0.3969\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0833 Acc: 0.4049\n",
      "val Loss: 1.0937 Acc: 0.3792\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0817 Acc: 0.4042\n",
      "val Loss: 1.0942 Acc: 0.3939\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0794 Acc: 0.4081\n",
      "val Loss: 1.0988 Acc: 0.3786\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0791 Acc: 0.4054\n",
      "val Loss: 1.1008 Acc: 0.3850\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0790 Acc: 0.4108\n",
      "val Loss: 1.0965 Acc: 0.3853\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0762 Acc: 0.4100\n",
      "val Loss: 1.0973 Acc: 0.3728\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0756 Acc: 0.4130\n",
      "val Loss: 1.0970 Acc: 0.3858\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0738 Acc: 0.4190\n",
      "val Loss: 1.0995 Acc: 0.3797\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0716 Acc: 0.4207\n",
      "val Loss: 1.0975 Acc: 0.3811\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0693 Acc: 0.4265\n",
      "val Loss: 1.0974 Acc: 0.3792\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0668 Acc: 0.4286\n",
      "val Loss: 1.0987 Acc: 0.3844\n",
      "\n",
      "Best val Acc: 0.396944\n",
      "Accuracy on test set using Forward Method: 0.8653333333333333\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1019 Acc: 0.3649\n",
      "val Loss: 1.0916 Acc: 0.3911\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0952 Acc: 0.3801\n",
      "val Loss: 1.0917 Acc: 0.3986\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0929 Acc: 0.3853\n",
      "val Loss: 1.0905 Acc: 0.3956\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0920 Acc: 0.3906\n",
      "val Loss: 1.0904 Acc: 0.3978\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0895 Acc: 0.3962\n",
      "val Loss: 1.0905 Acc: 0.3947\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0888 Acc: 0.3933\n",
      "val Loss: 1.0931 Acc: 0.3919\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0855 Acc: 0.3989\n",
      "val Loss: 1.0930 Acc: 0.3889\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0834 Acc: 0.4001\n",
      "val Loss: 1.0903 Acc: 0.3864\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0826 Acc: 0.4051\n",
      "val Loss: 1.0990 Acc: 0.3819\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0830 Acc: 0.4015\n",
      "val Loss: 1.0939 Acc: 0.3886\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0811 Acc: 0.4044\n",
      "val Loss: 1.0925 Acc: 0.3858\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0791 Acc: 0.4089\n",
      "val Loss: 1.0938 Acc: 0.3903\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0799 Acc: 0.4076\n",
      "val Loss: 1.0926 Acc: 0.3933\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0785 Acc: 0.4114\n",
      "val Loss: 1.1009 Acc: 0.3672\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0754 Acc: 0.4141\n",
      "val Loss: 1.0927 Acc: 0.3842\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0726 Acc: 0.4218\n",
      "val Loss: 1.0948 Acc: 0.3858\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0737 Acc: 0.4164\n",
      "val Loss: 1.0938 Acc: 0.3828\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0701 Acc: 0.4245\n",
      "val Loss: 1.0969 Acc: 0.3844\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0666 Acc: 0.4265\n",
      "val Loss: 1.0981 Acc: 0.3794\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0683 Acc: 0.4206\n",
      "val Loss: 1.1071 Acc: 0.3708\n",
      "\n",
      "Best val Acc: 0.398611\n",
      "Accuracy on test set using Forward Method: 0.8243333333333334\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1013 Acc: 0.3584\n",
      "val Loss: 1.0893 Acc: 0.3914\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0961 Acc: 0.3742\n",
      "val Loss: 1.0907 Acc: 0.3942\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0933 Acc: 0.3853\n",
      "val Loss: 1.0924 Acc: 0.3917\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0899 Acc: 0.3899\n",
      "val Loss: 1.0940 Acc: 0.3897\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0882 Acc: 0.3960\n",
      "val Loss: 1.0907 Acc: 0.3914\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0880 Acc: 0.3943\n",
      "val Loss: 1.0977 Acc: 0.3658\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0862 Acc: 0.3914\n",
      "val Loss: 1.0906 Acc: 0.3972\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0842 Acc: 0.3975\n",
      "val Loss: 1.0948 Acc: 0.3906\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0832 Acc: 0.4048\n",
      "val Loss: 1.0978 Acc: 0.3761\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0825 Acc: 0.4030\n",
      "val Loss: 1.0915 Acc: 0.3975\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0798 Acc: 0.4065\n",
      "val Loss: 1.0933 Acc: 0.3906\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0790 Acc: 0.4040\n",
      "val Loss: 1.0922 Acc: 0.3925\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0788 Acc: 0.4085\n",
      "val Loss: 1.0934 Acc: 0.3928\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0765 Acc: 0.4147\n",
      "val Loss: 1.0962 Acc: 0.3869\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0736 Acc: 0.4125\n",
      "val Loss: 1.0963 Acc: 0.3850\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0726 Acc: 0.4178\n",
      "val Loss: 1.1079 Acc: 0.3639\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0727 Acc: 0.4183\n",
      "val Loss: 1.0959 Acc: 0.3850\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0695 Acc: 0.4250\n",
      "val Loss: 1.1013 Acc: 0.3800\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0682 Acc: 0.4244\n",
      "val Loss: 1.0970 Acc: 0.3853\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0678 Acc: 0.4244\n",
      "val Loss: 1.0989 Acc: 0.3811\n",
      "\n",
      "Best val Acc: 0.397500\n",
      "Accuracy on test set using Forward Method: 0.871\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1032 Acc: 0.3617\n",
      "val Loss: 1.0909 Acc: 0.3947\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0945 Acc: 0.3799\n",
      "val Loss: 1.0982 Acc: 0.3828\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0943 Acc: 0.3803\n",
      "val Loss: 1.0944 Acc: 0.3914\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0895 Acc: 0.3913\n",
      "val Loss: 1.0940 Acc: 0.3950\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0887 Acc: 0.3919\n",
      "val Loss: 1.0912 Acc: 0.3950\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0857 Acc: 0.3987\n",
      "val Loss: 1.0932 Acc: 0.3914\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0853 Acc: 0.3974\n",
      "val Loss: 1.0929 Acc: 0.3992\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0831 Acc: 0.4069\n",
      "val Loss: 1.0929 Acc: 0.3900\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0824 Acc: 0.4011\n",
      "val Loss: 1.0934 Acc: 0.3947\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0812 Acc: 0.4063\n",
      "val Loss: 1.0983 Acc: 0.3769\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0794 Acc: 0.4085\n",
      "val Loss: 1.0951 Acc: 0.3853\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0787 Acc: 0.4091\n",
      "val Loss: 1.0996 Acc: 0.3764\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0748 Acc: 0.4138\n",
      "val Loss: 1.0959 Acc: 0.3872\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0745 Acc: 0.4135\n",
      "val Loss: 1.0956 Acc: 0.3906\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0733 Acc: 0.4170\n",
      "val Loss: 1.0952 Acc: 0.3881\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0713 Acc: 0.4201\n",
      "val Loss: 1.0963 Acc: 0.3947\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0692 Acc: 0.4215\n",
      "val Loss: 1.0978 Acc: 0.3864\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0690 Acc: 0.4151\n",
      "val Loss: 1.0986 Acc: 0.3878\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0690 Acc: 0.4215\n",
      "val Loss: 1.0997 Acc: 0.3789\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0631 Acc: 0.4313\n",
      "val Loss: 1.1024 Acc: 0.3744\n",
      "\n",
      "Best val Acc: 0.399167\n",
      "Accuracy on test set using Forward Method: 0.86\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1003 Acc: 0.3665\n",
      "val Loss: 1.0920 Acc: 0.3969\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0943 Acc: 0.3785\n",
      "val Loss: 1.0984 Acc: 0.3725\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0922 Acc: 0.3844\n",
      "val Loss: 1.0983 Acc: 0.3608\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0902 Acc: 0.3912\n",
      "val Loss: 1.0931 Acc: 0.3914\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0872 Acc: 0.3926\n",
      "val Loss: 1.0907 Acc: 0.3958\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0870 Acc: 0.3927\n",
      "val Loss: 1.1059 Acc: 0.3614\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0852 Acc: 0.3998\n",
      "val Loss: 1.0960 Acc: 0.3800\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0841 Acc: 0.4000\n",
      "val Loss: 1.0941 Acc: 0.3847\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0826 Acc: 0.4041\n",
      "val Loss: 1.0954 Acc: 0.3817\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0817 Acc: 0.4068\n",
      "val Loss: 1.0947 Acc: 0.3839\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0807 Acc: 0.4051\n",
      "val Loss: 1.0926 Acc: 0.3844\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0784 Acc: 0.4114\n",
      "val Loss: 1.0975 Acc: 0.3714\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0772 Acc: 0.4151\n",
      "val Loss: 1.1040 Acc: 0.3703\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0776 Acc: 0.4136\n",
      "val Loss: 1.0993 Acc: 0.3608\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0732 Acc: 0.4210\n",
      "val Loss: 1.0961 Acc: 0.3875\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0743 Acc: 0.4185\n",
      "val Loss: 1.0968 Acc: 0.3800\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0714 Acc: 0.4174\n",
      "val Loss: 1.0997 Acc: 0.3822\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0696 Acc: 0.4268\n",
      "val Loss: 1.1039 Acc: 0.3692\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0683 Acc: 0.4267\n",
      "val Loss: 1.0979 Acc: 0.3758\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0668 Acc: 0.4267\n",
      "val Loss: 1.1077 Acc: 0.3683\n",
      "\n",
      "Best val Acc: 0.396944\n",
      "Accuracy on test set using Forward Method: 0.8523333333333334\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1021 Acc: 0.3623\n",
      "val Loss: 1.0984 Acc: 0.3892\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0960 Acc: 0.3810\n",
      "val Loss: 1.0981 Acc: 0.3850\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0945 Acc: 0.3887\n",
      "val Loss: 1.0924 Acc: 0.3919\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0901 Acc: 0.3896\n",
      "val Loss: 1.0933 Acc: 0.3892\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0892 Acc: 0.3928\n",
      "val Loss: 1.0934 Acc: 0.3925\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0877 Acc: 0.3926\n",
      "val Loss: 1.0944 Acc: 0.3836\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0870 Acc: 0.3929\n",
      "val Loss: 1.0925 Acc: 0.3947\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0837 Acc: 0.4032\n",
      "val Loss: 1.0924 Acc: 0.3881\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0837 Acc: 0.4035\n",
      "val Loss: 1.0922 Acc: 0.3942\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0823 Acc: 0.3992\n",
      "val Loss: 1.0974 Acc: 0.3744\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0799 Acc: 0.4057\n",
      "val Loss: 1.0985 Acc: 0.3928\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0792 Acc: 0.4081\n",
      "val Loss: 1.0936 Acc: 0.3894\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0787 Acc: 0.4095\n",
      "val Loss: 1.0956 Acc: 0.3861\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0767 Acc: 0.4095\n",
      "val Loss: 1.0985 Acc: 0.3758\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0753 Acc: 0.4112\n",
      "val Loss: 1.0980 Acc: 0.3772\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0745 Acc: 0.4178\n",
      "val Loss: 1.0960 Acc: 0.3825\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0738 Acc: 0.4142\n",
      "val Loss: 1.0967 Acc: 0.3856\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0696 Acc: 0.4216\n",
      "val Loss: 1.0989 Acc: 0.3753\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0696 Acc: 0.4204\n",
      "val Loss: 1.1015 Acc: 0.3803\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0683 Acc: 0.4225\n",
      "val Loss: 1.1075 Acc: 0.3806\n",
      "\n",
      "Best val Acc: 0.394722\n",
      "Accuracy on test set using Forward Method: 0.861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "# Training the model n_iters\n",
    "n_iters = 10\n",
    "\n",
    "# Set seed\n",
    "seed_torch()\n",
    "\n",
    "test_acc_forward_results = {'cifar': [],\n",
    "                            'fashionmnist0.5': [],\n",
    "                            'fashionmnist0.6': []}\n",
    "\n",
    "# Iterate over each dataset\n",
    "for d in list(available_datasets.keys()):\n",
    "  dataset = d\n",
    "\n",
    "  # Load dataset\n",
    "  if dataset.lower() in available_datasets:\n",
    "    dir = available_datasets[dataset.lower()]\n",
    "    T_matrix = T_matrices[dataset.lower()]\n",
    "    X_train, y_train, X_test, y_test = load_data(dir)\n",
    "    print('-'*60)\n",
    "    print('-'*60)\n",
    "    print(f\"\\nDataset {dataset} loaded.\\n\")\n",
    "\n",
    "    print(f\"Shape Xtr: {X_train.shape}\")\n",
    "    print(f\"Shape Str: {y_train.shape}\")\n",
    "    print(f\"Shape Xts: {X_test.shape}\")\n",
    "    print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "    print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "    print('.'*30)\n",
    "  else:\n",
    "    print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")\n",
    "\n",
    "  # Detect if GPU or CPU\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "      \n",
    "  # Define parameters\n",
    "  if dataset.lower() == 'cifar':\n",
    "    n_channels = 3\n",
    "    n_filters = 7\n",
    "  else:\n",
    "    n_channels = 1\n",
    "    n_filters = 6\n",
    "\n",
    "  num_classes = 3\n",
    "  batch_size = 128\n",
    "  num_epochs = 20\n",
    "  learning_rate = 0.001\n",
    "  \n",
    "  for n in range(n_iters):\n",
    "    print(f\"\\nTraining iteration: {n}\")\n",
    "\n",
    "    # Clean cache each iteration\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # We did not set a seed for train_test_split thus, it would generate different samples each iteration\n",
    "    train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                    y_train=y_train, \n",
    "                                                    X_test=X_test, \n",
    "                                                    y_test=y_test, \n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "\n",
    "    # Initialize model\n",
    "    model_cnn_forward = CNN(num_classes=num_classes, n_channels=n_channels, n_filters=n_filters).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model_cnn_forward.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Train model\n",
    "    model_cnn_forward, hist = train_model(model_cnn_forward, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs, T=T_matrix)\n",
    "\n",
    "    # Generate predictions on the test set\n",
    "    y_true, y_pred, acc, outputs = prediction(model_cnn_forward, test_loader, device)\n",
    "\n",
    "    test_acc_forward_results[dataset].append(acc)\n",
    "    print(f\"Accuracy on test set using Forward Method: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMi3TQZpLazg"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUb2ncI6SRmi"
   },
   "source": [
    "We can print the dictionary `test_acc_forward_results` which has stored the results of the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 1342771,
     "status": "ok",
     "timestamp": 1605608593573,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "1EBMXrQsSRB2",
    "outputId": "588e0440-2cc7-4355-9155-cfbb87578316"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifar</th>\n",
       "      <th>fashionmnist0.5</th>\n",
       "      <th>fashionmnist0.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.469333</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.872667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.909333</td>\n",
       "      <td>0.872667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.498333</td>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.882667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.851667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.492667</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.865333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.544333</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.824333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.520333</td>\n",
       "      <td>0.932333</td>\n",
       "      <td>0.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.538333</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.523333</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.852333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.492333</td>\n",
       "      <td>0.924333</td>\n",
       "      <td>0.861000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cifar  fashionmnist0.5  fashionmnist0.6\n",
       "0  0.469333         0.915000         0.872667\n",
       "1  0.497000         0.909333         0.872667\n",
       "2  0.498333         0.912333         0.882667\n",
       "3  0.495000         0.900000         0.851667\n",
       "4  0.492667         0.904000         0.865333\n",
       "5  0.544333         0.910000         0.824333\n",
       "6  0.520333         0.932333         0.871000\n",
       "7  0.538333         0.911000         0.860000\n",
       "8  0.523333         0.901000         0.852333\n",
       "9  0.492333         0.924333         0.861000"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy values\n",
    "test_acc_forward_results_backup = test_acc_forward_results \n",
    "\n",
    "# Create dataframe from dictionary\n",
    "df = pd.DataFrame(test_acc_forward_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0L-mKvYtO4C"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "import pickle\n",
    "with open('test_acc_forward_results.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_acc_forward_results, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwzwv59PtUAc"
   },
   "outputs": [],
   "source": [
    "# Check that it correctly saved the results\n",
    "with open('test_acc_forward_results.pkl', 'rb') as fp:\n",
    "    test_load = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj5bl_BAX3a8"
   },
   "source": [
    "We will calculate the mean value a standard deviation for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSpelSSGX3JA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_results(results_dict, title, plot=True):\n",
    "  \"\"\"\n",
    "  Function to process results and display graph.\n",
    "\n",
    "  :param results_dict: Dictionary with results\n",
    "  :param title: Title of the plot\n",
    "  :param plot: True of False\n",
    "  :return dict_mean: Dictionary with mean values for each dataset\n",
    "  :return dict_std: Dictionary with std values for each dataset\n",
    "  \"\"\"\n",
    "  \n",
    "  dict_mean = {}\n",
    "  dict_std = {}\n",
    "  for k, v in results_dict.items():\n",
    "    dict_mean[k] = round(np.mean(v), 3)\n",
    "    dict_std[k] = round(np.std(v), 3)\n",
    "\n",
    "  if plot:\n",
    "    keys = list(results_dict.keys())\n",
    "    values = list(results_dict.values())\n",
    "    fig = plt.figure(constrained_layout=True,  figsize=(10,7), dpi=90)\n",
    "    gs = fig.add_gridspec(2, 3)\n",
    "\n",
    "    # Plot acc per training iteration\n",
    "    f_ax1 = fig.add_subplot(gs[0, 0])\n",
    "    f_ax1.set_title('CIFAR')\n",
    "    f_ax1.plot(np.arange(0, len(values[0])), values[0], c=\"orange\")\n",
    "    f_ax1.set_ylabel('Acc')\n",
    "    f_ax1.set_xlabel('Training Iterations')\n",
    "    f_ax1.grid(alpha=0.2) \n",
    "    \n",
    "    f_ax2 = fig.add_subplot(gs[0, -2])\n",
    "    f_ax2.set_title('FashionMNIST0.5')\n",
    "    f_ax2.plot(np.arange(0, len(values[1])), values[1], c=\"orange\")\n",
    "    f_ax2.set_xlabel('Training Iterations')\n",
    "    f_ax2.grid(alpha=0.2) \n",
    "\n",
    "    f_ax3 = fig.add_subplot(gs[0, -1])\n",
    "    f_ax3.set_title('FashionMNIST0.6')\n",
    "    f_ax3.plot(np.arange(0, len(values[2])), values[2], c=\"orange\")\n",
    "    f_ax3.set_xlabel('Training Iterations')\n",
    "    f_ax3.grid(alpha=0.2) \n",
    "\n",
    "    # Plot average acc and std\n",
    "    f_ax4 = fig.add_subplot(gs[1, :])\n",
    "    f_ax4.set_title(f'Average Accuracy and Standard Deviation using {title}')\n",
    "    f_ax4.errorbar(results_dict.keys(), dict_mean.values(), dict_std.values(), c=\"orange\", linestyle='None', marker='^')\n",
    "    f_ax4.grid(alpha=0.2) \n",
    "  return dict_mean, dict_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "executionInfo": {
     "elapsed": 1344184,
     "status": "ok",
     "timestamp": 1605608595012,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "8W-w-4kdYnhC",
    "outputId": "2d8cd134-71dd-450c-eef4-39c0ada7963e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x630 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean, std = process_results(test_acc_forward_results, title=\"Forward Method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrHxb3fedeUt"
   },
   "source": [
    "### 4.2.- Classifier using T-Revision Method\n",
    "\n",
    "In this section, we perform the experiments using the T-Revision method implemented before. \n",
    "\n",
    "As we did previously, we need to define the architecture of the ResNet model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH_F0wH9FxCC"
   },
   "source": [
    "#### ResNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7KS7VvYFmrX"
   },
   "outputs": [],
   "source": [
    "# Convolution Block\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Main Block ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_channels=1, num_filter=7, num_classes=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(num_channels, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(num_filter)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        self.delta_T = nn.Linear(num_classes, num_classes, False)\n",
    "        nn.init.zeros_(self.delta_T.weight)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, revision=False, plot_deltaT=False):\n",
    "        correction = self.delta_T.weight\n",
    "        if plot_deltaT:\n",
    "          print(\"Corr\", correction)\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        if revision:\n",
    "            return out, correction\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mWA7W27FtRo"
   },
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paKQX9NBFsgM"
   },
   "outputs": [],
   "source": [
    "class ReweightingLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super(ReweightingLoss, self).__init__()\n",
    "      \n",
    "    def forward(self, out, T, target, revision=None):\n",
    "      loss = 0.0\n",
    "      out_softmax = F.softmax(out, dim=1) \n",
    "      # If standard mode (revision false)\n",
    "      if revision is None:\n",
    "        g_Y = out_softmax.gather(1, target.view(-1,1))\n",
    "        Tg = torch.mm(T.t(), out_softmax.t())\n",
    "        Tg_Y = Tg.t().gather(1, target.view(-1,1))\n",
    "        # requires_grad=True indicates that we want to compute gradients with\n",
    "        # respect to these Variables during the backward pass.\n",
    "        beta = torch.nn.Parameter(g_Y / Tg_Y)\n",
    "        loss = F.cross_entropy(out, target, reduction='none')\n",
    "        loss = beta.view(1,-1) * loss\n",
    "        return torch.mean(loss)\n",
    "\n",
    "      # If revision mode\n",
    "      else:\n",
    "        len_t = len(target)\n",
    "        delta_T = revision\n",
    "        for i in range(len_t):\n",
    "            # Add new dimension\n",
    "            soft_iter = out_softmax[i].unsqueeze(0)\n",
    "            # Adding correction deltaT to transition matrix\n",
    "            T = T + delta_T\n",
    "            # Matrix product\n",
    "            out_T_transpose = torch.mm(T.t(), soft_iter.t()).t()\n",
    "            g_Y = soft_iter[:, target[i]]\n",
    "            Tg_Y = out_T_transpose[:, target[i]]\n",
    "            # Update beta\n",
    "            beta = (g_Y/Tg_Y)\n",
    "            cross_loss = F.cross_entropy(out[i].unsqueeze(0), target[i].unsqueeze(0))\n",
    "            # Sum loss\n",
    "            loss += beta * cross_loss\n",
    "        return loss / len_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUgoa8chF9Vt"
   },
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-srTMF2FF8Um"
   },
   "outputs": [],
   "source": [
    "def train_model_forward(model, dataloaders, criterion, optimizer, T_matrix, with_revision=False, num_epochs=25):\n",
    "  \"\"\"\n",
    "  Function to train the model\n",
    "  \n",
    "  :param model: Model\n",
    "  :param dataloaders: DataLoader\n",
    "  :param criterion: Loss function\n",
    "  :param optimizer: Optimizer\n",
    "  :param with_revision: True or False\n",
    "  :param num_epochs: Number of epochs\n",
    "  :return model: Trained model\n",
    "  \"\"\"\n",
    "\n",
    "  val_acc_history = []\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  revision_evl = []\n",
    "  best_acc = 0.0\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "      print('-' * 10)\n",
    "\n",
    "      # Each epoch has a training and validation phase\n",
    "      for phase in ['train', 'val']:\n",
    "          if phase == 'train':\n",
    "              model.train()  # Set model to training mode\n",
    "          else:\n",
    "              model.eval()   # Set model to evaluate mode\n",
    "\n",
    "          running_loss = 0.0\n",
    "          running_corrects = 0\n",
    "\n",
    "          # Iterate over data.\n",
    "          for b, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "              inputs = inputs.to(device)\n",
    "              labels = labels.to(device)\n",
    "\n",
    "              # zero the parameter gradients\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "              # forward\n",
    "              if with_revision==True:\n",
    "                out, revision = model(inputs, revision=with_revision)\n",
    "                loss = criterion(out, T_matrix, labels, revision)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                out = torch.matmul((T_matrix+revision).t(), prob.t())\n",
    "              else:\n",
    "                out = model(inputs)\n",
    "                loss = criterion(out, T_matrix, labels)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                out = torch.matmul(T_matrix.t(), prob.t())\n",
    "                \n",
    "              out = out.t()\n",
    "\n",
    "              # track history if only in train\n",
    "              if with_revision==False:\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                  \n",
    "                  _, preds = torch.max(out, 1)\n",
    "                  \n",
    "                  # backward + optimize only if in training phase\n",
    "                  if phase == 'train':\n",
    "                      loss.backward()\n",
    "                      optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "              else:\n",
    "                _, preds = torch.max(out, 1)\n",
    "\n",
    "                if phase == 'train':\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "                #print(loss.item()*inputs.size(0))\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if abs(loss.item()* inputs.size(0)) < 0.02:\n",
    "                  break\n",
    "\n",
    "          epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "          epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "          if with_revision==True:\n",
    "            revision_evl.append(revision)\n",
    "\n",
    "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "          # deep copy the model\n",
    "          if phase == 'val' and epoch_acc > best_acc:\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "          if phase == 'val':\n",
    "              val_acc_history.append(epoch_acc)\n",
    "\n",
    "      print()\n",
    "\n",
    "  print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "  # load best model weights\n",
    "  model.load_state_dict(best_model_wts)\n",
    "  return model, val_acc_history, revision_evl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXeOXQzRGZ1f"
   },
   "source": [
    "#### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3298676,
     "status": "ok",
     "timestamp": 1605615573329,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "ExC89aGZdqoK",
    "outputId": "3839e8be-97d7-48d0-e8df-b5a1431eb2c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "train Loss: 0.6540 Acc: 0.4735\n",
      "val Loss: 0.6655 Acc: 0.4689\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6451 Acc: 0.4727\n",
      "val Loss: 0.6481 Acc: 0.4861\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6460 Acc: 0.4796\n",
      "val Loss: 0.3987 Acc: 0.4225\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6475 Acc: 0.4799\n",
      "val Loss: 0.6406 Acc: 0.4853\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6316 Acc: 0.4833\n",
      "val Loss: 0.6961 Acc: 0.4819\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6390 Acc: 0.4829\n",
      "val Loss: 0.6267 Acc: 0.4881\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6233 Acc: 0.4824\n",
      "val Loss: 0.6330 Acc: 0.4636\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6150 Acc: 0.4847\n",
      "val Loss: 0.6269 Acc: 0.4756\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6161 Acc: 0.4901\n",
      "val Loss: 0.6072 Acc: 0.4764\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.6051 Acc: 0.4880\n",
      "val Loss: 0.6407 Acc: 0.4672\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.4894\n",
      "val Loss: 0.5984 Acc: 0.4808\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5870 Acc: 0.4899\n",
      "val Loss: 0.6381 Acc: 0.4644\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5591 Acc: 0.4911\n",
      "val Loss: 0.5635 Acc: 0.4831\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5568 Acc: 0.4999\n",
      "val Loss: 0.6177 Acc: 0.4669\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5251 Acc: 0.5004\n",
      "val Loss: 0.5182 Acc: 0.4544\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.5168 Acc: 0.5067\n",
      "val Loss: 0.4206 Acc: 0.4828\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.4871 Acc: 0.5138\n",
      "val Loss: 0.5030 Acc: 0.4653\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4762 Acc: 0.5226\n",
      "val Loss: 0.4678 Acc: 0.4675\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.4328 Acc: 0.5316\n",
      "val Loss: 0.4393 Acc: 0.4439\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.4102 Acc: 0.5351\n",
      "val Loss: 0.4101 Acc: 0.4433\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.3786 Acc: 0.5553\n",
      "val Loss: 0.3264 Acc: 0.4672\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3476 Acc: 0.5615\n",
      "val Loss: 0.3264 Acc: 0.4611\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3421 Acc: 0.5619\n",
      "val Loss: 0.3570 Acc: 0.4444\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.3248 Acc: 0.5810\n",
      "val Loss: 0.3323 Acc: 0.4375\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2915 Acc: 0.5938\n",
      "val Loss: 0.3980 Acc: 0.4197\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2830 Acc: 0.6013\n",
      "val Loss: 0.2966 Acc: 0.4461\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2609 Acc: 0.6184\n",
      "val Loss: 0.3138 Acc: 0.4325\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2492 Acc: 0.6238\n",
      "val Loss: 0.2467 Acc: 0.4436\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.2366 Acc: 0.6297\n",
      "val Loss: 0.2855 Acc: 0.4367\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.2191 Acc: 0.6360\n",
      "val Loss: 0.2715 Acc: 0.4250\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1987 Acc: 0.6528\n",
      "val Loss: 0.2467 Acc: 0.4325\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1883 Acc: 0.6615\n",
      "val Loss: 0.2521 Acc: 0.4264\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1699 Acc: 0.6765\n",
      "val Loss: 0.1954 Acc: 0.4406\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1584 Acc: 0.6794\n",
      "val Loss: 0.2116 Acc: 0.4278\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1508 Acc: 0.6878\n",
      "val Loss: 0.1967 Acc: 0.4419\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1346 Acc: 0.6985\n",
      "val Loss: 0.1811 Acc: 0.4397\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1284 Acc: 0.7074\n",
      "val Loss: 0.2233 Acc: 0.4144\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1246 Acc: 0.7091\n",
      "val Loss: 0.1716 Acc: 0.4283\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1190 Acc: 0.7111\n",
      "val Loss: 0.1816 Acc: 0.4361\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1129 Acc: 0.7192\n",
      "val Loss: 0.1724 Acc: 0.4275\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.0993 Acc: 0.7299\n",
      "val Loss: 0.1826 Acc: 0.4022\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.7360\n",
      "val Loss: 0.1462 Acc: 0.4214\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.0895 Acc: 0.7422\n",
      "val Loss: 0.1606 Acc: 0.4192\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.0910 Acc: 0.7380\n",
      "val Loss: 0.1400 Acc: 0.4242\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.7460\n",
      "val Loss: 0.1364 Acc: 0.4328\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0799 Acc: 0.7500\n",
      "val Loss: 0.1517 Acc: 0.4206\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0828 Acc: 0.7499\n",
      "val Loss: 0.1165 Acc: 0.4358\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0794 Acc: 0.7529\n",
      "val Loss: 0.1330 Acc: 0.4233\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0766 Acc: 0.7567\n",
      "val Loss: 0.1426 Acc: 0.4219\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0666 Acc: 0.7674\n",
      "val Loss: 0.1374 Acc: 0.4158\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0661 Acc: 0.7705\n",
      "val Loss: 0.1314 Acc: 0.4122\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0603 Acc: 0.7747\n",
      "val Loss: 0.1274 Acc: 0.4125\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0624 Acc: 0.7753\n",
      "val Loss: 0.1287 Acc: 0.4111\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0614 Acc: 0.7761\n",
      "val Loss: 0.1211 Acc: 0.4281\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0655 Acc: 0.7732\n",
      "val Loss: 0.1337 Acc: 0.4186\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0641 Acc: 0.7742\n",
      "val Loss: 0.1337 Acc: 0.4089\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0560 Acc: 0.7861\n",
      "val Loss: 0.1258 Acc: 0.4219\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0548 Acc: 0.7896\n",
      "val Loss: 0.1201 Acc: 0.4239\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0555 Acc: 0.7884\n",
      "val Loss: 0.1193 Acc: 0.4186\n",
      "\n",
      "Best val Acc: 0.488056\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0459 Acc: 0.4854\n",
      "val Loss: 0.0119 Acc: 0.4906\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0101 Acc: 0.4846\n",
      "val Loss: 0.0069 Acc: 0.4914\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.4818\n",
      "val Loss: 0.0052 Acc: 0.4875\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.4760\n",
      "val Loss: 0.0042 Acc: 0.4864\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.2145\n",
      "val Loss: 0.0018 Acc: 0.3206\n",
      "\n",
      "Best val Acc: 0.491389\n",
      "Accuracy on test set using T-Revision Method: 0.9336666666666666\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6749 Acc: 0.4591\n",
      "val Loss: 0.5765 Acc: 0.4806\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6421 Acc: 0.4747\n",
      "val Loss: 0.6268 Acc: 0.4878\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6508 Acc: 0.4760\n",
      "val Loss: 0.5483 Acc: 0.4786\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6424 Acc: 0.4777\n",
      "val Loss: 0.6077 Acc: 0.4811\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6372 Acc: 0.4799\n",
      "val Loss: 0.6453 Acc: 0.4869\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6336 Acc: 0.4808\n",
      "val Loss: 0.6760 Acc: 0.4811\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6296 Acc: 0.4843\n",
      "val Loss: 0.5552 Acc: 0.4872\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6185 Acc: 0.4852\n",
      "val Loss: 0.7117 Acc: 0.4803\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6079 Acc: 0.4866\n",
      "val Loss: 0.6495 Acc: 0.4811\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6022 Acc: 0.4856\n",
      "val Loss: 0.6360 Acc: 0.4900\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.6113 Acc: 0.4885\n",
      "val Loss: 0.6618 Acc: 0.4378\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5930 Acc: 0.4953\n",
      "val Loss: 0.5615 Acc: 0.4833\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5754 Acc: 0.4958\n",
      "val Loss: 0.5467 Acc: 0.4789\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5557 Acc: 0.4983\n",
      "val Loss: 0.5867 Acc: 0.4681\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5270 Acc: 0.5073\n",
      "val Loss: 0.5743 Acc: 0.4653\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5037 Acc: 0.5054\n",
      "val Loss: 0.5365 Acc: 0.4631\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.4840 Acc: 0.5181\n",
      "val Loss: 0.4573 Acc: 0.4686\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.4710 Acc: 0.5211\n",
      "val Loss: 0.4779 Acc: 0.4625\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4338 Acc: 0.5340\n",
      "val Loss: 0.4658 Acc: 0.4528\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.3951 Acc: 0.5465\n",
      "val Loss: 0.4395 Acc: 0.4339\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.3764 Acc: 0.5535\n",
      "val Loss: 0.4055 Acc: 0.4483\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.3491 Acc: 0.5611\n",
      "val Loss: 0.4008 Acc: 0.4378\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3325 Acc: 0.5693\n",
      "val Loss: 0.3627 Acc: 0.4486\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3063 Acc: 0.5817\n",
      "val Loss: 0.3519 Acc: 0.4217\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2806 Acc: 0.5983\n",
      "val Loss: 0.3235 Acc: 0.4353\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2689 Acc: 0.6069\n",
      "val Loss: 0.3296 Acc: 0.4386\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2491 Acc: 0.6139\n",
      "val Loss: 0.2888 Acc: 0.4378\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2382 Acc: 0.6285\n",
      "val Loss: 0.2928 Acc: 0.4425\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2284 Acc: 0.6358\n",
      "val Loss: 0.2309 Acc: 0.4333\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.2014 Acc: 0.6483\n",
      "val Loss: 0.2250 Acc: 0.4231\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.2001 Acc: 0.6499\n",
      "val Loss: 0.1769 Acc: 0.4367\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1879 Acc: 0.6665\n",
      "val Loss: 0.2418 Acc: 0.4256\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1717 Acc: 0.6726\n",
      "val Loss: 0.2237 Acc: 0.4217\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1630 Acc: 0.6799\n",
      "val Loss: 0.2002 Acc: 0.4275\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1533 Acc: 0.6905\n",
      "val Loss: 0.2065 Acc: 0.4486\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1508 Acc: 0.6965\n",
      "val Loss: 0.1838 Acc: 0.4303\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.7012\n",
      "val Loss: 0.1747 Acc: 0.4319\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1404 Acc: 0.7078\n",
      "val Loss: 0.1911 Acc: 0.4256\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1276 Acc: 0.7174\n",
      "val Loss: 0.1716 Acc: 0.4339\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1163 Acc: 0.7308\n",
      "val Loss: 0.1900 Acc: 0.4208\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1177 Acc: 0.7301\n",
      "val Loss: 0.1655 Acc: 0.4350\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1079 Acc: 0.7408\n",
      "val Loss: 0.1749 Acc: 0.4269\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1001 Acc: 0.7423\n",
      "val Loss: 0.1614 Acc: 0.4164\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.0943 Acc: 0.7488\n",
      "val Loss: 0.1546 Acc: 0.4031\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.7504\n",
      "val Loss: 0.1662 Acc: 0.4019\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0901 Acc: 0.7580\n",
      "val Loss: 0.1558 Acc: 0.4144\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0824 Acc: 0.7661\n",
      "val Loss: 0.1459 Acc: 0.4281\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.7638\n",
      "val Loss: 0.1363 Acc: 0.4350\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0803 Acc: 0.7691\n",
      "val Loss: 0.1420 Acc: 0.4147\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.7805\n",
      "val Loss: 0.1316 Acc: 0.4206\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0737 Acc: 0.7784\n",
      "val Loss: 0.1413 Acc: 0.4142\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0674 Acc: 0.7854\n",
      "val Loss: 0.1364 Acc: 0.4206\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0659 Acc: 0.7866\n",
      "val Loss: 0.1413 Acc: 0.4044\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 0.7832\n",
      "val Loss: 0.1231 Acc: 0.4281\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0656 Acc: 0.7902\n",
      "val Loss: 0.1251 Acc: 0.4203\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0603 Acc: 0.7938\n",
      "val Loss: 0.1285 Acc: 0.4192\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0661 Acc: 0.7922\n",
      "val Loss: 0.1305 Acc: 0.4194\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0643 Acc: 0.7920\n",
      "val Loss: 0.1473 Acc: 0.4044\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0609 Acc: 0.7959\n",
      "val Loss: 0.1263 Acc: 0.4114\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0610 Acc: 0.7978\n",
      "val Loss: 0.1261 Acc: 0.4239\n",
      "\n",
      "Best val Acc: 0.490000\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0506 Acc: 0.4877\n",
      "val Loss: 0.0152 Acc: 0.4897\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0113 Acc: 0.4858\n",
      "val Loss: 0.0095 Acc: 0.4881\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0080 Acc: 0.4843\n",
      "val Loss: 0.0076 Acc: 0.4908\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.4836\n",
      "val Loss: 0.0062 Acc: 0.4903\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.2587\n",
      "val Loss: 0.0026 Acc: 0.1603\n",
      "\n",
      "Best val Acc: 0.490833\n",
      "Accuracy on test set using T-Revision Method: 0.9333333333333333\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6912 Acc: 0.4531\n",
      "val Loss: 0.6286 Acc: 0.4817\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6061 Acc: 0.4704\n",
      "val Loss: 0.6796 Acc: 0.4764\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6531 Acc: 0.4728\n",
      "val Loss: 0.6481 Acc: 0.4814\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6531 Acc: 0.4781\n",
      "val Loss: 0.6357 Acc: 0.4672\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6394 Acc: 0.4786\n",
      "val Loss: 0.6032 Acc: 0.4894\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6261 Acc: 0.4802\n",
      "val Loss: 0.6951 Acc: 0.4811\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6384 Acc: 0.4829\n",
      "val Loss: 0.6191 Acc: 0.4886\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6167 Acc: 0.4865\n",
      "val Loss: 0.6534 Acc: 0.4761\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6119 Acc: 0.4860\n",
      "val Loss: 0.6480 Acc: 0.4842\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6127 Acc: 0.4854\n",
      "val Loss: 0.6225 Acc: 0.4906\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.6059 Acc: 0.4902\n",
      "val Loss: 0.5247 Acc: 0.4847\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5699 Acc: 0.4933\n",
      "val Loss: 0.5930 Acc: 0.4753\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5664 Acc: 0.4945\n",
      "val Loss: 0.6009 Acc: 0.4781\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5480 Acc: 0.4963\n",
      "val Loss: 0.5327 Acc: 0.4739\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5431 Acc: 0.5015\n",
      "val Loss: 0.5677 Acc: 0.4614\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5015 Acc: 0.5076\n",
      "val Loss: 0.5695 Acc: 0.4811\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.5006 Acc: 0.5109\n",
      "val Loss: 0.5052 Acc: 0.4694\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.4735 Acc: 0.5168\n",
      "val Loss: 0.5079 Acc: 0.4658\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4550 Acc: 0.5212\n",
      "val Loss: 0.4176 Acc: 0.4683\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.4248 Acc: 0.5322\n",
      "val Loss: 0.4380 Acc: 0.4708\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.4073 Acc: 0.5391\n",
      "val Loss: 0.4111 Acc: 0.4533\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.3774 Acc: 0.5472\n",
      "val Loss: 0.4252 Acc: 0.4503\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3660 Acc: 0.5540\n",
      "val Loss: 0.3917 Acc: 0.4625\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3333 Acc: 0.5669\n",
      "val Loss: 0.3582 Acc: 0.4583\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.3140 Acc: 0.5724\n",
      "val Loss: 0.2890 Acc: 0.4517\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.3016 Acc: 0.5861\n",
      "val Loss: 0.3289 Acc: 0.4450\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2887 Acc: 0.5913\n",
      "val Loss: 0.3564 Acc: 0.4489\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2713 Acc: 0.6024\n",
      "val Loss: 0.3348 Acc: 0.4272\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2608 Acc: 0.6137\n",
      "val Loss: 0.2536 Acc: 0.4431\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.2338 Acc: 0.6248\n",
      "val Loss: 0.2417 Acc: 0.4519\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.2240 Acc: 0.6269\n",
      "val Loss: 0.2801 Acc: 0.4256\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.2138 Acc: 0.6317\n",
      "val Loss: 0.2561 Acc: 0.4303\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1967 Acc: 0.6491\n",
      "val Loss: 0.1817 Acc: 0.4453\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1981 Acc: 0.6470\n",
      "val Loss: 0.2002 Acc: 0.4414\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1800 Acc: 0.6642\n",
      "val Loss: 0.1825 Acc: 0.4333\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1620 Acc: 0.6756\n",
      "val Loss: 0.2384 Acc: 0.4264\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1598 Acc: 0.6756\n",
      "val Loss: 0.1902 Acc: 0.4436\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1500 Acc: 0.6829\n",
      "val Loss: 0.2202 Acc: 0.4353\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1425 Acc: 0.6942\n",
      "val Loss: 0.1737 Acc: 0.4303\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.6954\n",
      "val Loss: 0.1664 Acc: 0.4158\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1175 Acc: 0.7076\n",
      "val Loss: 0.1865 Acc: 0.4297\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1208 Acc: 0.7108\n",
      "val Loss: 0.1761 Acc: 0.4250\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1096 Acc: 0.7137\n",
      "val Loss: 0.1406 Acc: 0.4286\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.7240\n",
      "val Loss: 0.1341 Acc: 0.4372\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.7265\n",
      "val Loss: 0.1575 Acc: 0.4186\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.7312\n",
      "val Loss: 0.1562 Acc: 0.4328\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0934 Acc: 0.7356\n",
      "val Loss: 0.1509 Acc: 0.4081\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0940 Acc: 0.7385\n",
      "val Loss: 0.1374 Acc: 0.4186\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0937 Acc: 0.7381\n",
      "val Loss: 0.1517 Acc: 0.4058\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0831 Acc: 0.7482\n",
      "val Loss: 0.1384 Acc: 0.4131\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0811 Acc: 0.7544\n",
      "val Loss: 0.1144 Acc: 0.4278\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0809 Acc: 0.7522\n",
      "val Loss: 0.1217 Acc: 0.4206\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0792 Acc: 0.7550\n",
      "val Loss: 0.1158 Acc: 0.4264\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0753 Acc: 0.7603\n",
      "val Loss: 0.1444 Acc: 0.4172\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0706 Acc: 0.7668\n",
      "val Loss: 0.1335 Acc: 0.4097\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 0.7688\n",
      "val Loss: 0.1364 Acc: 0.4039\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0661 Acc: 0.7723\n",
      "val Loss: 0.1387 Acc: 0.4144\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0627 Acc: 0.7772\n",
      "val Loss: 0.1116 Acc: 0.4239\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0626 Acc: 0.7783\n",
      "val Loss: 0.1384 Acc: 0.4050\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0653 Acc: 0.7786\n",
      "val Loss: 0.1161 Acc: 0.4192\n",
      "\n",
      "Best val Acc: 0.490556\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0489 Acc: 0.4894\n",
      "val Loss: 0.0144 Acc: 0.4903\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0103 Acc: 0.4878\n",
      "val Loss: 0.0096 Acc: 0.4900\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.4874\n",
      "val Loss: 0.0062 Acc: 0.4900\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.4853\n",
      "val Loss: 0.0036 Acc: 0.3375\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.4875\n",
      "val Loss: 0.0030 Acc: 0.3375\n",
      "\n",
      "Best val Acc: 0.490278\n",
      "Accuracy on test set using T-Revision Method: 0.9436666666666667\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6333 Acc: 0.4556\n",
      "val Loss: 0.5967 Acc: 0.4811\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6655 Acc: 0.4683\n",
      "val Loss: 0.6684 Acc: 0.4806\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6456 Acc: 0.4756\n",
      "val Loss: 0.5816 Acc: 0.4753\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6425 Acc: 0.4753\n",
      "val Loss: 0.6073 Acc: 0.4811\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6396 Acc: 0.4819\n",
      "val Loss: 0.6539 Acc: 0.4847\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6435 Acc: 0.4824\n",
      "val Loss: 0.6363 Acc: 0.4878\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6282 Acc: 0.4813\n",
      "val Loss: 0.7276 Acc: 0.4597\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6413 Acc: 0.4817\n",
      "val Loss: 0.6362 Acc: 0.4881\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6290 Acc: 0.4849\n",
      "val Loss: 0.6399 Acc: 0.4786\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6109 Acc: 0.4865\n",
      "val Loss: 0.6753 Acc: 0.4800\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.6099 Acc: 0.4860\n",
      "val Loss: 0.6486 Acc: 0.4728\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.6169 Acc: 0.4903\n",
      "val Loss: 0.5560 Acc: 0.4867\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5754 Acc: 0.4913\n",
      "val Loss: 0.5939 Acc: 0.4739\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5878 Acc: 0.4915\n",
      "val Loss: 0.6169 Acc: 0.4769\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5602 Acc: 0.4994\n",
      "val Loss: 0.5969 Acc: 0.4561\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5463 Acc: 0.5017\n",
      "val Loss: 0.6320 Acc: 0.4614\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.5318 Acc: 0.5061\n",
      "val Loss: 0.5481 Acc: 0.4650\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.5003 Acc: 0.5133\n",
      "val Loss: 0.5192 Acc: 0.4736\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4907 Acc: 0.5179\n",
      "val Loss: 0.5329 Acc: 0.4717\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.4576 Acc: 0.5249\n",
      "val Loss: 0.4715 Acc: 0.4672\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.4362 Acc: 0.5329\n",
      "val Loss: 0.3969 Acc: 0.4622\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.4060 Acc: 0.5413\n",
      "val Loss: 0.3707 Acc: 0.4650\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3654 Acc: 0.5562\n",
      "val Loss: 0.3359 Acc: 0.4586\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3537 Acc: 0.5631\n",
      "val Loss: 0.3858 Acc: 0.4497\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.3265 Acc: 0.5805\n",
      "val Loss: 0.3153 Acc: 0.4622\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2977 Acc: 0.5874\n",
      "val Loss: 0.2713 Acc: 0.4272\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2749 Acc: 0.6047\n",
      "val Loss: 0.2787 Acc: 0.4456\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2582 Acc: 0.6136\n",
      "val Loss: 0.3001 Acc: 0.4250\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2468 Acc: 0.6194\n",
      "val Loss: 0.3080 Acc: 0.4278\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.2281 Acc: 0.6377\n",
      "val Loss: 0.3147 Acc: 0.4225\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.6460\n",
      "val Loss: 0.2211 Acc: 0.4464\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1892 Acc: 0.6614\n",
      "val Loss: 0.2480 Acc: 0.4303\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.6718\n",
      "val Loss: 0.2294 Acc: 0.4197\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1681 Acc: 0.6724\n",
      "val Loss: 0.2696 Acc: 0.4006\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1589 Acc: 0.6827\n",
      "val Loss: 0.2165 Acc: 0.4197\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1512 Acc: 0.6913\n",
      "val Loss: 0.2241 Acc: 0.4139\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1425 Acc: 0.6973\n",
      "val Loss: 0.1982 Acc: 0.4247\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1383 Acc: 0.7031\n",
      "val Loss: 0.1932 Acc: 0.4181\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1264 Acc: 0.7103\n",
      "val Loss: 0.1768 Acc: 0.4211\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1220 Acc: 0.7202\n",
      "val Loss: 0.1830 Acc: 0.4014\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1136 Acc: 0.7210\n",
      "val Loss: 0.1937 Acc: 0.4053\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1145 Acc: 0.7258\n",
      "val Loss: 0.1481 Acc: 0.4281\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1046 Acc: 0.7350\n",
      "val Loss: 0.1591 Acc: 0.4250\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1033 Acc: 0.7369\n",
      "val Loss: 0.1743 Acc: 0.3983\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.0920 Acc: 0.7459\n",
      "val Loss: 0.1511 Acc: 0.4131\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0898 Acc: 0.7569\n",
      "val Loss: 0.1559 Acc: 0.4275\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0893 Acc: 0.7512\n",
      "val Loss: 0.1766 Acc: 0.4033\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0820 Acc: 0.7612\n",
      "val Loss: 0.1679 Acc: 0.4025\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.7666\n",
      "val Loss: 0.1629 Acc: 0.4050\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0802 Acc: 0.7690\n",
      "val Loss: 0.1213 Acc: 0.4056\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0774 Acc: 0.7671\n",
      "val Loss: 0.1366 Acc: 0.4100\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0725 Acc: 0.7750\n",
      "val Loss: 0.1342 Acc: 0.4119\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.7753\n",
      "val Loss: 0.1272 Acc: 0.4161\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0645 Acc: 0.7823\n",
      "val Loss: 0.1364 Acc: 0.4139\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0665 Acc: 0.7811\n",
      "val Loss: 0.1272 Acc: 0.4228\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0624 Acc: 0.7869\n",
      "val Loss: 0.1357 Acc: 0.4106\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0601 Acc: 0.7944\n",
      "val Loss: 0.1390 Acc: 0.4097\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0587 Acc: 0.7966\n",
      "val Loss: 0.1331 Acc: 0.4075\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0568 Acc: 0.7950\n",
      "val Loss: 0.1252 Acc: 0.4106\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0613 Acc: 0.7951\n",
      "val Loss: 0.1227 Acc: 0.4142\n",
      "\n",
      "Best val Acc: 0.488056\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0461 Acc: 0.4855\n",
      "val Loss: 0.0136 Acc: 0.4892\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0102 Acc: 0.4848\n",
      "val Loss: 0.0086 Acc: 0.4872\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.4825\n",
      "val Loss: 0.0072 Acc: 0.4917\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.4801\n",
      "val Loss: 0.0031 Acc: 0.3339\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0040 Acc: 0.4757\n",
      "val Loss: 0.0030 Acc: 0.3314\n",
      "\n",
      "Best val Acc: 0.491667\n",
      "Accuracy on test set using T-Revision Method: 0.9283333333333333\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6859 Acc: 0.4587\n",
      "val Loss: 0.7172 Acc: 0.4739\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6627 Acc: 0.4724\n",
      "val Loss: 0.6332 Acc: 0.4831\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6550 Acc: 0.4780\n",
      "val Loss: 0.6794 Acc: 0.4847\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6519 Acc: 0.4767\n",
      "val Loss: 0.6660 Acc: 0.4914\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6555 Acc: 0.4812\n",
      "val Loss: 0.6507 Acc: 0.4831\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6326 Acc: 0.4818\n",
      "val Loss: 0.6606 Acc: 0.4894\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6434 Acc: 0.4842\n",
      "val Loss: 0.5503 Acc: 0.4903\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6250 Acc: 0.4845\n",
      "val Loss: 0.7166 Acc: 0.4533\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6342 Acc: 0.4838\n",
      "val Loss: 0.6167 Acc: 0.4883\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6167 Acc: 0.4890\n",
      "val Loss: 0.6332 Acc: 0.4842\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.5974 Acc: 0.4905\n",
      "val Loss: 0.6225 Acc: 0.4803\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.4910\n",
      "val Loss: 0.6653 Acc: 0.4758\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5912 Acc: 0.4951\n",
      "val Loss: 0.5913 Acc: 0.4750\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5744 Acc: 0.4954\n",
      "val Loss: 0.5508 Acc: 0.4800\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5464 Acc: 0.5030\n",
      "val Loss: 0.5957 Acc: 0.4669\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5439 Acc: 0.5041\n",
      "val Loss: 0.5293 Acc: 0.4694\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.5189 Acc: 0.5118\n",
      "val Loss: 0.4484 Acc: 0.4736\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.5015 Acc: 0.5156\n",
      "val Loss: 0.5399 Acc: 0.4633\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4735 Acc: 0.5290\n",
      "val Loss: 0.5179 Acc: 0.4608\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.4446 Acc: 0.5327\n",
      "val Loss: 0.4928 Acc: 0.4517\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.4257 Acc: 0.5452\n",
      "val Loss: 0.4308 Acc: 0.4639\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.4019 Acc: 0.5530\n",
      "val Loss: 0.4275 Acc: 0.4489\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3732 Acc: 0.5684\n",
      "val Loss: 0.4241 Acc: 0.4367\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3554 Acc: 0.5697\n",
      "val Loss: 0.3944 Acc: 0.4342\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.3323 Acc: 0.5890\n",
      "val Loss: 0.3415 Acc: 0.4281\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.3057 Acc: 0.5990\n",
      "val Loss: 0.3463 Acc: 0.4400\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.6075\n",
      "val Loss: 0.2967 Acc: 0.4397\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2766 Acc: 0.6199\n",
      "val Loss: 0.2585 Acc: 0.4369\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2518 Acc: 0.6294\n",
      "val Loss: 0.2958 Acc: 0.4133\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.2380 Acc: 0.6356\n",
      "val Loss: 0.2520 Acc: 0.4281\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.2197 Acc: 0.6517\n",
      "val Loss: 0.2427 Acc: 0.4508\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.2047 Acc: 0.6587\n",
      "val Loss: 0.2965 Acc: 0.4131\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1950 Acc: 0.6647\n",
      "val Loss: 0.2460 Acc: 0.4408\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1868 Acc: 0.6731\n",
      "val Loss: 0.2620 Acc: 0.4186\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1705 Acc: 0.6878\n",
      "val Loss: 0.1917 Acc: 0.4322\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.7014\n",
      "val Loss: 0.2055 Acc: 0.4167\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1539 Acc: 0.7081\n",
      "val Loss: 0.2193 Acc: 0.4169\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1488 Acc: 0.7085\n",
      "val Loss: 0.2121 Acc: 0.4144\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1298 Acc: 0.7271\n",
      "val Loss: 0.2001 Acc: 0.4200\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1213 Acc: 0.7308\n",
      "val Loss: 0.1748 Acc: 0.4158\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1229 Acc: 0.7297\n",
      "val Loss: 0.1713 Acc: 0.4175\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1107 Acc: 0.7349\n",
      "val Loss: 0.1851 Acc: 0.4264\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1097 Acc: 0.7434\n",
      "val Loss: 0.2054 Acc: 0.4133\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1090 Acc: 0.7429\n",
      "val Loss: 0.1848 Acc: 0.4217\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 0.7515\n",
      "val Loss: 0.1721 Acc: 0.4200\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0981 Acc: 0.7556\n",
      "val Loss: 0.1515 Acc: 0.4342\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.7661\n",
      "val Loss: 0.1687 Acc: 0.4244\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0937 Acc: 0.7606\n",
      "val Loss: 0.1722 Acc: 0.4153\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0835 Acc: 0.7701\n",
      "val Loss: 0.1605 Acc: 0.4181\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0790 Acc: 0.7783\n",
      "val Loss: 0.1560 Acc: 0.4231\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0740 Acc: 0.7860\n",
      "val Loss: 0.1178 Acc: 0.4181\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0728 Acc: 0.7858\n",
      "val Loss: 0.1407 Acc: 0.4142\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0674 Acc: 0.7933\n",
      "val Loss: 0.1548 Acc: 0.4250\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0711 Acc: 0.7901\n",
      "val Loss: 0.1390 Acc: 0.4261\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0675 Acc: 0.7964\n",
      "val Loss: 0.1334 Acc: 0.4192\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0633 Acc: 0.7987\n",
      "val Loss: 0.1220 Acc: 0.4231\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0658 Acc: 0.7932\n",
      "val Loss: 0.1358 Acc: 0.4097\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0644 Acc: 0.7968\n",
      "val Loss: 0.1332 Acc: 0.4103\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0588 Acc: 0.8054\n",
      "val Loss: 0.1561 Acc: 0.4067\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0543 Acc: 0.8122\n",
      "val Loss: 0.1394 Acc: 0.4075\n",
      "\n",
      "Best val Acc: 0.491389\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0478 Acc: 0.4797\n",
      "val Loss: 0.0128 Acc: 0.4883\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0100 Acc: 0.4806\n",
      "val Loss: 0.0072 Acc: 0.4853\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.4785\n",
      "val Loss: 0.0065 Acc: 0.4856\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.4795\n",
      "val Loss: 0.0058 Acc: 0.4844\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.2638\n",
      "val Loss: 0.0035 Acc: 0.3233\n",
      "\n",
      "Best val Acc: 0.488333\n",
      "Accuracy on test set using T-Revision Method: 0.9373333333333334\n",
      "\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset fashionmnist0.6 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "..............................\n",
      "\n",
      "Training iteration: 0\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6145 Acc: 0.3756\n",
      "val Loss: 0.5200 Acc: 0.3686\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4303 Acc: 0.3896\n",
      "val Loss: 0.3305 Acc: 0.3958\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.2996 Acc: 0.3944\n",
      "val Loss: 0.2986 Acc: 0.3997\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3358 Acc: 0.3924\n",
      "val Loss: 0.3319 Acc: 0.3833\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.2698 Acc: 0.3935\n",
      "val Loss: 0.2695 Acc: 0.3939\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.2387 Acc: 0.3983\n",
      "val Loss: 0.2004 Acc: 0.3950\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.2653 Acc: 0.3931\n",
      "val Loss: 0.2670 Acc: 0.3928\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2447 Acc: 0.3959\n",
      "val Loss: 0.2280 Acc: 0.4006\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2581 Acc: 0.3970\n",
      "val Loss: 0.2146 Acc: 0.3972\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2113 Acc: 0.3999\n",
      "val Loss: 0.1946 Acc: 0.4017\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2166 Acc: 0.3991\n",
      "val Loss: 0.1654 Acc: 0.4006\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2009 Acc: 0.3989\n",
      "val Loss: 0.2030 Acc: 0.3883\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2449 Acc: 0.3944\n",
      "val Loss: 0.2299 Acc: 0.3861\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2466 Acc: 0.3976\n",
      "val Loss: 0.1807 Acc: 0.3900\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2177 Acc: 0.4004\n",
      "val Loss: 0.1918 Acc: 0.3967\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2184 Acc: 0.4035\n",
      "val Loss: 0.2208 Acc: 0.3775\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2350 Acc: 0.4024\n",
      "val Loss: 0.2080 Acc: 0.3989\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2161 Acc: 0.4058\n",
      "val Loss: 0.2083 Acc: 0.3867\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.1925 Acc: 0.4083\n",
      "val Loss: 0.1713 Acc: 0.4003\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.1819 Acc: 0.4100\n",
      "val Loss: 0.2246 Acc: 0.3878\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2084 Acc: 0.4144\n",
      "val Loss: 0.2052 Acc: 0.3833\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1731 Acc: 0.4155\n",
      "val Loss: 0.1515 Acc: 0.3922\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1914 Acc: 0.4169\n",
      "val Loss: 0.1794 Acc: 0.3883\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1917 Acc: 0.4224\n",
      "val Loss: 0.1890 Acc: 0.3792\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1959 Acc: 0.4207\n",
      "val Loss: 0.1647 Acc: 0.3836\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1819 Acc: 0.4294\n",
      "val Loss: 0.1482 Acc: 0.3894\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1797 Acc: 0.4262\n",
      "val Loss: 0.1771 Acc: 0.3792\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1708 Acc: 0.4363\n",
      "val Loss: 0.1585 Acc: 0.3831\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1664 Acc: 0.4399\n",
      "val Loss: 0.1875 Acc: 0.3725\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1689 Acc: 0.4415\n",
      "val Loss: 0.1661 Acc: 0.3828\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1759 Acc: 0.4470\n",
      "val Loss: 0.1535 Acc: 0.3792\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1679 Acc: 0.4517\n",
      "val Loss: 0.1766 Acc: 0.3714\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.4615\n",
      "val Loss: 0.1553 Acc: 0.3833\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1531 Acc: 0.4688\n",
      "val Loss: 0.1504 Acc: 0.3733\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1481 Acc: 0.4716\n",
      "val Loss: 0.1377 Acc: 0.3747\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1530 Acc: 0.4786\n",
      "val Loss: 0.1562 Acc: 0.3672\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1430 Acc: 0.4851\n",
      "val Loss: 0.1315 Acc: 0.3753\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1356 Acc: 0.4935\n",
      "val Loss: 0.1389 Acc: 0.3719\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1296 Acc: 0.5017\n",
      "val Loss: 0.1290 Acc: 0.3744\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1298 Acc: 0.5072\n",
      "val Loss: 0.1378 Acc: 0.3728\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1245 Acc: 0.5110\n",
      "val Loss: 0.1293 Acc: 0.3719\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1176 Acc: 0.5214\n",
      "val Loss: 0.1301 Acc: 0.3681\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1214 Acc: 0.5212\n",
      "val Loss: 0.1394 Acc: 0.3644\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1153 Acc: 0.5301\n",
      "val Loss: 0.1175 Acc: 0.3625\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1123 Acc: 0.5327\n",
      "val Loss: 0.1303 Acc: 0.3714\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1082 Acc: 0.5394\n",
      "val Loss: 0.1370 Acc: 0.3592\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1105 Acc: 0.5444\n",
      "val Loss: 0.1136 Acc: 0.3650\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1062 Acc: 0.5472\n",
      "val Loss: 0.1100 Acc: 0.3739\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1071 Acc: 0.5578\n",
      "val Loss: 0.1174 Acc: 0.3692\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.5639\n",
      "val Loss: 0.1089 Acc: 0.3758\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1053 Acc: 0.5595\n",
      "val Loss: 0.1376 Acc: 0.3667\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1029 Acc: 0.5742\n",
      "val Loss: 0.1208 Acc: 0.3642\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0981 Acc: 0.5800\n",
      "val Loss: 0.1235 Acc: 0.3553\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1037 Acc: 0.5837\n",
      "val Loss: 0.1302 Acc: 0.3633\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0999 Acc: 0.5853\n",
      "val Loss: 0.1080 Acc: 0.3617\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0989 Acc: 0.5897\n",
      "val Loss: 0.1097 Acc: 0.3708\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0948 Acc: 0.6004\n",
      "val Loss: 0.1082 Acc: 0.3553\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0926 Acc: 0.6072\n",
      "val Loss: 0.1200 Acc: 0.3497\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0896 Acc: 0.6085\n",
      "val Loss: 0.1189 Acc: 0.3553\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0948 Acc: 0.6119\n",
      "val Loss: 0.1081 Acc: 0.3517\n",
      "\n",
      "Best val Acc: 0.401667\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0348 Acc: 0.4021\n",
      "val Loss: 0.0124 Acc: 0.4000\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.3987\n",
      "val Loss: 0.0075 Acc: 0.3989\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.3957\n",
      "val Loss: 0.0048 Acc: 0.4022\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3940\n",
      "val Loss: 0.0042 Acc: 0.3997\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.3594\n",
      "val Loss: 0.0032 Acc: 0.4006\n",
      "\n",
      "Best val Acc: 0.402222\n",
      "Accuracy on test set using T-Revision Method: 0.8736666666666667\n",
      "\n",
      "\n",
      "Training iteration: 1\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6545 Acc: 0.3691\n",
      "val Loss: 0.5903 Acc: 0.3761\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4670 Acc: 0.3853\n",
      "val Loss: 0.3776 Acc: 0.3925\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.4185 Acc: 0.3880\n",
      "val Loss: 0.2841 Acc: 0.3917\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3435 Acc: 0.3898\n",
      "val Loss: 0.3318 Acc: 0.3975\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3353 Acc: 0.3928\n",
      "val Loss: 0.3064 Acc: 0.3825\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.3935\n",
      "val Loss: 0.3141 Acc: 0.3928\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.2866 Acc: 0.3947\n",
      "val Loss: 0.2367 Acc: 0.3997\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2851 Acc: 0.3978\n",
      "val Loss: 0.2966 Acc: 0.3933\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.3026 Acc: 0.3947\n",
      "val Loss: 0.3103 Acc: 0.3867\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.3015 Acc: 0.3958\n",
      "val Loss: 0.2362 Acc: 0.3989\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2640 Acc: 0.3923\n",
      "val Loss: 0.2912 Acc: 0.3961\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2708 Acc: 0.3992\n",
      "val Loss: 0.2317 Acc: 0.3986\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2507 Acc: 0.4013\n",
      "val Loss: 0.2456 Acc: 0.3894\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2442 Acc: 0.4037\n",
      "val Loss: 0.3308 Acc: 0.3744\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2372 Acc: 0.4051\n",
      "val Loss: 0.2164 Acc: 0.3956\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2438 Acc: 0.4034\n",
      "val Loss: 0.1942 Acc: 0.3969\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2072 Acc: 0.4074\n",
      "val Loss: 0.1936 Acc: 0.3958\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.1919 Acc: 0.4074\n",
      "val Loss: 0.1695 Acc: 0.4008\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2054 Acc: 0.4099\n",
      "val Loss: 0.1593 Acc: 0.3931\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.1788 Acc: 0.4150\n",
      "val Loss: 0.1704 Acc: 0.3897\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.1814 Acc: 0.4144\n",
      "val Loss: 0.1803 Acc: 0.3942\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1891 Acc: 0.4187\n",
      "val Loss: 0.1846 Acc: 0.3839\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1845 Acc: 0.4219\n",
      "val Loss: 0.2422 Acc: 0.3872\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1990 Acc: 0.4181\n",
      "val Loss: 0.2360 Acc: 0.3850\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1861 Acc: 0.4236\n",
      "val Loss: 0.1820 Acc: 0.3961\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1892 Acc: 0.4302\n",
      "val Loss: 0.2070 Acc: 0.3942\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1783 Acc: 0.4339\n",
      "val Loss: 0.1641 Acc: 0.3844\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1826 Acc: 0.4338\n",
      "val Loss: 0.1319 Acc: 0.3903\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1704 Acc: 0.4383\n",
      "val Loss: 0.1479 Acc: 0.3969\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1745 Acc: 0.4428\n",
      "val Loss: 0.1998 Acc: 0.3814\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.4478\n",
      "val Loss: 0.1922 Acc: 0.3897\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1777 Acc: 0.4583\n",
      "val Loss: 0.1641 Acc: 0.3792\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1654 Acc: 0.4595\n",
      "val Loss: 0.1435 Acc: 0.3933\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.4673\n",
      "val Loss: 0.1396 Acc: 0.3861\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1644 Acc: 0.4704\n",
      "val Loss: 0.1321 Acc: 0.3847\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1445 Acc: 0.4751\n",
      "val Loss: 0.2056 Acc: 0.3736\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1545 Acc: 0.4753\n",
      "val Loss: 0.1544 Acc: 0.3808\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1454 Acc: 0.4875\n",
      "val Loss: 0.1424 Acc: 0.3808\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.4911\n",
      "val Loss: 0.1516 Acc: 0.3761\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1428 Acc: 0.4983\n",
      "val Loss: 0.1598 Acc: 0.3764\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1341 Acc: 0.5084\n",
      "val Loss: 0.1603 Acc: 0.3669\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1294 Acc: 0.5171\n",
      "val Loss: 0.1165 Acc: 0.3828\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1293 Acc: 0.5164\n",
      "val Loss: 0.1319 Acc: 0.3792\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1242 Acc: 0.5263\n",
      "val Loss: 0.1502 Acc: 0.3706\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1197 Acc: 0.5272\n",
      "val Loss: 0.1267 Acc: 0.3731\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1168 Acc: 0.5356\n",
      "val Loss: 0.1263 Acc: 0.3711\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1076 Acc: 0.5450\n",
      "val Loss: 0.1170 Acc: 0.3700\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1135 Acc: 0.5493\n",
      "val Loss: 0.1208 Acc: 0.3714\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1025 Acc: 0.5569\n",
      "val Loss: 0.1350 Acc: 0.3683\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1035 Acc: 0.5626\n",
      "val Loss: 0.0986 Acc: 0.3764\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0925 Acc: 0.5717\n",
      "val Loss: 0.1209 Acc: 0.3689\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0977 Acc: 0.5726\n",
      "val Loss: 0.1172 Acc: 0.3689\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0992 Acc: 0.5751\n",
      "val Loss: 0.1188 Acc: 0.3781\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.5820\n",
      "val Loss: 0.1023 Acc: 0.3769\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0901 Acc: 0.5903\n",
      "val Loss: 0.1163 Acc: 0.3572\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0863 Acc: 0.5940\n",
      "val Loss: 0.1116 Acc: 0.3653\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0845 Acc: 0.6046\n",
      "val Loss: 0.1189 Acc: 0.3656\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.6042\n",
      "val Loss: 0.1197 Acc: 0.3681\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0896 Acc: 0.6057\n",
      "val Loss: 0.1090 Acc: 0.3689\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0850 Acc: 0.6099\n",
      "val Loss: 0.0988 Acc: 0.3719\n",
      "\n",
      "Best val Acc: 0.400833\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0357 Acc: 0.4097\n",
      "val Loss: 0.0159 Acc: 0.3986\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0143 Acc: 0.4065\n",
      "val Loss: 0.0092 Acc: 0.3997\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0093 Acc: 0.4049\n",
      "val Loss: 0.0063 Acc: 0.3994\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.4036\n",
      "val Loss: 0.0040 Acc: 0.3986\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 0.3219\n",
      "val Loss: 0.0032 Acc: 0.3992\n",
      "\n",
      "Best val Acc: 0.399722\n",
      "Accuracy on test set using T-Revision Method: 0.8816666666666667\n",
      "\n",
      "\n",
      "Training iteration: 2\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.3779\n",
      "val Loss: 0.4678 Acc: 0.3908\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4298 Acc: 0.3859\n",
      "val Loss: 0.3669 Acc: 0.3936\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3355 Acc: 0.3906\n",
      "val Loss: 0.2786 Acc: 0.3997\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3032 Acc: 0.3912\n",
      "val Loss: 0.3468 Acc: 0.3372\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3088 Acc: 0.3934\n",
      "val Loss: 0.2786 Acc: 0.3994\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.2947 Acc: 0.3940\n",
      "val Loss: 0.2522 Acc: 0.3961\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.2660 Acc: 0.3942\n",
      "val Loss: 0.3361 Acc: 0.3894\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.3208 Acc: 0.3951\n",
      "val Loss: 0.2230 Acc: 0.3975\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2697 Acc: 0.3917\n",
      "val Loss: 0.2491 Acc: 0.4003\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2692 Acc: 0.3958\n",
      "val Loss: 0.3190 Acc: 0.3947\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2925 Acc: 0.3973\n",
      "val Loss: 0.2360 Acc: 0.4006\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2843 Acc: 0.4008\n",
      "val Loss: 0.2366 Acc: 0.3956\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2327 Acc: 0.3975\n",
      "val Loss: 0.2351 Acc: 0.3967\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2367 Acc: 0.4025\n",
      "val Loss: 0.2660 Acc: 0.3806\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2202 Acc: 0.4008\n",
      "val Loss: 0.2086 Acc: 0.3989\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2408 Acc: 0.4040\n",
      "val Loss: 0.2557 Acc: 0.3914\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2293 Acc: 0.4071\n",
      "val Loss: 0.2032 Acc: 0.3967\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.1833 Acc: 0.4101\n",
      "val Loss: 0.2205 Acc: 0.3928\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2258 Acc: 0.4051\n",
      "val Loss: 0.1837 Acc: 0.3975\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.1874 Acc: 0.4118\n",
      "val Loss: 0.2602 Acc: 0.3786\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2156 Acc: 0.4133\n",
      "val Loss: 0.1457 Acc: 0.3950\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1737 Acc: 0.4156\n",
      "val Loss: 0.1640 Acc: 0.3992\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1792 Acc: 0.4182\n",
      "val Loss: 0.1623 Acc: 0.3917\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1837 Acc: 0.4175\n",
      "val Loss: 0.1811 Acc: 0.3894\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.4207\n",
      "val Loss: 0.2018 Acc: 0.4028\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1978 Acc: 0.4297\n",
      "val Loss: 0.2210 Acc: 0.3872\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2068 Acc: 0.4254\n",
      "val Loss: 0.1681 Acc: 0.3944\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1657 Acc: 0.4369\n",
      "val Loss: 0.2243 Acc: 0.3817\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1839 Acc: 0.4347\n",
      "val Loss: 0.1630 Acc: 0.3975\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1574 Acc: 0.4385\n",
      "val Loss: 0.1874 Acc: 0.3875\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1955 Acc: 0.4417\n",
      "val Loss: 0.1500 Acc: 0.3925\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.2001 Acc: 0.4440\n",
      "val Loss: 0.1497 Acc: 0.3906\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1894 Acc: 0.4507\n",
      "val Loss: 0.1860 Acc: 0.3878\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1803 Acc: 0.4581\n",
      "val Loss: 0.2390 Acc: 0.3822\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1644 Acc: 0.4613\n",
      "val Loss: 0.1778 Acc: 0.3908\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1732 Acc: 0.4646\n",
      "val Loss: 0.1734 Acc: 0.3836\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1642 Acc: 0.4694\n",
      "val Loss: 0.2020 Acc: 0.3819\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1819 Acc: 0.4736\n",
      "val Loss: 0.1674 Acc: 0.3825\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1586 Acc: 0.4831\n",
      "val Loss: 0.1309 Acc: 0.3883\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1502 Acc: 0.4888\n",
      "val Loss: 0.1332 Acc: 0.3797\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1409 Acc: 0.4944\n",
      "val Loss: 0.1453 Acc: 0.3856\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1462 Acc: 0.4915\n",
      "val Loss: 0.1371 Acc: 0.3819\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1398 Acc: 0.5002\n",
      "val Loss: 0.1657 Acc: 0.3781\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1400 Acc: 0.5046\n",
      "val Loss: 0.1168 Acc: 0.3831\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1366 Acc: 0.5146\n",
      "val Loss: 0.1290 Acc: 0.3800\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1378 Acc: 0.5181\n",
      "val Loss: 0.1493 Acc: 0.3742\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1223 Acc: 0.5285\n",
      "val Loss: 0.1288 Acc: 0.3828\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1256 Acc: 0.5318\n",
      "val Loss: 0.1484 Acc: 0.3669\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1231 Acc: 0.5379\n",
      "val Loss: 0.1395 Acc: 0.3678\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1279 Acc: 0.5395\n",
      "val Loss: 0.1364 Acc: 0.3781\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1287 Acc: 0.5513\n",
      "val Loss: 0.1178 Acc: 0.3761\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1095 Acc: 0.5625\n",
      "val Loss: 0.1407 Acc: 0.3811\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.1093 Acc: 0.5608\n",
      "val Loss: 0.1269 Acc: 0.3697\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1055 Acc: 0.5707\n",
      "val Loss: 0.1312 Acc: 0.3700\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.1077 Acc: 0.5744\n",
      "val Loss: 0.1057 Acc: 0.3714\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.1022 Acc: 0.5858\n",
      "val Loss: 0.1190 Acc: 0.3733\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.1039 Acc: 0.5842\n",
      "val Loss: 0.1117 Acc: 0.3639\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.5906\n",
      "val Loss: 0.1232 Acc: 0.3614\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0952 Acc: 0.5974\n",
      "val Loss: 0.1393 Acc: 0.3658\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.6037\n",
      "val Loss: 0.1218 Acc: 0.3692\n",
      "\n",
      "Best val Acc: 0.402778\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0335 Acc: 0.4276\n",
      "val Loss: 0.0172 Acc: 0.4003\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0129 Acc: 0.4246\n",
      "val Loss: 0.0111 Acc: 0.4022\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0096 Acc: 0.4223\n",
      "val Loss: 0.0084 Acc: 0.4008\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.4215\n",
      "val Loss: 0.0068 Acc: 0.4011\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.4228\n",
      "val Loss: 0.0054 Acc: 0.4028\n",
      "\n",
      "Best val Acc: 0.402778\n",
      "Accuracy on test set using T-Revision Method: 0.866\n",
      "\n",
      "\n",
      "Training iteration: 3\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.5930 Acc: 0.3791\n",
      "val Loss: 0.4622 Acc: 0.3881\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4167 Acc: 0.3897\n",
      "val Loss: 0.3370 Acc: 0.3975\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3525 Acc: 0.3904\n",
      "val Loss: 0.3387 Acc: 0.3817\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3463 Acc: 0.3908\n",
      "val Loss: 0.3073 Acc: 0.4008\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3029 Acc: 0.3928\n",
      "val Loss: 0.3350 Acc: 0.3975\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3332 Acc: 0.3937\n",
      "val Loss: 0.2851 Acc: 0.3947\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3020 Acc: 0.3948\n",
      "val Loss: 0.1853 Acc: 0.4006\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2815 Acc: 0.3929\n",
      "val Loss: 0.2601 Acc: 0.3953\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2453 Acc: 0.3969\n",
      "val Loss: 0.3008 Acc: 0.3908\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2604 Acc: 0.3997\n",
      "val Loss: 0.1959 Acc: 0.3947\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2522 Acc: 0.3987\n",
      "val Loss: 0.2671 Acc: 0.3953\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2666 Acc: 0.4027\n",
      "val Loss: 0.1909 Acc: 0.3917\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2680 Acc: 0.4028\n",
      "val Loss: 0.2604 Acc: 0.3942\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2311 Acc: 0.4096\n",
      "val Loss: 0.1675 Acc: 0.3986\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2323 Acc: 0.4055\n",
      "val Loss: 0.1615 Acc: 0.3897\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2264 Acc: 0.4124\n",
      "val Loss: 0.2045 Acc: 0.3869\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2078 Acc: 0.4151\n",
      "val Loss: 0.1454 Acc: 0.3917\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2069 Acc: 0.4158\n",
      "val Loss: 0.2593 Acc: 0.3897\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2085 Acc: 0.4187\n",
      "val Loss: 0.2989 Acc: 0.3778\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2171 Acc: 0.4231\n",
      "val Loss: 0.1545 Acc: 0.3867\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.1932 Acc: 0.4314\n",
      "val Loss: 0.2233 Acc: 0.3700\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1996 Acc: 0.4347\n",
      "val Loss: 0.2007 Acc: 0.3828\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1876 Acc: 0.4393\n",
      "val Loss: 0.1792 Acc: 0.3856\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1602 Acc: 0.4455\n",
      "val Loss: 0.1620 Acc: 0.3789\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1570 Acc: 0.4499\n",
      "val Loss: 0.1637 Acc: 0.3811\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1556 Acc: 0.4548\n",
      "val Loss: 0.1714 Acc: 0.3769\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1610 Acc: 0.4593\n",
      "val Loss: 0.1571 Acc: 0.3803\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1549 Acc: 0.4662\n",
      "val Loss: 0.1623 Acc: 0.3753\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1485 Acc: 0.4748\n",
      "val Loss: 0.1408 Acc: 0.3828\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1394 Acc: 0.4806\n",
      "val Loss: 0.1418 Acc: 0.3847\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1356 Acc: 0.4875\n",
      "val Loss: 0.1423 Acc: 0.3764\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1349 Acc: 0.4902\n",
      "val Loss: 0.1488 Acc: 0.3811\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1260 Acc: 0.4993\n",
      "val Loss: 0.1291 Acc: 0.3711\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1199 Acc: 0.5028\n",
      "val Loss: 0.1268 Acc: 0.3728\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1195 Acc: 0.5025\n",
      "val Loss: 0.1377 Acc: 0.3714\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1188 Acc: 0.5158\n",
      "val Loss: 0.1213 Acc: 0.3800\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1184 Acc: 0.5208\n",
      "val Loss: 0.1285 Acc: 0.3747\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1206 Acc: 0.5205\n",
      "val Loss: 0.1130 Acc: 0.3714\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1060 Acc: 0.5249\n",
      "val Loss: 0.1335 Acc: 0.3742\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.5359\n",
      "val Loss: 0.1262 Acc: 0.3706\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1110 Acc: 0.5416\n",
      "val Loss: 0.1542 Acc: 0.3794\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1107 Acc: 0.5474\n",
      "val Loss: 0.1382 Acc: 0.3756\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1078 Acc: 0.5521\n",
      "val Loss: 0.1163 Acc: 0.3833\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1105 Acc: 0.5567\n",
      "val Loss: 0.1161 Acc: 0.3747\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1119 Acc: 0.5595\n",
      "val Loss: 0.1098 Acc: 0.3700\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1092 Acc: 0.5623\n",
      "val Loss: 0.1020 Acc: 0.3758\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1055 Acc: 0.5720\n",
      "val Loss: 0.1075 Acc: 0.3647\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1031 Acc: 0.5753\n",
      "val Loss: 0.1145 Acc: 0.3689\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0980 Acc: 0.5806\n",
      "val Loss: 0.1065 Acc: 0.3642\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0978 Acc: 0.5832\n",
      "val Loss: 0.1065 Acc: 0.3789\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0914 Acc: 0.5888\n",
      "val Loss: 0.1160 Acc: 0.3697\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0894 Acc: 0.6016\n",
      "val Loss: 0.1189 Acc: 0.3694\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0819 Acc: 0.6018\n",
      "val Loss: 0.1098 Acc: 0.3700\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0854 Acc: 0.6084\n",
      "val Loss: 0.1003 Acc: 0.3661\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0938 Acc: 0.6083\n",
      "val Loss: 0.1054 Acc: 0.3631\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0877 Acc: 0.6130\n",
      "val Loss: 0.0994 Acc: 0.3689\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0777 Acc: 0.6177\n",
      "val Loss: 0.0956 Acc: 0.3594\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0814 Acc: 0.6222\n",
      "val Loss: 0.1106 Acc: 0.3647\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0752 Acc: 0.6326\n",
      "val Loss: 0.1037 Acc: 0.3736\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.6311\n",
      "val Loss: 0.1041 Acc: 0.3647\n",
      "\n",
      "Best val Acc: 0.400833\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0366 Acc: 0.3944\n",
      "val Loss: 0.0104 Acc: 0.3972\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0094 Acc: 0.3947\n",
      "val Loss: 0.0064 Acc: 0.4003\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.3938\n",
      "val Loss: 0.0045 Acc: 0.4014\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3952\n",
      "val Loss: 0.0031 Acc: 0.3864\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0037 Acc: 0.3935\n",
      "val Loss: 0.0030 Acc: 0.3983\n",
      "\n",
      "Best val Acc: 0.401389\n",
      "Accuracy on test set using T-Revision Method: 0.8773333333333333\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6146 Acc: 0.3699\n",
      "val Loss: 0.4706 Acc: 0.3664\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4211 Acc: 0.3835\n",
      "val Loss: 0.4041 Acc: 0.3919\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.4257 Acc: 0.3809\n",
      "val Loss: 0.4628 Acc: 0.3669\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.4539 Acc: 0.3820\n",
      "val Loss: 0.4290 Acc: 0.3900\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3582 Acc: 0.3931\n",
      "val Loss: 0.3722 Acc: 0.3847\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3421 Acc: 0.3904\n",
      "val Loss: 0.3408 Acc: 0.3911\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3276 Acc: 0.3909\n",
      "val Loss: 0.2974 Acc: 0.3992\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.3310 Acc: 0.3932\n",
      "val Loss: 0.2854 Acc: 0.3953\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.3193 Acc: 0.3942\n",
      "val Loss: 0.3567 Acc: 0.3819\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.3153 Acc: 0.3922\n",
      "val Loss: 0.2745 Acc: 0.4011\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2806 Acc: 0.3953\n",
      "val Loss: 0.2709 Acc: 0.3944\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2978 Acc: 0.3929\n",
      "val Loss: 0.2874 Acc: 0.3936\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2906 Acc: 0.3952\n",
      "val Loss: 0.2764 Acc: 0.4036\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2841 Acc: 0.3997\n",
      "val Loss: 0.2146 Acc: 0.3919\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2475 Acc: 0.3998\n",
      "val Loss: 0.2304 Acc: 0.3947\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2665 Acc: 0.4026\n",
      "val Loss: 0.2137 Acc: 0.3958\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2501 Acc: 0.4010\n",
      "val Loss: 0.2752 Acc: 0.4000\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2459 Acc: 0.4011\n",
      "val Loss: 0.2232 Acc: 0.3917\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2451 Acc: 0.4063\n",
      "val Loss: 0.1921 Acc: 0.3922\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2419 Acc: 0.4090\n",
      "val Loss: 0.3058 Acc: 0.3783\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2444 Acc: 0.4149\n",
      "val Loss: 0.2164 Acc: 0.3906\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2378 Acc: 0.4132\n",
      "val Loss: 0.2554 Acc: 0.3878\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2158 Acc: 0.4194\n",
      "val Loss: 0.2084 Acc: 0.3919\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.2037 Acc: 0.4260\n",
      "val Loss: 0.2287 Acc: 0.3708\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1922 Acc: 0.4219\n",
      "val Loss: 0.2542 Acc: 0.3783\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1854 Acc: 0.4303\n",
      "val Loss: 0.1920 Acc: 0.3858\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1801 Acc: 0.4353\n",
      "val Loss: 0.1627 Acc: 0.3814\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1717 Acc: 0.4405\n",
      "val Loss: 0.1609 Acc: 0.3808\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1565 Acc: 0.4473\n",
      "val Loss: 0.1712 Acc: 0.3803\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1663 Acc: 0.4490\n",
      "val Loss: 0.1609 Acc: 0.3761\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1522 Acc: 0.4558\n",
      "val Loss: 0.1749 Acc: 0.3811\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1439 Acc: 0.4649\n",
      "val Loss: 0.1510 Acc: 0.3756\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1368 Acc: 0.4683\n",
      "val Loss: 0.1427 Acc: 0.3761\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1393 Acc: 0.4710\n",
      "val Loss: 0.1800 Acc: 0.3614\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1446 Acc: 0.4769\n",
      "val Loss: 0.1679 Acc: 0.3797\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1349 Acc: 0.4823\n",
      "val Loss: 0.1162 Acc: 0.3833\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1229 Acc: 0.4906\n",
      "val Loss: 0.1281 Acc: 0.3856\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1170 Acc: 0.4924\n",
      "val Loss: 0.1215 Acc: 0.3886\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1213 Acc: 0.4994\n",
      "val Loss: 0.1276 Acc: 0.3653\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.5030\n",
      "val Loss: 0.0968 Acc: 0.3808\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1270 Acc: 0.4990\n",
      "val Loss: 0.1369 Acc: 0.3767\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1238 Acc: 0.5094\n",
      "val Loss: 0.1387 Acc: 0.3728\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1209 Acc: 0.5178\n",
      "val Loss: 0.1067 Acc: 0.3775\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1064 Acc: 0.5235\n",
      "val Loss: 0.1402 Acc: 0.3694\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1096 Acc: 0.5313\n",
      "val Loss: 0.1139 Acc: 0.3825\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1138 Acc: 0.5357\n",
      "val Loss: 0.1079 Acc: 0.3708\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1129 Acc: 0.5323\n",
      "val Loss: 0.1149 Acc: 0.3769\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1070 Acc: 0.5374\n",
      "val Loss: 0.1340 Acc: 0.3636\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1081 Acc: 0.5490\n",
      "val Loss: 0.1194 Acc: 0.3714\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1046 Acc: 0.5489\n",
      "val Loss: 0.1208 Acc: 0.3686\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1091 Acc: 0.5513\n",
      "val Loss: 0.1078 Acc: 0.3644\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1105 Acc: 0.5511\n",
      "val Loss: 0.1218 Acc: 0.3647\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0990 Acc: 0.5656\n",
      "val Loss: 0.1213 Acc: 0.3742\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1020 Acc: 0.5628\n",
      "val Loss: 0.1105 Acc: 0.3753\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0991 Acc: 0.5693\n",
      "val Loss: 0.1077 Acc: 0.3739\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0947 Acc: 0.5805\n",
      "val Loss: 0.1119 Acc: 0.3697\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0938 Acc: 0.5842\n",
      "val Loss: 0.1158 Acc: 0.3769\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0950 Acc: 0.5883\n",
      "val Loss: 0.1185 Acc: 0.3736\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0913 Acc: 0.5916\n",
      "val Loss: 0.1101 Acc: 0.3758\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0906 Acc: 0.5996\n",
      "val Loss: 0.1124 Acc: 0.3661\n",
      "\n",
      "Best val Acc: 0.403611\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0410 Acc: 0.4003\n",
      "val Loss: 0.0156 Acc: 0.4011\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0136 Acc: 0.4000\n",
      "val Loss: 0.0105 Acc: 0.4031\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0095 Acc: 0.4003\n",
      "val Loss: 0.0071 Acc: 0.4014\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.3978\n",
      "val Loss: 0.0058 Acc: 0.4003\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 0.3988\n",
      "val Loss: 0.0039 Acc: 0.3972\n",
      "\n",
      "Best val Acc: 0.403056\n",
      "Accuracy on test set using T-Revision Method: 0.8863333333333333\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6671 Acc: 0.3710\n",
      "val Loss: 0.4839 Acc: 0.3900\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4576 Acc: 0.3878\n",
      "val Loss: 0.4840 Acc: 0.3867\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3950 Acc: 0.3887\n",
      "val Loss: 0.3803 Acc: 0.3922\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3937 Acc: 0.3881\n",
      "val Loss: 0.3789 Acc: 0.3922\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3638 Acc: 0.3935\n",
      "val Loss: 0.3219 Acc: 0.3978\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3353 Acc: 0.3936\n",
      "val Loss: 0.3820 Acc: 0.3878\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3522 Acc: 0.3904\n",
      "val Loss: 0.2965 Acc: 0.3989\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.3407 Acc: 0.3945\n",
      "val Loss: 0.2717 Acc: 0.3978\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.3089 Acc: 0.3983\n",
      "val Loss: 0.3354 Acc: 0.3842\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2622 Acc: 0.3972\n",
      "val Loss: 0.2961 Acc: 0.3933\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2535 Acc: 0.4014\n",
      "val Loss: 0.3379 Acc: 0.3872\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2990 Acc: 0.3975\n",
      "val Loss: 0.2691 Acc: 0.3994\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2870 Acc: 0.4021\n",
      "val Loss: 0.2623 Acc: 0.3914\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2686 Acc: 0.4034\n",
      "val Loss: 0.2333 Acc: 0.3792\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2686 Acc: 0.4073\n",
      "val Loss: 0.2516 Acc: 0.3978\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2665 Acc: 0.4097\n",
      "val Loss: 0.2507 Acc: 0.3969\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2464 Acc: 0.4160\n",
      "val Loss: 0.2340 Acc: 0.3972\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2318 Acc: 0.4174\n",
      "val Loss: 0.2388 Acc: 0.3808\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2303 Acc: 0.4214\n",
      "val Loss: 0.2276 Acc: 0.3886\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2157 Acc: 0.4276\n",
      "val Loss: 0.1658 Acc: 0.3939\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.1990 Acc: 0.4251\n",
      "val Loss: 0.2160 Acc: 0.3889\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2128 Acc: 0.4353\n",
      "val Loss: 0.2048 Acc: 0.3781\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1784 Acc: 0.4447\n",
      "val Loss: 0.2064 Acc: 0.3892\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1928 Acc: 0.4447\n",
      "val Loss: 0.1824 Acc: 0.3919\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1798 Acc: 0.4525\n",
      "val Loss: 0.1807 Acc: 0.3844\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1826 Acc: 0.4547\n",
      "val Loss: 0.1867 Acc: 0.3906\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1861 Acc: 0.4674\n",
      "val Loss: 0.1737 Acc: 0.3894\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1697 Acc: 0.4744\n",
      "val Loss: 0.1823 Acc: 0.3847\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1639 Acc: 0.4856\n",
      "val Loss: 0.1907 Acc: 0.3783\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.4878\n",
      "val Loss: 0.1523 Acc: 0.3789\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1688 Acc: 0.4947\n",
      "val Loss: 0.1425 Acc: 0.3844\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1589 Acc: 0.4944\n",
      "val Loss: 0.1526 Acc: 0.3819\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1582 Acc: 0.5094\n",
      "val Loss: 0.1397 Acc: 0.3800\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1408 Acc: 0.5172\n",
      "val Loss: 0.1317 Acc: 0.3789\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1460 Acc: 0.5203\n",
      "val Loss: 0.1521 Acc: 0.3842\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1387 Acc: 0.5285\n",
      "val Loss: 0.1437 Acc: 0.3867\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1385 Acc: 0.5419\n",
      "val Loss: 0.1458 Acc: 0.3836\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1313 Acc: 0.5465\n",
      "val Loss: 0.1424 Acc: 0.3731\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1281 Acc: 0.5544\n",
      "val Loss: 0.1123 Acc: 0.3819\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1240 Acc: 0.5601\n",
      "val Loss: 0.1572 Acc: 0.3722\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1260 Acc: 0.5674\n",
      "val Loss: 0.1359 Acc: 0.3700\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1172 Acc: 0.5743\n",
      "val Loss: 0.1323 Acc: 0.3678\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1154 Acc: 0.5779\n",
      "val Loss: 0.1323 Acc: 0.3742\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1040 Acc: 0.5936\n",
      "val Loss: 0.1376 Acc: 0.3572\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1094 Acc: 0.5941\n",
      "val Loss: 0.1550 Acc: 0.3706\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.5990\n",
      "val Loss: 0.1216 Acc: 0.3661\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1125 Acc: 0.6088\n",
      "val Loss: 0.1199 Acc: 0.3714\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1032 Acc: 0.6196\n",
      "val Loss: 0.1308 Acc: 0.3647\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0986 Acc: 0.6238\n",
      "val Loss: 0.1207 Acc: 0.3561\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 0.6228\n",
      "val Loss: 0.1094 Acc: 0.3672\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.6376\n",
      "val Loss: 0.1185 Acc: 0.3719\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.6375\n",
      "val Loss: 0.1354 Acc: 0.3678\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0972 Acc: 0.6444\n",
      "val Loss: 0.1142 Acc: 0.3650\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.6442\n",
      "val Loss: 0.1107 Acc: 0.3703\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0906 Acc: 0.6604\n",
      "val Loss: 0.1172 Acc: 0.3606\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0858 Acc: 0.6657\n",
      "val Loss: 0.1235 Acc: 0.3592\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0810 Acc: 0.6769\n",
      "val Loss: 0.0973 Acc: 0.3583\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0806 Acc: 0.6796\n",
      "val Loss: 0.1030 Acc: 0.3619\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0825 Acc: 0.6759\n",
      "val Loss: 0.1106 Acc: 0.3558\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0774 Acc: 0.6819\n",
      "val Loss: 0.1054 Acc: 0.3603\n",
      "\n",
      "Best val Acc: 0.399444\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0384 Acc: 0.4040\n",
      "val Loss: 0.0154 Acc: 0.3972\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.3987\n",
      "val Loss: 0.0087 Acc: 0.3992\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.3983\n",
      "val Loss: 0.0068 Acc: 0.3986\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.3987\n",
      "val Loss: 0.0056 Acc: 0.3969\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.3988\n",
      "val Loss: 0.0041 Acc: 0.3978\n",
      "\n",
      "Best val Acc: 0.399167\n",
      "Accuracy on test set using T-Revision Method: 0.863\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6534 Acc: 0.3746\n",
      "val Loss: 0.5098 Acc: 0.3514\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4415 Acc: 0.3874\n",
      "val Loss: 0.3676 Acc: 0.3814\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3713 Acc: 0.3885\n",
      "val Loss: 0.3636 Acc: 0.3933\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3278 Acc: 0.3933\n",
      "val Loss: 0.2699 Acc: 0.3969\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3134 Acc: 0.3919\n",
      "val Loss: 0.2946 Acc: 0.3969\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.2985 Acc: 0.3942\n",
      "val Loss: 0.2498 Acc: 0.4008\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.2758 Acc: 0.3919\n",
      "val Loss: 0.2506 Acc: 0.3978\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2753 Acc: 0.3957\n",
      "val Loss: 0.2571 Acc: 0.3950\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2884 Acc: 0.3962\n",
      "val Loss: 0.3389 Acc: 0.3756\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2490 Acc: 0.3956\n",
      "val Loss: 0.2601 Acc: 0.3975\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2451 Acc: 0.4006\n",
      "val Loss: 0.2634 Acc: 0.3975\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.3969\n",
      "val Loss: 0.4048 Acc: 0.3700\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2548 Acc: 0.3980\n",
      "val Loss: 0.2330 Acc: 0.3883\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2521 Acc: 0.4035\n",
      "val Loss: 0.2652 Acc: 0.3908\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2208 Acc: 0.4038\n",
      "val Loss: 0.2003 Acc: 0.3944\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2159 Acc: 0.4059\n",
      "val Loss: 0.2328 Acc: 0.3833\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2267 Acc: 0.4060\n",
      "val Loss: 0.2371 Acc: 0.3878\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2118 Acc: 0.4122\n",
      "val Loss: 0.2310 Acc: 0.3731\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.1967 Acc: 0.4139\n",
      "val Loss: 0.1563 Acc: 0.3925\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.1849 Acc: 0.4199\n",
      "val Loss: 0.1794 Acc: 0.3897\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.1858 Acc: 0.4194\n",
      "val Loss: 0.1410 Acc: 0.3947\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.4212\n",
      "val Loss: 0.1687 Acc: 0.3856\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1653 Acc: 0.4308\n",
      "val Loss: 0.1767 Acc: 0.3814\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1741 Acc: 0.4294\n",
      "val Loss: 0.1665 Acc: 0.3886\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1682 Acc: 0.4328\n",
      "val Loss: 0.1307 Acc: 0.3919\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1617 Acc: 0.4382\n",
      "val Loss: 0.1756 Acc: 0.3853\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1742 Acc: 0.4380\n",
      "val Loss: 0.1824 Acc: 0.3883\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1910 Acc: 0.4394\n",
      "val Loss: 0.1560 Acc: 0.3883\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1711 Acc: 0.4493\n",
      "val Loss: 0.1827 Acc: 0.3756\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1498 Acc: 0.4518\n",
      "val Loss: 0.1683 Acc: 0.3839\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1517 Acc: 0.4570\n",
      "val Loss: 0.1618 Acc: 0.3867\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1592 Acc: 0.4647\n",
      "val Loss: 0.1980 Acc: 0.3708\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1697 Acc: 0.4629\n",
      "val Loss: 0.1060 Acc: 0.3978\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1600 Acc: 0.4708\n",
      "val Loss: 0.1478 Acc: 0.3917\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1569 Acc: 0.4738\n",
      "val Loss: 0.1330 Acc: 0.3928\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1682 Acc: 0.4776\n",
      "val Loss: 0.1500 Acc: 0.3847\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1469 Acc: 0.4888\n",
      "val Loss: 0.1322 Acc: 0.3889\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1516 Acc: 0.4919\n",
      "val Loss: 0.1245 Acc: 0.3856\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1467 Acc: 0.4971\n",
      "val Loss: 0.1474 Acc: 0.3794\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1467 Acc: 0.5022\n",
      "val Loss: 0.1496 Acc: 0.3792\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1377 Acc: 0.5122\n",
      "val Loss: 0.1283 Acc: 0.3833\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1372 Acc: 0.5149\n",
      "val Loss: 0.1305 Acc: 0.3744\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1363 Acc: 0.5231\n",
      "val Loss: 0.1349 Acc: 0.3792\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1375 Acc: 0.5185\n",
      "val Loss: 0.1453 Acc: 0.3675\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1225 Acc: 0.5314\n",
      "val Loss: 0.1314 Acc: 0.3722\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1251 Acc: 0.5389\n",
      "val Loss: 0.1273 Acc: 0.3644\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1121 Acc: 0.5453\n",
      "val Loss: 0.1354 Acc: 0.3761\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1166 Acc: 0.5543\n",
      "val Loss: 0.1232 Acc: 0.3689\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.5524\n",
      "val Loss: 0.1282 Acc: 0.3494\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1166 Acc: 0.5567\n",
      "val Loss: 0.1253 Acc: 0.3572\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1112 Acc: 0.5609\n",
      "val Loss: 0.1297 Acc: 0.3586\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1120 Acc: 0.5734\n",
      "val Loss: 0.1137 Acc: 0.3636\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.1018 Acc: 0.5806\n",
      "val Loss: 0.1265 Acc: 0.3586\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.5779\n",
      "val Loss: 0.1106 Acc: 0.3636\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0994 Acc: 0.5813\n",
      "val Loss: 0.1109 Acc: 0.3522\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0934 Acc: 0.5903\n",
      "val Loss: 0.0983 Acc: 0.3589\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0893 Acc: 0.5932\n",
      "val Loss: 0.0960 Acc: 0.3561\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0890 Acc: 0.5951\n",
      "val Loss: 0.1123 Acc: 0.3586\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0869 Acc: 0.6072\n",
      "val Loss: 0.1167 Acc: 0.3478\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0856 Acc: 0.6074\n",
      "val Loss: 0.1119 Acc: 0.3467\n",
      "\n",
      "Best val Acc: 0.400833\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0356 Acc: 0.3953\n",
      "val Loss: 0.0138 Acc: 0.3942\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.3935\n",
      "val Loss: 0.0080 Acc: 0.3986\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3892\n",
      "val Loss: 0.0064 Acc: 0.4008\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.3897\n",
      "val Loss: 0.0043 Acc: 0.4017\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3886\n",
      "val Loss: 0.0052 Acc: 0.3933\n",
      "\n",
      "Best val Acc: 0.401667\n",
      "Accuracy on test set using T-Revision Method: 0.8266666666666667\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.5968 Acc: 0.3734\n",
      "val Loss: 0.4697 Acc: 0.3547\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4514 Acc: 0.3840\n",
      "val Loss: 0.3664 Acc: 0.3961\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3772 Acc: 0.3885\n",
      "val Loss: 0.3572 Acc: 0.3967\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3803 Acc: 0.3887\n",
      "val Loss: 0.3229 Acc: 0.3964\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3208 Acc: 0.3924\n",
      "val Loss: 0.2398 Acc: 0.3711\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3513 Acc: 0.3891\n",
      "val Loss: 0.3245 Acc: 0.3856\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3525 Acc: 0.3883\n",
      "val Loss: 0.2570 Acc: 0.3958\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2773 Acc: 0.3910\n",
      "val Loss: 0.2849 Acc: 0.3922\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.3019 Acc: 0.3929\n",
      "val Loss: 0.3090 Acc: 0.3939\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2888 Acc: 0.3951\n",
      "val Loss: 0.2564 Acc: 0.3967\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2725 Acc: 0.3945\n",
      "val Loss: 0.3211 Acc: 0.3942\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.3103 Acc: 0.3956\n",
      "val Loss: 0.2181 Acc: 0.3958\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2586 Acc: 0.3976\n",
      "val Loss: 0.2486 Acc: 0.3939\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2440 Acc: 0.3946\n",
      "val Loss: 0.2870 Acc: 0.3850\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2256 Acc: 0.4006\n",
      "val Loss: 0.2138 Acc: 0.3892\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2465 Acc: 0.4011\n",
      "val Loss: 0.2348 Acc: 0.3917\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2245 Acc: 0.4022\n",
      "val Loss: 0.2037 Acc: 0.3892\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.3994\n",
      "val Loss: 0.2298 Acc: 0.3906\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2069 Acc: 0.4027\n",
      "val Loss: 0.1867 Acc: 0.3969\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2094 Acc: 0.4054\n",
      "val Loss: 0.2677 Acc: 0.3881\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2331 Acc: 0.4048\n",
      "val Loss: 0.2377 Acc: 0.3792\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1914 Acc: 0.4068\n",
      "val Loss: 0.1615 Acc: 0.3947\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1857 Acc: 0.4069\n",
      "val Loss: 0.2458 Acc: 0.3803\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1950 Acc: 0.4108\n",
      "val Loss: 0.2138 Acc: 0.3831\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2010 Acc: 0.4133\n",
      "val Loss: 0.1909 Acc: 0.3817\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1833 Acc: 0.4117\n",
      "val Loss: 0.1745 Acc: 0.3833\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1952 Acc: 0.4195\n",
      "val Loss: 0.1841 Acc: 0.3789\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1864 Acc: 0.4223\n",
      "val Loss: 0.2128 Acc: 0.3772\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1905 Acc: 0.4198\n",
      "val Loss: 0.1438 Acc: 0.3786\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1820 Acc: 0.4253\n",
      "val Loss: 0.2038 Acc: 0.3808\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1881 Acc: 0.4274\n",
      "val Loss: 0.2058 Acc: 0.3842\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1716 Acc: 0.4318\n",
      "val Loss: 0.1590 Acc: 0.3767\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1667 Acc: 0.4297\n",
      "val Loss: 0.1754 Acc: 0.3833\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1542 Acc: 0.4400\n",
      "val Loss: 0.1570 Acc: 0.3772\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1636 Acc: 0.4338\n",
      "val Loss: 0.1606 Acc: 0.3864\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1726 Acc: 0.4448\n",
      "val Loss: 0.1620 Acc: 0.3794\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.4551\n",
      "val Loss: 0.1748 Acc: 0.3606\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1606 Acc: 0.4520\n",
      "val Loss: 0.1489 Acc: 0.3842\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1579 Acc: 0.4597\n",
      "val Loss: 0.1452 Acc: 0.3653\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1613 Acc: 0.4577\n",
      "val Loss: 0.1478 Acc: 0.3694\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1503 Acc: 0.4660\n",
      "val Loss: 0.1491 Acc: 0.3717\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1458 Acc: 0.4739\n",
      "val Loss: 0.1430 Acc: 0.3711\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1444 Acc: 0.4774\n",
      "val Loss: 0.1625 Acc: 0.3772\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1499 Acc: 0.4850\n",
      "val Loss: 0.1557 Acc: 0.3742\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1521 Acc: 0.4932\n",
      "val Loss: 0.1642 Acc: 0.3631\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1560 Acc: 0.4911\n",
      "val Loss: 0.1723 Acc: 0.3644\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1473 Acc: 0.5014\n",
      "val Loss: 0.1440 Acc: 0.3647\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.5005\n",
      "val Loss: 0.1453 Acc: 0.3728\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1415 Acc: 0.5142\n",
      "val Loss: 0.1236 Acc: 0.3761\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.5197\n",
      "val Loss: 0.1345 Acc: 0.3697\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.5175\n",
      "val Loss: 0.1271 Acc: 0.3700\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1328 Acc: 0.5255\n",
      "val Loss: 0.1500 Acc: 0.3647\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.5331\n",
      "val Loss: 0.1490 Acc: 0.3708\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1328 Acc: 0.5415\n",
      "val Loss: 0.1326 Acc: 0.3689\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.1236 Acc: 0.5450\n",
      "val Loss: 0.1163 Acc: 0.3717\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.1236 Acc: 0.5513\n",
      "val Loss: 0.1399 Acc: 0.3683\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.1245 Acc: 0.5541\n",
      "val Loss: 0.1484 Acc: 0.3642\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.1221 Acc: 0.5624\n",
      "val Loss: 0.1358 Acc: 0.3539\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.1238 Acc: 0.5654\n",
      "val Loss: 0.1174 Acc: 0.3733\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.1248 Acc: 0.5687\n",
      "val Loss: 0.1363 Acc: 0.3617\n",
      "\n",
      "Best val Acc: 0.396944\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0347 Acc: 0.4035\n",
      "val Loss: 0.0133 Acc: 0.3981\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0113 Acc: 0.4023\n",
      "val Loss: 0.0079 Acc: 0.3964\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.4010\n",
      "val Loss: 0.0056 Acc: 0.3958\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3988\n",
      "val Loss: 0.0048 Acc: 0.3950\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.3971\n",
      "val Loss: 0.0041 Acc: 0.3947\n",
      "\n",
      "Best val Acc: 0.398056\n",
      "Accuracy on test set using T-Revision Method: 0.8673333333333333\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6206 Acc: 0.3725\n",
      "val Loss: 0.4259 Acc: 0.3889\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.5024 Acc: 0.3844\n",
      "val Loss: 0.4291 Acc: 0.3775\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3987 Acc: 0.3895\n",
      "val Loss: 0.2449 Acc: 0.3781\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3719 Acc: 0.3876\n",
      "val Loss: 0.3489 Acc: 0.3942\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3741 Acc: 0.3901\n",
      "val Loss: 0.3196 Acc: 0.3950\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3539 Acc: 0.3934\n",
      "val Loss: 0.2410 Acc: 0.3861\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3284 Acc: 0.3893\n",
      "val Loss: 0.3145 Acc: 0.3908\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.3343 Acc: 0.3915\n",
      "val Loss: 0.3297 Acc: 0.3844\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.3626 Acc: 0.3890\n",
      "val Loss: 0.2902 Acc: 0.3947\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.3093 Acc: 0.3942\n",
      "val Loss: 0.2348 Acc: 0.3961\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.3001 Acc: 0.3948\n",
      "val Loss: 0.3249 Acc: 0.3844\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2923 Acc: 0.3957\n",
      "val Loss: 0.2739 Acc: 0.3978\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.3015 Acc: 0.3976\n",
      "val Loss: 0.2875 Acc: 0.3914\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2935 Acc: 0.3978\n",
      "val Loss: 0.3059 Acc: 0.3883\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2856 Acc: 0.3987\n",
      "val Loss: 0.2452 Acc: 0.3892\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2903 Acc: 0.4000\n",
      "val Loss: 0.2576 Acc: 0.3869\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2817 Acc: 0.4041\n",
      "val Loss: 0.3683 Acc: 0.3808\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2921 Acc: 0.4019\n",
      "val Loss: 0.2402 Acc: 0.3939\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2560 Acc: 0.4040\n",
      "val Loss: 0.2624 Acc: 0.3864\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2533 Acc: 0.4102\n",
      "val Loss: 0.2516 Acc: 0.3908\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2633 Acc: 0.4134\n",
      "val Loss: 0.2159 Acc: 0.3886\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2432 Acc: 0.4161\n",
      "val Loss: 0.2713 Acc: 0.3769\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2285 Acc: 0.4224\n",
      "val Loss: 0.1642 Acc: 0.3942\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.2295 Acc: 0.4203\n",
      "val Loss: 0.2120 Acc: 0.3867\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2235 Acc: 0.4297\n",
      "val Loss: 0.1662 Acc: 0.3922\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1881 Acc: 0.4345\n",
      "val Loss: 0.2092 Acc: 0.3900\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1830 Acc: 0.4401\n",
      "val Loss: 0.1511 Acc: 0.3836\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1741 Acc: 0.4427\n",
      "val Loss: 0.2386 Acc: 0.3653\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1754 Acc: 0.4550\n",
      "val Loss: 0.1847 Acc: 0.3808\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1555 Acc: 0.4573\n",
      "val Loss: 0.1847 Acc: 0.3733\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1633 Acc: 0.4609\n",
      "val Loss: 0.1430 Acc: 0.3833\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1493 Acc: 0.4712\n",
      "val Loss: 0.1423 Acc: 0.3781\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1396 Acc: 0.4772\n",
      "val Loss: 0.1782 Acc: 0.3686\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1446 Acc: 0.4775\n",
      "val Loss: 0.1291 Acc: 0.3817\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1353 Acc: 0.4835\n",
      "val Loss: 0.1488 Acc: 0.3758\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.4924\n",
      "val Loss: 0.1480 Acc: 0.3694\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1258 Acc: 0.5001\n",
      "val Loss: 0.1524 Acc: 0.3656\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1239 Acc: 0.5044\n",
      "val Loss: 0.1227 Acc: 0.3761\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1249 Acc: 0.5101\n",
      "val Loss: 0.1190 Acc: 0.3764\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1123 Acc: 0.5172\n",
      "val Loss: 0.1226 Acc: 0.3744\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.5247\n",
      "val Loss: 0.1353 Acc: 0.3736\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1163 Acc: 0.5268\n",
      "val Loss: 0.1429 Acc: 0.3575\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.5310\n",
      "val Loss: 0.1401 Acc: 0.3719\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1017 Acc: 0.5410\n",
      "val Loss: 0.1344 Acc: 0.3714\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1004 Acc: 0.5502\n",
      "val Loss: 0.1160 Acc: 0.3672\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0995 Acc: 0.5517\n",
      "val Loss: 0.1112 Acc: 0.3733\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 0.5633\n",
      "val Loss: 0.1227 Acc: 0.3669\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1033 Acc: 0.5625\n",
      "val Loss: 0.1061 Acc: 0.3531\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0963 Acc: 0.5713\n",
      "val Loss: 0.1162 Acc: 0.3631\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0959 Acc: 0.5725\n",
      "val Loss: 0.1102 Acc: 0.3619\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0928 Acc: 0.5825\n",
      "val Loss: 0.1190 Acc: 0.3619\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0922 Acc: 0.5848\n",
      "val Loss: 0.1321 Acc: 0.3575\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0903 Acc: 0.5944\n",
      "val Loss: 0.1386 Acc: 0.3628\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0886 Acc: 0.5967\n",
      "val Loss: 0.1266 Acc: 0.3667\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0883 Acc: 0.6018\n",
      "val Loss: 0.1326 Acc: 0.3628\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0852 Acc: 0.6055\n",
      "val Loss: 0.1359 Acc: 0.3525\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0876 Acc: 0.6124\n",
      "val Loss: 0.1003 Acc: 0.3603\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0874 Acc: 0.6149\n",
      "val Loss: 0.1232 Acc: 0.3650\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0836 Acc: 0.6192\n",
      "val Loss: 0.1127 Acc: 0.3592\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0870 Acc: 0.6217\n",
      "val Loss: 0.1078 Acc: 0.3650\n",
      "\n",
      "Best val Acc: 0.397778\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0436 Acc: 0.3997\n",
      "val Loss: 0.0179 Acc: 0.4028\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0130 Acc: 0.3988\n",
      "val Loss: 0.0100 Acc: 0.3964\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3993\n",
      "val Loss: 0.0074 Acc: 0.4022\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.3984\n",
      "val Loss: 0.0060 Acc: 0.4014\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.3991\n",
      "val Loss: 0.0048 Acc: 0.4011\n",
      "\n",
      "Best val Acc: 0.402778\n",
      "Accuracy on test set using T-Revision Method: 0.869\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6580 Acc: 0.3653\n",
      "val Loss: 0.4170 Acc: 0.3714\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.5073 Acc: 0.3794\n",
      "val Loss: 0.5132 Acc: 0.3669\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.4255 Acc: 0.3855\n",
      "val Loss: 0.4879 Acc: 0.3728\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3972 Acc: 0.3885\n",
      "val Loss: 0.4983 Acc: 0.3764\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3646 Acc: 0.3919\n",
      "val Loss: 0.3093 Acc: 0.3914\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3528 Acc: 0.3929\n",
      "val Loss: 0.3325 Acc: 0.3944\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3471 Acc: 0.3889\n",
      "val Loss: 0.2891 Acc: 0.3928\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.3362 Acc: 0.3921\n",
      "val Loss: 0.2963 Acc: 0.3919\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.3263 Acc: 0.3936\n",
      "val Loss: 0.2689 Acc: 0.3892\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2824 Acc: 0.3969\n",
      "val Loss: 0.2458 Acc: 0.3906\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2627 Acc: 0.3965\n",
      "val Loss: 0.2851 Acc: 0.3847\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.3060 Acc: 0.3958\n",
      "val Loss: 0.2607 Acc: 0.3792\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2767 Acc: 0.3937\n",
      "val Loss: 0.2912 Acc: 0.3819\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2869 Acc: 0.3969\n",
      "val Loss: 0.2977 Acc: 0.3881\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2617 Acc: 0.4016\n",
      "val Loss: 0.2231 Acc: 0.3906\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2470 Acc: 0.4046\n",
      "val Loss: 0.2628 Acc: 0.3844\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2512 Acc: 0.4012\n",
      "val Loss: 0.2417 Acc: 0.3753\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2324 Acc: 0.4053\n",
      "val Loss: 0.2759 Acc: 0.3928\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2612 Acc: 0.4085\n",
      "val Loss: 0.2232 Acc: 0.3944\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2521 Acc: 0.4108\n",
      "val Loss: 0.2204 Acc: 0.3947\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2379 Acc: 0.4190\n",
      "val Loss: 0.2520 Acc: 0.3811\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2237 Acc: 0.4197\n",
      "val Loss: 0.1994 Acc: 0.3922\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2174 Acc: 0.4234\n",
      "val Loss: 0.1426 Acc: 0.3942\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.2078 Acc: 0.4235\n",
      "val Loss: 0.1880 Acc: 0.3900\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2002 Acc: 0.4325\n",
      "val Loss: 0.1695 Acc: 0.3867\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2057 Acc: 0.4318\n",
      "val Loss: 0.1739 Acc: 0.3864\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2034 Acc: 0.4423\n",
      "val Loss: 0.1622 Acc: 0.3844\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1979 Acc: 0.4384\n",
      "val Loss: 0.2265 Acc: 0.3822\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1936 Acc: 0.4518\n",
      "val Loss: 0.1995 Acc: 0.3803\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.2035 Acc: 0.4532\n",
      "val Loss: 0.2368 Acc: 0.3806\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.2031 Acc: 0.4630\n",
      "val Loss: 0.1597 Acc: 0.3806\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.2009 Acc: 0.4710\n",
      "val Loss: 0.1620 Acc: 0.3872\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1860 Acc: 0.4740\n",
      "val Loss: 0.1862 Acc: 0.3781\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1867 Acc: 0.4697\n",
      "val Loss: 0.1906 Acc: 0.3733\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1791 Acc: 0.4844\n",
      "val Loss: 0.2544 Acc: 0.3575\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1809 Acc: 0.4820\n",
      "val Loss: 0.1534 Acc: 0.3772\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1655 Acc: 0.4969\n",
      "val Loss: 0.1754 Acc: 0.3756\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1713 Acc: 0.4995\n",
      "val Loss: 0.1461 Acc: 0.3781\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1642 Acc: 0.5053\n",
      "val Loss: 0.1489 Acc: 0.3692\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1487 Acc: 0.5060\n",
      "val Loss: 0.1431 Acc: 0.3819\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1545 Acc: 0.5244\n",
      "val Loss: 0.1484 Acc: 0.3678\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1446 Acc: 0.5322\n",
      "val Loss: 0.1485 Acc: 0.3653\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1367 Acc: 0.5394\n",
      "val Loss: 0.1579 Acc: 0.3622\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1331 Acc: 0.5458\n",
      "val Loss: 0.1286 Acc: 0.3686\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1308 Acc: 0.5485\n",
      "val Loss: 0.1417 Acc: 0.3653\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1200 Acc: 0.5560\n",
      "val Loss: 0.1215 Acc: 0.3661\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1273 Acc: 0.5513\n",
      "val Loss: 0.1277 Acc: 0.3764\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.5724\n",
      "val Loss: 0.1226 Acc: 0.3711\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1115 Acc: 0.5773\n",
      "val Loss: 0.1316 Acc: 0.3703\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1134 Acc: 0.5806\n",
      "val Loss: 0.1195 Acc: 0.3656\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1072 Acc: 0.5872\n",
      "val Loss: 0.1051 Acc: 0.3725\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1105 Acc: 0.5832\n",
      "val Loss: 0.1189 Acc: 0.3644\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.1104 Acc: 0.5834\n",
      "val Loss: 0.1317 Acc: 0.3611\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0945 Acc: 0.6014\n",
      "val Loss: 0.1308 Acc: 0.3608\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.6036\n",
      "val Loss: 0.1100 Acc: 0.3694\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.6080\n",
      "val Loss: 0.1125 Acc: 0.3769\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0891 Acc: 0.6141\n",
      "val Loss: 0.1150 Acc: 0.3625\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0881 Acc: 0.6178\n",
      "val Loss: 0.1098 Acc: 0.3600\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0929 Acc: 0.6162\n",
      "val Loss: 0.1127 Acc: 0.3650\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.6256\n",
      "val Loss: 0.1005 Acc: 0.3642\n",
      "\n",
      "Best val Acc: 0.394722\n",
      "Revision TRUE\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.0355 Acc: 0.4181\n",
      "val Loss: 0.0151 Acc: 0.3969\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.0144 Acc: 0.4142\n",
      "val Loss: 0.0095 Acc: 0.3964\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.4109\n",
      "val Loss: 0.0073 Acc: 0.3961\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.4109\n",
      "val Loss: 0.0059 Acc: 0.3958\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.4103\n",
      "val Loss: 0.0050 Acc: 0.3939\n",
      "\n",
      "Best val Acc: 0.396944\n",
      "Accuracy on test set using T-Revision Method: 0.8513333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "# Training the model n_iters\n",
    "n_iters = 10\n",
    "\n",
    "# Set seed\n",
    "seed_torch()\n",
    "\n",
    "test_acc_trevision_results = {'cifar': [],\n",
    "                              'fashionmnist0.5': [],\n",
    "                              'fashionmnist0.6': []}\n",
    "\n",
    "test_revision_trevision_results = {'cifar': [],\n",
    "                                   'fashionmnist0.5': [],\n",
    "                                   'fashionmnist0.6': []}\n",
    "\n",
    "# Iterate over each dataset\n",
    "for d in list(available_datasets.keys()):\n",
    "  dataset = d\n",
    "\n",
    "  # Load dataset\n",
    "  if dataset.lower() in available_datasets:\n",
    "    dir = available_datasets[dataset.lower()]\n",
    "    T_matrix = T_matrices[dataset.lower()]\n",
    "    X_train, y_train, X_test, y_test = load_data(dir)\n",
    "    print('-'*60)\n",
    "    print('-'*60)\n",
    "    print(f\"\\nDataset {dataset} loaded.\\n\")\n",
    "\n",
    "    print(f\"Shape Xtr: {X_train.shape}\")\n",
    "    print(f\"Shape Str: {y_train.shape}\")\n",
    "    print(f\"Shape Xts: {X_test.shape}\")\n",
    "    print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "    print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "    print('.'*30)\n",
    "  else:\n",
    "    print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")\n",
    "\n",
    "  # Detect if GPU or CPU\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "      \n",
    "  # Define parameters\n",
    "  if dataset.lower() == 'cifar':\n",
    "    n_channels = 3\n",
    "    n_filters = 8\n",
    "  else:\n",
    "    n_channels = 1\n",
    "    n_filters = 7\n",
    "\n",
    "  num_classes = 3\n",
    "  batch_size = 100\n",
    "  num_epochs = 60\n",
    "  learning_rate = 0.001\n",
    "  \n",
    "  for n in range(n_iters):\n",
    "    print(f\"\\nTraining iteration: {n}\")\n",
    "\n",
    "    # Clean cache each iteration\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # We did not set a seed for train_test_split thus, it would generate different samples each iteration\n",
    "    train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                    y_train=y_train, \n",
    "                                                    X_test=X_test, \n",
    "                                                    y_test=y_test, \n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "\n",
    "    # Initialize model\n",
    "    model_modified_resnet = ResNet(ResidualBlock, [2, 2, 2], \n",
    "                                   num_channels=n_channels, \n",
    "                                   num_filter=n_filters, \n",
    "                                   num_classes=num_classes).to(device)\n",
    "\n",
    "    # If GPU: send the model to GPU\n",
    "    model_modified_resnet = model_modified_resnet.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model_modified_resnet.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define Loss Function\n",
    "    criterion = ReweightingLoss()\n",
    "\n",
    "    # Train model - Revision FALSE\n",
    "    model_modified_resnet, hist, _ = train_model_forward(model_modified_resnet, \n",
    "                                                         dataloaders_dict, \n",
    "                                                         criterion, \n",
    "                                                         optimizer, \n",
    "                                                         T_matrix, \n",
    "                                                         with_revision=False, \n",
    "                                                         num_epochs=num_epochs)\n",
    "    \n",
    "    # Train and Evaluate model - Revision TRUE\n",
    "    print(\"Revision TRUE\")\n",
    "    model_modified_resnet, hist, revision_ls = train_model_forward(model_modified_resnet,\n",
    "                                                                   dataloaders_dict, \n",
    "                                                                   criterion, \n",
    "                                                                   optimizer, \n",
    "                                                                   T_matrix, \n",
    "                                                                   with_revision=True, \n",
    "                                                                   num_epochs=5)\n",
    "    \n",
    "    # Generate predictions on the test set\n",
    "    y_true, y_pred, acc, outputs = prediction(model_modified_resnet, test_loader, device, revision=True)\n",
    "\n",
    "    test_acc_trevision_results[dataset].append(acc)\n",
    "    test_revision_trevision_results[dataset].append(revision_ls)\n",
    "    print(f\"Accuracy on test set using T-Revision Method: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xgb8ynqLdbx"
   },
   "source": [
    "#### Results\n",
    "\n",
    "We can print the dictionary `test_acc_forward_results` which has stored the results of the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1605615573331,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "BIOIQJwsLiSe",
    "outputId": "a7d71cc3-48ee-4d9e-f879-8b33750b91d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifar</th>\n",
       "      <th>fashionmnist0.5</th>\n",
       "      <th>fashionmnist0.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.626333</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.873667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.630333</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.881667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.529667</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600333</td>\n",
       "      <td>0.913667</td>\n",
       "      <td>0.877333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.538333</td>\n",
       "      <td>0.939333</td>\n",
       "      <td>0.886333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.531333</td>\n",
       "      <td>0.933667</td>\n",
       "      <td>0.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.457333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.826667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.943667</td>\n",
       "      <td>0.867333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.928333</td>\n",
       "      <td>0.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.937333</td>\n",
       "      <td>0.851333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cifar  fashionmnist0.5  fashionmnist0.6\n",
       "0  0.626333         0.911000         0.873667\n",
       "1  0.630333         0.936000         0.881667\n",
       "2  0.529667         0.915000         0.866000\n",
       "3  0.600333         0.913667         0.877333\n",
       "4  0.538333         0.939333         0.886333\n",
       "5  0.531333         0.933667         0.863000\n",
       "6  0.457333         0.933333         0.826667\n",
       "7  0.683000         0.943667         0.867333\n",
       "8  0.588000         0.928333         0.869000\n",
       "9  0.486667         0.937333         0.851333"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy values\n",
    "test_acc_trevision_results_backup = test_acc_trevision_results \n",
    "\n",
    "# Create dataframe from dictionary\n",
    "df = pd.DataFrame(test_acc_trevision_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuI2yFGys4PG"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "import pickle\n",
    "with open('test_acc_trevision_results.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_acc_trevision_results, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOgmJocYtGZP"
   },
   "outputs": [],
   "source": [
    "with open('test_acc_trevision_results.pkl', 'rb') as fp:\n",
    "    test_load = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68Ww3u_lM1VL"
   },
   "source": [
    "We will calculate the mean value a standard deviation for each dataset using the function implemented in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1605615575067,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "etf6JON7M-xq",
    "outputId": "f96ef8d5-97c1-477b-b027-62374a50c0a2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x630 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean, std = process_results(test_acc_trevision_results, title=\"T-Revision Method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fio8nXyxnGhh"
   },
   "source": [
    "We can show the transition matrices modified by the correction layer:\n",
    "\n",
    "$$T + \\Delta T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1605615575068,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "Gz4wsaRsm_FL",
    "outputId": "25b08216-6f4a-4300-c3fd-3f6e5554061e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cifar\n",
      "\n",
      "T_matrix: \n",
      "tensor([[0.4395, 0.3019, 0.2586],\n",
      "        [0.2830, 0.4672, 0.2498],\n",
      "        [0.2788, 0.2901, 0.4311]], device='cuda:0')\n",
      "\n",
      "Correction: \n",
      "tensor([[0.0332, 0.0366, 0.0286],\n",
      "        [0.0416, 0.0449, 0.0462],\n",
      "        [0.0322, 0.0508, 0.0372]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\n",
      "T+Correction: \n",
      "tensor([[0.4726, 0.3386, 0.2872],\n",
      "        [0.3246, 0.5122, 0.2960],\n",
      "        [0.3111, 0.3409, 0.4683]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "------------------------------\n",
      "Dataset: fashionmnist0.5\n",
      "\n",
      "T_matrix: \n",
      "tensor([[0.5000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.5000, 0.2000],\n",
      "        [0.2000, 0.3000, 0.5000]], device='cuda:0')\n",
      "\n",
      "Correction: \n",
      "tensor([[0.0279, 0.0216, 0.0400],\n",
      "        [0.0243, 0.0219, 0.0228],\n",
      "        [0.0307, 0.0331, 0.0282]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\n",
      "T+Correction: \n",
      "tensor([[0.5279, 0.2216, 0.3400],\n",
      "        [0.3243, 0.5219, 0.2228],\n",
      "        [0.2307, 0.3331, 0.5282]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "------------------------------\n",
      "Dataset: fashionmnist0.6\n",
      "\n",
      "T_matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "\n",
      "Correction: \n",
      "tensor([[0.0482, 0.0340, 0.0452],\n",
      "        [0.0389, 0.0447, 0.0420],\n",
      "        [0.0463, 0.0338, 0.0434]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\n",
      "T+Correction: \n",
      "tensor([[0.4482, 0.3340, 0.3452],\n",
      "        [0.3389, 0.4447, 0.3420],\n",
      "        [0.3463, 0.3338, 0.4434]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for d in list(available_datasets.keys()):\n",
    "  dataset = d\n",
    "  T_matrix = T_matrices[dataset.lower()]\n",
    "  corr_aver = sum(x[-1] for x in test_revision_trevision_results[dataset])/len(test_revision_trevision_results[dataset])\n",
    "  print(f\"Dataset: {dataset}\")\n",
    "  print(f\"\\nT_matrix: \\n{T_matrix}\")\n",
    "  print(f\"\\nCorrection: \\n{corr_aver}\")\n",
    "  print(f\"\\nT+Correction: \\n{T_matrix + corr_aver}\")\n",
    "  print('---'*10)\n",
    "#test_revision_trevision_results['cifar'][-1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz-mJ06HL39Q"
   },
   "source": [
    "### 4.3.- Classifier using Importance Reweighting Method\n",
    "\n",
    "In this section, we perform the experiments using the Importance Reweighting method. We use the functions we implemented previously for T-Revision but we won't perform the `Revision=True` step.\n",
    "\n",
    "As we did previously, we need to define the architecture of the ResNet model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtGQl0vgMICx"
   },
   "source": [
    "#### ResNet Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXeqZ06JMKJY"
   },
   "outputs": [],
   "source": [
    "# Convolution Block\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Main Block ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_channels=1, num_filter=7, num_classes=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(num_channels, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(num_filter)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        self.delta_T = nn.Linear(num_classes, num_classes, False)\n",
    "        nn.init.zeros_(self.delta_T.weight)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, revision=False, plot_deltaT=False):\n",
    "        correction = self.delta_T.weight\n",
    "        if plot_deltaT:\n",
    "          print(\"Corr\", correction)\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        if revision:\n",
    "            return out, correction\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSM85vhMMM37"
   },
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqsstfuqMOaX"
   },
   "outputs": [],
   "source": [
    "class ReweightingLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super(ReweightingLoss, self).__init__()\n",
    "      \n",
    "    def forward(self, out, T, target):\n",
    "      loss = 0.0\n",
    "      out_softmax = F.softmax(out, dim=1) \n",
    "      # If standard mode (revision false)\n",
    "      g_Y = out_softmax.gather(1, target.view(-1,1))\n",
    "      Tg = torch.mm(T.t(), out_softmax.t())\n",
    "      Tg_Y = Tg.t().gather(1, target.view(-1,1))\n",
    "      # requires_grad=True indicates that we want to compute gradients with\n",
    "      # respect to these Variables during the backward pass.\n",
    "      beta = torch.nn.Parameter(g_Y / Tg_Y)\n",
    "      loss = F.cross_entropy(out, target, reduction='none')\n",
    "      loss = beta.view(1,-1) * loss\n",
    "      return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVpYzorpMUc-"
   },
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6KdnLI4MWQT"
   },
   "outputs": [],
   "source": [
    "def train_model_forward(model, dataloaders, criterion, optimizer, T_matrix, with_revision=False, num_epochs=25):\n",
    "  \"\"\"\n",
    "  Function to train the model\n",
    "  \n",
    "  :param model: Model\n",
    "  :param dataloaders: DataLoader\n",
    "  :param criterion: Loss function\n",
    "  :param optimizer: Optimizer\n",
    "  :param with_revision: True or False\n",
    "  :param num_epochs: Number of epochs\n",
    "  :return model: Trained model\n",
    "  \"\"\"\n",
    "\n",
    "  val_acc_history = []\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_acc = 0.0\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "      print('-' * 10)\n",
    "\n",
    "      # Each epoch has a training and validation phase\n",
    "      for phase in ['train', 'val']:\n",
    "          if phase == 'train':\n",
    "              model.train()  # Set model to training mode\n",
    "          else:\n",
    "              model.eval()   # Set model to evaluate mode\n",
    "\n",
    "          running_loss = 0.0\n",
    "          running_corrects = 0\n",
    "\n",
    "          # Iterate over data.\n",
    "          for b, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "              inputs = inputs.to(device)\n",
    "              labels = labels.to(device)\n",
    "\n",
    "              # zero the parameter gradients\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "              # forward\n",
    "              if with_revision==True:\n",
    "                out, revision = model(inputs, revision=with_revision)\n",
    "                loss = criterion(out, T_matrix, labels, revision)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                out = torch.matmul((T_matrix+revision).t(), prob.t())\n",
    "              else:\n",
    "                out = model(inputs)\n",
    "                loss = criterion(out, T_matrix, labels)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                out = torch.matmul(T_matrix.t(), prob.t())\n",
    "                \n",
    "              out = out.t()\n",
    "\n",
    "              # track history if only in train\n",
    "              if with_revision==False:\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                  \n",
    "                  _, preds = torch.max(out, 1)\n",
    "                  \n",
    "                  # backward + optimize only if in training phase\n",
    "                  if phase == 'train':\n",
    "                      loss.backward()\n",
    "                      optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "              else:\n",
    "                _, preds = torch.max(out, 1)\n",
    "\n",
    "                if phase == 'train':\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "                #print(loss.item()*inputs.size(0))\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if abs(loss.item()* inputs.size(0)) < 0.02:\n",
    "                  break\n",
    "\n",
    "          epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "          epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "          # deep copy the model\n",
    "          if phase == 'val' and epoch_acc > best_acc:\n",
    "              best_acc = epoch_acc\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "          if phase == 'val':\n",
    "              val_acc_history.append(epoch_acc)\n",
    "\n",
    "      print()\n",
    "\n",
    "  print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "  # load best model weights\n",
    "  model.load_state_dict(best_model_wts)\n",
    "  return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dWfvpriMe6s"
   },
   "source": [
    "#### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGNOdNtKMhW8",
    "outputId": "9841b577-b6d9-407b-8a63-b1d20afa69be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "val Loss: 0.1879 Acc: 0.4306\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1386 Acc: 0.6880\n",
      "val Loss: 0.2034 Acc: 0.4142\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1316 Acc: 0.6922\n",
      "val Loss: 0.1747 Acc: 0.4247\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1242 Acc: 0.6978\n",
      "val Loss: 0.1778 Acc: 0.4142\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1160 Acc: 0.7067\n",
      "val Loss: 0.1664 Acc: 0.4333\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1122 Acc: 0.7074\n",
      "val Loss: 0.1728 Acc: 0.4111\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1051 Acc: 0.7144\n",
      "val Loss: 0.1728 Acc: 0.4172\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.7201\n",
      "val Loss: 0.1643 Acc: 0.4058\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.7258\n",
      "val Loss: 0.1558 Acc: 0.4275\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.0935 Acc: 0.7301\n",
      "val Loss: 0.1491 Acc: 0.4244\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.0932 Acc: 0.7318\n",
      "val Loss: 0.1370 Acc: 0.4203\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0866 Acc: 0.7410\n",
      "val Loss: 0.1431 Acc: 0.4114\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0891 Acc: 0.7367\n",
      "val Loss: 0.1464 Acc: 0.4125\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0853 Acc: 0.7406\n",
      "val Loss: 0.1275 Acc: 0.4261\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0791 Acc: 0.7446\n",
      "val Loss: 0.1435 Acc: 0.4164\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0795 Acc: 0.7511\n",
      "val Loss: 0.1348 Acc: 0.4211\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0744 Acc: 0.7569\n",
      "val Loss: 0.1560 Acc: 0.4006\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.7610\n",
      "val Loss: 0.1252 Acc: 0.4119\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0705 Acc: 0.7630\n",
      "val Loss: 0.1327 Acc: 0.4219\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 0.7653\n",
      "val Loss: 0.1182 Acc: 0.4239\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0688 Acc: 0.7676\n",
      "val Loss: 0.1273 Acc: 0.4136\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0661 Acc: 0.7715\n",
      "val Loss: 0.1119 Acc: 0.4178\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0609 Acc: 0.7724\n",
      "val Loss: 0.1001 Acc: 0.4261\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0621 Acc: 0.7715\n",
      "val Loss: 0.1227 Acc: 0.4136\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0635 Acc: 0.7763\n",
      "val Loss: 0.1352 Acc: 0.3997\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0634 Acc: 0.7774\n",
      "val Loss: 0.1175 Acc: 0.4081\n",
      "\n",
      "Best val Acc: 0.490833\n",
      "Accuracy on test set using Importance Reweighting Method: 0.9253333333333333\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6784 Acc: 0.4573\n",
      "val Loss: 0.6559 Acc: 0.4778\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6539 Acc: 0.4704\n",
      "val Loss: 0.6031 Acc: 0.4831\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6622 Acc: 0.4740\n",
      "val Loss: 0.7376 Acc: 0.4819\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6445 Acc: 0.4761\n",
      "val Loss: 0.6306 Acc: 0.4900\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.4815\n",
      "val Loss: 0.7065 Acc: 0.4769\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6447 Acc: 0.4831\n",
      "val Loss: 0.6371 Acc: 0.4881\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6220 Acc: 0.4838\n",
      "val Loss: 0.6296 Acc: 0.4864\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6425 Acc: 0.4859\n",
      "val Loss: 0.5742 Acc: 0.4853\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.4859\n",
      "val Loss: 0.6473 Acc: 0.4858\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6221 Acc: 0.4884\n",
      "val Loss: 0.6421 Acc: 0.4894\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.6146 Acc: 0.4888\n",
      "val Loss: 0.6159 Acc: 0.4853\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.6192 Acc: 0.4863\n",
      "val Loss: 0.6360 Acc: 0.4828\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5961 Acc: 0.4924\n",
      "val Loss: 0.5992 Acc: 0.4822\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5741 Acc: 0.4992\n",
      "val Loss: 0.5572 Acc: 0.4625\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5512 Acc: 0.4984\n",
      "val Loss: 0.5341 Acc: 0.4853\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5368 Acc: 0.5026\n",
      "val Loss: 0.5443 Acc: 0.4750\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.5178 Acc: 0.5076\n",
      "val Loss: 0.5347 Acc: 0.4797\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.4920 Acc: 0.5106\n",
      "val Loss: 0.5547 Acc: 0.4617\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4803 Acc: 0.5187\n",
      "val Loss: 0.5040 Acc: 0.4686\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.4474 Acc: 0.5213\n",
      "val Loss: 0.5415 Acc: 0.4439\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.4219 Acc: 0.5306\n",
      "val Loss: 0.4815 Acc: 0.4600\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.4030 Acc: 0.5402\n",
      "val Loss: 0.4072 Acc: 0.4608\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3703 Acc: 0.5480\n",
      "val Loss: 0.4080 Acc: 0.4589\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3466 Acc: 0.5612\n",
      "val Loss: 0.3950 Acc: 0.4503\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.3372 Acc: 0.5674\n",
      "val Loss: 0.3212 Acc: 0.4556\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.3093 Acc: 0.5713\n",
      "val Loss: 0.3108 Acc: 0.4478\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2951 Acc: 0.5847\n",
      "val Loss: 0.3056 Acc: 0.4539\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2799 Acc: 0.5935\n",
      "val Loss: 0.3323 Acc: 0.4506\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2499 Acc: 0.6044\n",
      "val Loss: 0.2881 Acc: 0.4417\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.2469 Acc: 0.6088\n",
      "val Loss: 0.3073 Acc: 0.4108\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.2244 Acc: 0.6214\n",
      "val Loss: 0.2601 Acc: 0.4383\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.2175 Acc: 0.6292\n",
      "val Loss: 0.2816 Acc: 0.4231\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.6313\n",
      "val Loss: 0.2658 Acc: 0.4375\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1949 Acc: 0.6436\n",
      "val Loss: 0.2373 Acc: 0.4336\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1809 Acc: 0.6485\n",
      "val Loss: 0.2365 Acc: 0.4253\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1727 Acc: 0.6588\n",
      "val Loss: 0.2459 Acc: 0.4139\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1625 Acc: 0.6647\n",
      "val Loss: 0.2247 Acc: 0.4314\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.6694\n",
      "val Loss: 0.2130 Acc: 0.4197\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1432 Acc: 0.6787\n",
      "val Loss: 0.1915 Acc: 0.4381\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1336 Acc: 0.6867\n",
      "val Loss: 0.1900 Acc: 0.4258\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1344 Acc: 0.6897\n",
      "val Loss: 0.1919 Acc: 0.4067\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1227 Acc: 0.6973\n",
      "val Loss: 0.1606 Acc: 0.4328\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1171 Acc: 0.7047\n",
      "val Loss: 0.1699 Acc: 0.4300\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1129 Acc: 0.7041\n",
      "val Loss: 0.1678 Acc: 0.4094\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1064 Acc: 0.7120\n",
      "val Loss: 0.1630 Acc: 0.4108\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.7099\n",
      "val Loss: 0.1683 Acc: 0.4114\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0943 Acc: 0.7208\n",
      "val Loss: 0.1469 Acc: 0.4131\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0883 Acc: 0.7272\n",
      "val Loss: 0.1484 Acc: 0.4144\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0826 Acc: 0.7369\n",
      "val Loss: 0.1475 Acc: 0.4069\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0882 Acc: 0.7335\n",
      "val Loss: 0.1443 Acc: 0.4125\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0869 Acc: 0.7340\n",
      "val Loss: 0.1321 Acc: 0.4261\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0782 Acc: 0.7415\n",
      "val Loss: 0.1295 Acc: 0.4108\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0754 Acc: 0.7451\n",
      "val Loss: 0.1445 Acc: 0.4175\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0726 Acc: 0.7481\n",
      "val Loss: 0.1280 Acc: 0.4222\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0707 Acc: 0.7519\n",
      "val Loss: 0.1299 Acc: 0.4050\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0707 Acc: 0.7535\n",
      "val Loss: 0.1226 Acc: 0.4219\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0710 Acc: 0.7528\n",
      "val Loss: 0.1169 Acc: 0.4142\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0722 Acc: 0.7507\n",
      "val Loss: 0.1246 Acc: 0.4164\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0667 Acc: 0.7591\n",
      "val Loss: 0.1195 Acc: 0.4231\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0660 Acc: 0.7590\n",
      "val Loss: 0.1184 Acc: 0.4167\n",
      "\n",
      "Best val Acc: 0.490000\n",
      "Accuracy on test set using Importance Reweighting Method: 0.9003333333333333\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6888 Acc: 0.4550\n",
      "val Loss: 0.5747 Acc: 0.4753\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6574 Acc: 0.4690\n",
      "val Loss: 0.6856 Acc: 0.4761\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6576 Acc: 0.4769\n",
      "val Loss: 0.6415 Acc: 0.4850\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6537 Acc: 0.4744\n",
      "val Loss: 0.6777 Acc: 0.4844\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6248 Acc: 0.4806\n",
      "val Loss: 0.7572 Acc: 0.4842\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6171 Acc: 0.4833\n",
      "val Loss: 0.6858 Acc: 0.4842\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6309 Acc: 0.4832\n",
      "val Loss: 0.5130 Acc: 0.4858\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6100 Acc: 0.4851\n",
      "val Loss: 0.5871 Acc: 0.4889\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6185 Acc: 0.4846\n",
      "val Loss: 0.5906 Acc: 0.4872\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6108 Acc: 0.4874\n",
      "val Loss: 0.5839 Acc: 0.4753\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.5883 Acc: 0.4884\n",
      "val Loss: 0.5914 Acc: 0.4853\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5711 Acc: 0.4925\n",
      "val Loss: 0.5494 Acc: 0.4769\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5624 Acc: 0.4960\n",
      "val Loss: 0.5190 Acc: 0.4811\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5402 Acc: 0.5024\n",
      "val Loss: 0.5259 Acc: 0.4706\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5096 Acc: 0.5081\n",
      "val Loss: 0.5578 Acc: 0.4725\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.4855 Acc: 0.5172\n",
      "val Loss: 0.4597 Acc: 0.4589\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.4658 Acc: 0.5224\n",
      "val Loss: 0.5348 Acc: 0.4706\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.4445 Acc: 0.5316\n",
      "val Loss: 0.4947 Acc: 0.4594\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4087 Acc: 0.5392\n",
      "val Loss: 0.4438 Acc: 0.4558\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.3923 Acc: 0.5492\n",
      "val Loss: 0.4384 Acc: 0.4481\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.3709 Acc: 0.5640\n",
      "val Loss: 0.4180 Acc: 0.4558\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.3425 Acc: 0.5726\n",
      "val Loss: 0.3710 Acc: 0.4511\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3284 Acc: 0.5797\n",
      "val Loss: 0.2906 Acc: 0.4531\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3003 Acc: 0.5917\n",
      "val Loss: 0.3621 Acc: 0.4431\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2820 Acc: 0.6013\n",
      "val Loss: 0.3246 Acc: 0.4381\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2701 Acc: 0.6109\n",
      "val Loss: 0.3083 Acc: 0.4506\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2429 Acc: 0.6253\n",
      "val Loss: 0.2851 Acc: 0.4456\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2122 Acc: 0.6422\n",
      "val Loss: 0.2416 Acc: 0.4539\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2033 Acc: 0.6508\n",
      "val Loss: 0.2495 Acc: 0.4353\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1994 Acc: 0.6574\n",
      "val Loss: 0.2096 Acc: 0.4403\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1792 Acc: 0.6669\n",
      "val Loss: 0.2452 Acc: 0.4156\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.6731\n",
      "val Loss: 0.1967 Acc: 0.4306\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.6825\n",
      "val Loss: 0.1935 Acc: 0.4233\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1519 Acc: 0.6872\n",
      "val Loss: 0.2061 Acc: 0.4239\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1390 Acc: 0.7042\n",
      "val Loss: 0.1833 Acc: 0.4422\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1266 Acc: 0.7164\n",
      "val Loss: 0.1986 Acc: 0.4372\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1231 Acc: 0.7149\n",
      "val Loss: 0.1835 Acc: 0.4369\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1219 Acc: 0.7228\n",
      "val Loss: 0.1800 Acc: 0.4192\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1174 Acc: 0.7258\n",
      "val Loss: 0.1511 Acc: 0.4311\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.7337\n",
      "val Loss: 0.1657 Acc: 0.4378\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1061 Acc: 0.7366\n",
      "val Loss: 0.1596 Acc: 0.4256\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.0951 Acc: 0.7467\n",
      "val Loss: 0.1354 Acc: 0.4483\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.0923 Acc: 0.7508\n",
      "val Loss: 0.1418 Acc: 0.4264\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.0929 Acc: 0.7521\n",
      "val Loss: 0.1625 Acc: 0.4256\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.0902 Acc: 0.7572\n",
      "val Loss: 0.1348 Acc: 0.4344\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0826 Acc: 0.7625\n",
      "val Loss: 0.1477 Acc: 0.4225\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.7669\n",
      "val Loss: 0.1528 Acc: 0.4161\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0783 Acc: 0.7699\n",
      "val Loss: 0.1498 Acc: 0.4092\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.7708\n",
      "val Loss: 0.1452 Acc: 0.4169\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0740 Acc: 0.7753\n",
      "val Loss: 0.1236 Acc: 0.4300\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0663 Acc: 0.7838\n",
      "val Loss: 0.1356 Acc: 0.4225\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0650 Acc: 0.7857\n",
      "val Loss: 0.1394 Acc: 0.4050\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0707 Acc: 0.7837\n",
      "val Loss: 0.1201 Acc: 0.4269\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0650 Acc: 0.7872\n",
      "val Loss: 0.1195 Acc: 0.4244\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0617 Acc: 0.7922\n",
      "val Loss: 0.1267 Acc: 0.4158\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0601 Acc: 0.7967\n",
      "val Loss: 0.1186 Acc: 0.4222\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0628 Acc: 0.7941\n",
      "val Loss: 0.1394 Acc: 0.4128\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0647 Acc: 0.7964\n",
      "val Loss: 0.1298 Acc: 0.4139\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0593 Acc: 0.8027\n",
      "val Loss: 0.1169 Acc: 0.4292\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0579 Acc: 0.8021\n",
      "val Loss: 0.1192 Acc: 0.4047\n",
      "\n",
      "Best val Acc: 0.488889\n",
      "Accuracy on test set using Importance Reweighting Method: 0.8846666666666667\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6911 Acc: 0.4590\n",
      "val Loss: 0.6085 Acc: 0.4617\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6529 Acc: 0.4728\n",
      "val Loss: 0.6148 Acc: 0.4850\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6475 Acc: 0.4788\n",
      "val Loss: 0.6617 Acc: 0.4803\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6484 Acc: 0.4778\n",
      "val Loss: 0.6522 Acc: 0.4800\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6276 Acc: 0.4832\n",
      "val Loss: 0.6581 Acc: 0.4886\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6166 Acc: 0.4838\n",
      "val Loss: 0.6738 Acc: 0.4808\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6361 Acc: 0.4794\n",
      "val Loss: 0.6189 Acc: 0.4861\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6384 Acc: 0.4858\n",
      "val Loss: 0.5875 Acc: 0.4839\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6096 Acc: 0.4908\n",
      "val Loss: 0.6215 Acc: 0.4764\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6077 Acc: 0.4917\n",
      "val Loss: 0.5842 Acc: 0.4739\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.5950 Acc: 0.4921\n",
      "val Loss: 0.5832 Acc: 0.4794\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5848 Acc: 0.4936\n",
      "val Loss: 0.6540 Acc: 0.4764\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5695 Acc: 0.5006\n",
      "val Loss: 0.6408 Acc: 0.4644\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5565 Acc: 0.5034\n",
      "val Loss: 0.5645 Acc: 0.4711\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5221 Acc: 0.5085\n",
      "val Loss: 0.5287 Acc: 0.4794\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5031 Acc: 0.5139\n",
      "val Loss: 0.4813 Acc: 0.4686\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.4764 Acc: 0.5189\n",
      "val Loss: 0.4944 Acc: 0.4561\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.4640 Acc: 0.5258\n",
      "val Loss: 0.5261 Acc: 0.4422\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4307 Acc: 0.5333\n",
      "val Loss: 0.5447 Acc: 0.4386\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.4248 Acc: 0.5428\n",
      "val Loss: 0.5252 Acc: 0.4439\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.3835 Acc: 0.5535\n",
      "val Loss: 0.3870 Acc: 0.4522\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.3485 Acc: 0.5603\n",
      "val Loss: 0.3638 Acc: 0.4372\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3426 Acc: 0.5746\n",
      "val Loss: 0.3497 Acc: 0.4289\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3105 Acc: 0.5812\n",
      "val Loss: 0.3243 Acc: 0.4436\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2941 Acc: 0.5947\n",
      "val Loss: 0.3203 Acc: 0.4442\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2745 Acc: 0.6070\n",
      "val Loss: 0.2754 Acc: 0.4331\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2534 Acc: 0.6139\n",
      "val Loss: 0.2816 Acc: 0.4269\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2312 Acc: 0.6317\n",
      "val Loss: 0.2610 Acc: 0.4308\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2133 Acc: 0.6371\n",
      "val Loss: 0.2508 Acc: 0.4222\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1945 Acc: 0.6506\n",
      "val Loss: 0.2279 Acc: 0.4375\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1945 Acc: 0.6536\n",
      "val Loss: 0.2432 Acc: 0.4175\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1799 Acc: 0.6645\n",
      "val Loss: 0.1949 Acc: 0.4369\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1676 Acc: 0.6786\n",
      "val Loss: 0.2223 Acc: 0.4369\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1536 Acc: 0.6887\n",
      "val Loss: 0.2011 Acc: 0.4250\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1440 Acc: 0.6932\n",
      "val Loss: 0.1957 Acc: 0.4325\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.7031\n",
      "val Loss: 0.1725 Acc: 0.4356\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1284 Acc: 0.7023\n",
      "val Loss: 0.1900 Acc: 0.4294\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1180 Acc: 0.7130\n",
      "val Loss: 0.1752 Acc: 0.4106\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1189 Acc: 0.7149\n",
      "val Loss: 0.1894 Acc: 0.4333\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1036 Acc: 0.7299\n",
      "val Loss: 0.1776 Acc: 0.4267\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.7356\n",
      "val Loss: 0.1505 Acc: 0.4300\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1018 Acc: 0.7306\n",
      "val Loss: 0.1736 Acc: 0.4106\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1007 Acc: 0.7362\n",
      "val Loss: 0.1506 Acc: 0.4369\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.0905 Acc: 0.7427\n",
      "val Loss: 0.1586 Acc: 0.4231\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.0943 Acc: 0.7415\n",
      "val Loss: 0.1346 Acc: 0.4322\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0928 Acc: 0.7493\n",
      "val Loss: 0.1569 Acc: 0.4258\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0824 Acc: 0.7565\n",
      "val Loss: 0.1588 Acc: 0.4153\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.7635\n",
      "val Loss: 0.1294 Acc: 0.4214\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0820 Acc: 0.7622\n",
      "val Loss: 0.1426 Acc: 0.4189\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.7673\n",
      "val Loss: 0.1344 Acc: 0.4267\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0757 Acc: 0.7687\n",
      "val Loss: 0.1391 Acc: 0.4186\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0691 Acc: 0.7777\n",
      "val Loss: 0.1395 Acc: 0.4233\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0684 Acc: 0.7800\n",
      "val Loss: 0.1290 Acc: 0.4114\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0709 Acc: 0.7781\n",
      "val Loss: 0.1304 Acc: 0.4139\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0667 Acc: 0.7808\n",
      "val Loss: 0.1152 Acc: 0.4181\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0650 Acc: 0.7872\n",
      "val Loss: 0.1343 Acc: 0.4114\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0630 Acc: 0.7876\n",
      "val Loss: 0.1283 Acc: 0.4194\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0627 Acc: 0.7876\n",
      "val Loss: 0.1264 Acc: 0.4119\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0609 Acc: 0.7904\n",
      "val Loss: 0.1232 Acc: 0.4069\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0565 Acc: 0.7949\n",
      "val Loss: 0.1261 Acc: 0.4147\n",
      "\n",
      "Best val Acc: 0.488611\n",
      "Accuracy on test set using Importance Reweighting Method: 0.9026666666666666\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6626 Acc: 0.4542\n",
      "val Loss: 0.7458 Acc: 0.4806\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6561 Acc: 0.4745\n",
      "val Loss: 0.6443 Acc: 0.4700\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6568 Acc: 0.4771\n",
      "val Loss: 0.6097 Acc: 0.4875\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6347 Acc: 0.4809\n",
      "val Loss: 0.6833 Acc: 0.4847\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6473 Acc: 0.4823\n",
      "val Loss: 0.6735 Acc: 0.4539\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6178 Acc: 0.4858\n",
      "val Loss: 0.7151 Acc: 0.4833\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6159 Acc: 0.4885\n",
      "val Loss: 0.6317 Acc: 0.4867\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.5977 Acc: 0.4911\n",
      "val Loss: 0.6518 Acc: 0.4686\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.5941 Acc: 0.4935\n",
      "val Loss: 0.5377 Acc: 0.4861\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.5714 Acc: 0.4949\n",
      "val Loss: 0.6168 Acc: 0.4764\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.5360 Acc: 0.5058\n",
      "val Loss: 0.5482 Acc: 0.4806\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5191 Acc: 0.5113\n",
      "val Loss: 0.4179 Acc: 0.4842\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.4922 Acc: 0.5215\n",
      "val Loss: 0.5062 Acc: 0.4633\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.4511 Acc: 0.5299\n",
      "val Loss: 0.4899 Acc: 0.4564\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.4212 Acc: 0.5451\n",
      "val Loss: 0.4602 Acc: 0.4597\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.3886 Acc: 0.5508\n",
      "val Loss: 0.4186 Acc: 0.4631\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.3687 Acc: 0.5695\n",
      "val Loss: 0.3742 Acc: 0.4478\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.3342 Acc: 0.5820\n",
      "val Loss: 0.3386 Acc: 0.4411\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.3173 Acc: 0.5975\n",
      "val Loss: 0.3132 Acc: 0.4508\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2923 Acc: 0.6129\n",
      "val Loss: 0.3321 Acc: 0.4219\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2697 Acc: 0.6212\n",
      "val Loss: 0.2969 Acc: 0.4403\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2484 Acc: 0.6371\n",
      "val Loss: 0.2657 Acc: 0.4222\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2296 Acc: 0.6480\n",
      "val Loss: 0.2870 Acc: 0.4256\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.2085 Acc: 0.6678\n",
      "val Loss: 0.2844 Acc: 0.4092\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1962 Acc: 0.6763\n",
      "val Loss: 0.2313 Acc: 0.4297\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.6908\n",
      "val Loss: 0.2110 Acc: 0.4300\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1655 Acc: 0.6933\n",
      "val Loss: 0.2048 Acc: 0.4147\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1613 Acc: 0.6999\n",
      "val Loss: 0.2156 Acc: 0.4114\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1409 Acc: 0.7148\n",
      "val Loss: 0.2248 Acc: 0.4089\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1341 Acc: 0.7274\n",
      "val Loss: 0.1892 Acc: 0.4208\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1241 Acc: 0.7344\n",
      "val Loss: 0.1973 Acc: 0.4064\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1190 Acc: 0.7369\n",
      "val Loss: 0.1693 Acc: 0.4197\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1149 Acc: 0.7452\n",
      "val Loss: 0.1744 Acc: 0.4064\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1044 Acc: 0.7539\n",
      "val Loss: 0.1843 Acc: 0.4008\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1012 Acc: 0.7563\n",
      "val Loss: 0.1774 Acc: 0.4133\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.0937 Acc: 0.7633\n",
      "val Loss: 0.1582 Acc: 0.4075\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.0897 Acc: 0.7704\n",
      "val Loss: 0.1620 Acc: 0.3922\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.0911 Acc: 0.7691\n",
      "val Loss: 0.1620 Acc: 0.3978\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.7721\n",
      "val Loss: 0.1454 Acc: 0.4028\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.0820 Acc: 0.7810\n",
      "val Loss: 0.1467 Acc: 0.4031\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.0779 Acc: 0.7733\n",
      "val Loss: 0.1330 Acc: 0.4067\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.0760 Acc: 0.7883\n",
      "val Loss: 0.1550 Acc: 0.3958\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.0717 Acc: 0.7885\n",
      "val Loss: 0.1519 Acc: 0.4022\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.0683 Acc: 0.7944\n",
      "val Loss: 0.1444 Acc: 0.3969\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.0652 Acc: 0.7992\n",
      "val Loss: 0.1425 Acc: 0.4089\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0659 Acc: 0.7987\n",
      "val Loss: 0.1393 Acc: 0.4122\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0638 Acc: 0.8034\n",
      "val Loss: 0.1329 Acc: 0.4011\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0565 Acc: 0.8097\n",
      "val Loss: 0.1216 Acc: 0.4086\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0587 Acc: 0.8100\n",
      "val Loss: 0.1227 Acc: 0.4092\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0585 Acc: 0.8109\n",
      "val Loss: 0.1340 Acc: 0.3975\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0597 Acc: 0.8102\n",
      "val Loss: 0.1273 Acc: 0.4031\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0593 Acc: 0.8120\n",
      "val Loss: 0.1321 Acc: 0.4053\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0521 Acc: 0.8214\n",
      "val Loss: 0.1293 Acc: 0.4022\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0508 Acc: 0.8207\n",
      "val Loss: 0.1127 Acc: 0.4000\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0509 Acc: 0.8237\n",
      "val Loss: 0.1171 Acc: 0.4003\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0555 Acc: 0.8159\n",
      "val Loss: 0.1224 Acc: 0.3992\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0532 Acc: 0.8250\n",
      "val Loss: 0.1276 Acc: 0.3975\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0475 Acc: 0.8292\n",
      "val Loss: 0.1125 Acc: 0.4000\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0460 Acc: 0.8334\n",
      "val Loss: 0.1087 Acc: 0.4014\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0467 Acc: 0.8317\n",
      "val Loss: 0.1243 Acc: 0.4028\n",
      "\n",
      "Best val Acc: 0.487500\n",
      "Accuracy on test set using Importance Reweighting Method: 0.9026666666666666\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6577 Acc: 0.4587\n",
      "val Loss: 0.6493 Acc: 0.4792\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6397 Acc: 0.4702\n",
      "val Loss: 0.6694 Acc: 0.4764\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6394 Acc: 0.4743\n",
      "val Loss: 0.6314 Acc: 0.4781\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6351 Acc: 0.4796\n",
      "val Loss: 0.6586 Acc: 0.4864\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6619 Acc: 0.4753\n",
      "val Loss: 0.6182 Acc: 0.4936\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6431 Acc: 0.4804\n",
      "val Loss: 0.5899 Acc: 0.4822\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6301 Acc: 0.4809\n",
      "val Loss: 0.6248 Acc: 0.4867\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6214 Acc: 0.4829\n",
      "val Loss: 0.5621 Acc: 0.4892\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6240 Acc: 0.4860\n",
      "val Loss: 0.5415 Acc: 0.4883\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6104 Acc: 0.4876\n",
      "val Loss: 0.6098 Acc: 0.4892\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.6009 Acc: 0.4886\n",
      "val Loss: 0.6501 Acc: 0.4786\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5875 Acc: 0.4883\n",
      "val Loss: 0.6505 Acc: 0.4858\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5993 Acc: 0.4919\n",
      "val Loss: 0.5539 Acc: 0.4839\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5737 Acc: 0.4934\n",
      "val Loss: 0.5773 Acc: 0.4861\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5460 Acc: 0.5004\n",
      "val Loss: 0.5437 Acc: 0.4769\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5408 Acc: 0.5012\n",
      "val Loss: 0.6274 Acc: 0.4781\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.5249 Acc: 0.5035\n",
      "val Loss: 0.5712 Acc: 0.4611\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.4960 Acc: 0.5133\n",
      "val Loss: 0.5650 Acc: 0.4689\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4756 Acc: 0.5165\n",
      "val Loss: 0.4758 Acc: 0.4781\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.4478 Acc: 0.5279\n",
      "val Loss: 0.4740 Acc: 0.4742\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.4330 Acc: 0.5315\n",
      "val Loss: 0.4488 Acc: 0.4697\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.4065 Acc: 0.5415\n",
      "val Loss: 0.4609 Acc: 0.4603\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3799 Acc: 0.5510\n",
      "val Loss: 0.3730 Acc: 0.4694\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3641 Acc: 0.5572\n",
      "val Loss: 0.4035 Acc: 0.4472\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.3411 Acc: 0.5694\n",
      "val Loss: 0.3452 Acc: 0.4486\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.3190 Acc: 0.5792\n",
      "val Loss: 0.3569 Acc: 0.4458\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2971 Acc: 0.5953\n",
      "val Loss: 0.3314 Acc: 0.4494\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2827 Acc: 0.6010\n",
      "val Loss: 0.3106 Acc: 0.4597\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.6144\n",
      "val Loss: 0.2897 Acc: 0.4489\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.2424 Acc: 0.6263\n",
      "val Loss: 0.2362 Acc: 0.4522\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.2311 Acc: 0.6349\n",
      "val Loss: 0.2679 Acc: 0.4486\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.2110 Acc: 0.6475\n",
      "val Loss: 0.2222 Acc: 0.4517\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.2017 Acc: 0.6544\n",
      "val Loss: 0.2177 Acc: 0.4464\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1916 Acc: 0.6622\n",
      "val Loss: 0.2409 Acc: 0.4428\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1916 Acc: 0.6687\n",
      "val Loss: 0.2442 Acc: 0.4378\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1630 Acc: 0.6790\n",
      "val Loss: 0.1947 Acc: 0.4364\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1570 Acc: 0.6874\n",
      "val Loss: 0.2302 Acc: 0.4106\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1427 Acc: 0.6979\n",
      "val Loss: 0.1928 Acc: 0.4350\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1366 Acc: 0.7049\n",
      "val Loss: 0.1838 Acc: 0.4267\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1281 Acc: 0.7133\n",
      "val Loss: 0.1855 Acc: 0.4283\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1251 Acc: 0.7193\n",
      "val Loss: 0.1805 Acc: 0.4214\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1179 Acc: 0.7186\n",
      "val Loss: 0.2055 Acc: 0.4261\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1198 Acc: 0.7235\n",
      "val Loss: 0.1812 Acc: 0.4292\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1057 Acc: 0.7378\n",
      "val Loss: 0.1723 Acc: 0.4161\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1047 Acc: 0.7392\n",
      "val Loss: 0.1777 Acc: 0.4189\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0942 Acc: 0.7440\n",
      "val Loss: 0.1483 Acc: 0.4378\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0897 Acc: 0.7507\n",
      "val Loss: 0.1611 Acc: 0.4350\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.7561\n",
      "val Loss: 0.1577 Acc: 0.4331\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0834 Acc: 0.7597\n",
      "val Loss: 0.1353 Acc: 0.4392\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0778 Acc: 0.7652\n",
      "val Loss: 0.1528 Acc: 0.4258\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.7688\n",
      "val Loss: 0.1597 Acc: 0.4158\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0747 Acc: 0.7744\n",
      "val Loss: 0.1248 Acc: 0.4328\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0746 Acc: 0.7762\n",
      "val Loss: 0.1425 Acc: 0.4153\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0721 Acc: 0.7810\n",
      "val Loss: 0.1219 Acc: 0.4275\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0732 Acc: 0.7751\n",
      "val Loss: 0.1440 Acc: 0.4200\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0659 Acc: 0.7878\n",
      "val Loss: 0.1267 Acc: 0.4258\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0606 Acc: 0.7899\n",
      "val Loss: 0.1297 Acc: 0.4278\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0625 Acc: 0.7895\n",
      "val Loss: 0.1274 Acc: 0.4197\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0626 Acc: 0.7958\n",
      "val Loss: 0.1240 Acc: 0.4339\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0588 Acc: 0.7997\n",
      "val Loss: 0.1234 Acc: 0.4136\n",
      "\n",
      "Best val Acc: 0.493611\n",
      "Accuracy on test set using Importance Reweighting Method: 0.9263333333333333\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6845 Acc: 0.4555\n",
      "val Loss: 0.7546 Acc: 0.4753\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.6639 Acc: 0.4703\n",
      "val Loss: 0.7152 Acc: 0.4836\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.4740\n",
      "val Loss: 0.7098 Acc: 0.4783\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.6560 Acc: 0.4754\n",
      "val Loss: 0.6310 Acc: 0.4764\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.6371 Acc: 0.4806\n",
      "val Loss: 0.6499 Acc: 0.4747\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.6466 Acc: 0.4788\n",
      "val Loss: 0.6314 Acc: 0.4800\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.6330 Acc: 0.4835\n",
      "val Loss: 0.6437 Acc: 0.4047\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.6298 Acc: 0.4835\n",
      "val Loss: 0.6812 Acc: 0.4861\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.6398 Acc: 0.4851\n",
      "val Loss: 0.6016 Acc: 0.4839\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.6089 Acc: 0.4859\n",
      "val Loss: 0.6204 Acc: 0.4783\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.6026 Acc: 0.4896\n",
      "val Loss: 0.5931 Acc: 0.4786\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.5751 Acc: 0.4935\n",
      "val Loss: 0.5889 Acc: 0.4764\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.5721 Acc: 0.4944\n",
      "val Loss: 0.5604 Acc: 0.4764\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.5477 Acc: 0.5006\n",
      "val Loss: 0.5475 Acc: 0.4728\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.5109 Acc: 0.5078\n",
      "val Loss: 0.5839 Acc: 0.4606\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.5126 Acc: 0.5097\n",
      "val Loss: 0.5313 Acc: 0.4547\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.4724 Acc: 0.5242\n",
      "val Loss: 0.5043 Acc: 0.4508\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.4555 Acc: 0.5340\n",
      "val Loss: 0.5165 Acc: 0.4458\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.4265 Acc: 0.5347\n",
      "val Loss: 0.4868 Acc: 0.4472\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.3942 Acc: 0.5483\n",
      "val Loss: 0.4169 Acc: 0.4497\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.3695 Acc: 0.5589\n",
      "val Loss: 0.3938 Acc: 0.4619\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.3495 Acc: 0.5706\n",
      "val Loss: 0.3446 Acc: 0.4522\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.3226 Acc: 0.5834\n",
      "val Loss: 0.3299 Acc: 0.4294\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.3019 Acc: 0.5940\n",
      "val Loss: 0.3468 Acc: 0.4478\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2896 Acc: 0.6028\n",
      "val Loss: 0.3154 Acc: 0.4469\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2641 Acc: 0.6147\n",
      "val Loss: 0.3476 Acc: 0.4214\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2529 Acc: 0.6216\n",
      "val Loss: 0.2910 Acc: 0.4508\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2269 Acc: 0.6387\n",
      "val Loss: 0.2620 Acc: 0.4378\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.2148 Acc: 0.6465\n",
      "val Loss: 0.2478 Acc: 0.4358\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1986 Acc: 0.6602\n",
      "val Loss: 0.2621 Acc: 0.4322\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1805 Acc: 0.6719\n",
      "val Loss: 0.2163 Acc: 0.4328\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.6756\n",
      "val Loss: 0.2550 Acc: 0.4219\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1638 Acc: 0.6823\n",
      "val Loss: 0.1944 Acc: 0.4372\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1540 Acc: 0.6929\n",
      "val Loss: 0.1827 Acc: 0.4303\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.7018\n",
      "val Loss: 0.2023 Acc: 0.4264\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1407 Acc: 0.7072\n",
      "val Loss: 0.1949 Acc: 0.4239\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1372 Acc: 0.7095\n",
      "val Loss: 0.1912 Acc: 0.4144\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1242 Acc: 0.7265\n",
      "val Loss: 0.1872 Acc: 0.4042\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1205 Acc: 0.7242\n",
      "val Loss: 0.1892 Acc: 0.4067\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1059 Acc: 0.7404\n",
      "val Loss: 0.1641 Acc: 0.4131\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1029 Acc: 0.7428\n",
      "val Loss: 0.1492 Acc: 0.4339\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.7468\n",
      "val Loss: 0.1725 Acc: 0.4192\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 0.7504\n",
      "val Loss: 0.1751 Acc: 0.3925\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.0912 Acc: 0.7553\n",
      "val Loss: 0.1529 Acc: 0.4044\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.0871 Acc: 0.7578\n",
      "val Loss: 0.1400 Acc: 0.4153\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0852 Acc: 0.7670\n",
      "val Loss: 0.1487 Acc: 0.4064\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0815 Acc: 0.7750\n",
      "val Loss: 0.1560 Acc: 0.4111\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.7737\n",
      "val Loss: 0.1406 Acc: 0.4211\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0741 Acc: 0.7794\n",
      "val Loss: 0.1483 Acc: 0.4031\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0734 Acc: 0.7797\n",
      "val Loss: 0.1362 Acc: 0.4131\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0685 Acc: 0.7864\n",
      "val Loss: 0.1356 Acc: 0.4097\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0688 Acc: 0.7881\n",
      "val Loss: 0.1573 Acc: 0.4028\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0642 Acc: 0.7933\n",
      "val Loss: 0.1495 Acc: 0.3803\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0657 Acc: 0.7929\n",
      "val Loss: 0.1287 Acc: 0.4067\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0612 Acc: 0.8006\n",
      "val Loss: 0.1351 Acc: 0.4078\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0629 Acc: 0.7963\n",
      "val Loss: 0.1452 Acc: 0.4017\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0644 Acc: 0.7978\n",
      "val Loss: 0.1318 Acc: 0.4183\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0584 Acc: 0.8018\n",
      "val Loss: 0.1340 Acc: 0.4139\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0539 Acc: 0.8099\n",
      "val Loss: 0.1354 Acc: 0.4119\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0523 Acc: 0.8111\n",
      "val Loss: 0.1208 Acc: 0.4106\n",
      "\n",
      "Best val Acc: 0.486111\n",
      "Accuracy on test set using Importance Reweighting Method: 0.928\n",
      "\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset fashionmnist0.6 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "tensor([[0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000]], device='cuda:0')\n",
      "..............................\n",
      "\n",
      "Training iteration: 0\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6686 Acc: 0.3694\n",
      "val Loss: 0.4927 Acc: 0.3558\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.5599 Acc: 0.3814\n",
      "val Loss: 0.4336 Acc: 0.3844\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.4483 Acc: 0.3867\n",
      "val Loss: 0.4036 Acc: 0.3964\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3814 Acc: 0.3915\n",
      "val Loss: 0.4927 Acc: 0.3711\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3993 Acc: 0.3890\n",
      "val Loss: 0.3582 Acc: 0.3933\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3797 Acc: 0.3920\n",
      "val Loss: 0.3076 Acc: 0.3936\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3578 Acc: 0.3942\n",
      "val Loss: 0.3447 Acc: 0.3894\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2974 Acc: 0.3992\n",
      "val Loss: 0.2827 Acc: 0.3989\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2916 Acc: 0.3986\n",
      "val Loss: 0.2973 Acc: 0.3947\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.3074 Acc: 0.3992\n",
      "val Loss: 0.2497 Acc: 0.3950\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2568 Acc: 0.4065\n",
      "val Loss: 0.2244 Acc: 0.3958\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2459 Acc: 0.4061\n",
      "val Loss: 0.2184 Acc: 0.3931\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2411 Acc: 0.4051\n",
      "val Loss: 0.2167 Acc: 0.3944\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2309 Acc: 0.4097\n",
      "val Loss: 0.3155 Acc: 0.3836\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2740 Acc: 0.4073\n",
      "val Loss: 0.2374 Acc: 0.3850\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2312 Acc: 0.4163\n",
      "val Loss: 0.2213 Acc: 0.3747\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2101 Acc: 0.4205\n",
      "val Loss: 0.2646 Acc: 0.3828\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2185 Acc: 0.4214\n",
      "val Loss: 0.1690 Acc: 0.3889\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2111 Acc: 0.4249\n",
      "val Loss: 0.1433 Acc: 0.3844\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2032 Acc: 0.4333\n",
      "val Loss: 0.2468 Acc: 0.3742\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2029 Acc: 0.4389\n",
      "val Loss: 0.2215 Acc: 0.3811\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1939 Acc: 0.4434\n",
      "val Loss: 0.1889 Acc: 0.3767\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1913 Acc: 0.4475\n",
      "val Loss: 0.1929 Acc: 0.3850\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1879 Acc: 0.4577\n",
      "val Loss: 0.1862 Acc: 0.3794\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1730 Acc: 0.4674\n",
      "val Loss: 0.1654 Acc: 0.3747\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1584 Acc: 0.4723\n",
      "val Loss: 0.1658 Acc: 0.3767\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1621 Acc: 0.4810\n",
      "val Loss: 0.1680 Acc: 0.3775\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1559 Acc: 0.4840\n",
      "val Loss: 0.1576 Acc: 0.3781\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1640 Acc: 0.4940\n",
      "val Loss: 0.1714 Acc: 0.3761\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1585 Acc: 0.5059\n",
      "val Loss: 0.1827 Acc: 0.3642\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1481 Acc: 0.5180\n",
      "val Loss: 0.1700 Acc: 0.3742\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1427 Acc: 0.5211\n",
      "val Loss: 0.1537 Acc: 0.3689\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1374 Acc: 0.5293\n",
      "val Loss: 0.1537 Acc: 0.3639\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1299 Acc: 0.5382\n",
      "val Loss: 0.1546 Acc: 0.3633\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1309 Acc: 0.5438\n",
      "val Loss: 0.1454 Acc: 0.3628\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1261 Acc: 0.5549\n",
      "val Loss: 0.1523 Acc: 0.3672\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1223 Acc: 0.5613\n",
      "val Loss: 0.1503 Acc: 0.3667\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1223 Acc: 0.5673\n",
      "val Loss: 0.1574 Acc: 0.3675\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.5681\n",
      "val Loss: 0.1474 Acc: 0.3708\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.5786\n",
      "val Loss: 0.1378 Acc: 0.3631\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1121 Acc: 0.5874\n",
      "val Loss: 0.1352 Acc: 0.3703\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1084 Acc: 0.5901\n",
      "val Loss: 0.1260 Acc: 0.3697\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1039 Acc: 0.5990\n",
      "val Loss: 0.1271 Acc: 0.3639\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.0994 Acc: 0.6067\n",
      "val Loss: 0.1259 Acc: 0.3592\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1018 Acc: 0.6084\n",
      "val Loss: 0.1313 Acc: 0.3661\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.6182\n",
      "val Loss: 0.1068 Acc: 0.3700\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.0944 Acc: 0.6212\n",
      "val Loss: 0.1380 Acc: 0.3561\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 0.6208\n",
      "val Loss: 0.1190 Acc: 0.3547\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0901 Acc: 0.6347\n",
      "val Loss: 0.1072 Acc: 0.3647\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0912 Acc: 0.6408\n",
      "val Loss: 0.1141 Acc: 0.3569\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0896 Acc: 0.6466\n",
      "val Loss: 0.1153 Acc: 0.3503\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0840 Acc: 0.6503\n",
      "val Loss: 0.1078 Acc: 0.3517\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0823 Acc: 0.6560\n",
      "val Loss: 0.1096 Acc: 0.3547\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.6567\n",
      "val Loss: 0.1074 Acc: 0.3650\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0784 Acc: 0.6669\n",
      "val Loss: 0.1269 Acc: 0.3556\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0753 Acc: 0.6740\n",
      "val Loss: 0.1037 Acc: 0.3628\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.6730\n",
      "val Loss: 0.1055 Acc: 0.3467\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0711 Acc: 0.6767\n",
      "val Loss: 0.1056 Acc: 0.3600\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.6773\n",
      "val Loss: 0.0905 Acc: 0.3633\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0690 Acc: 0.6891\n",
      "val Loss: 0.1103 Acc: 0.3533\n",
      "\n",
      "Best val Acc: 0.398889\n",
      "Accuracy on test set using Importance Reweighting Method: 0.873\n",
      "\n",
      "\n",
      "Training iteration: 1\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6334 Acc: 0.3778\n",
      "val Loss: 0.3497 Acc: 0.3831\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4286 Acc: 0.3874\n",
      "val Loss: 0.4313 Acc: 0.3908\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3660 Acc: 0.3892\n",
      "val Loss: 0.4071 Acc: 0.3839\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3631 Acc: 0.3890\n",
      "val Loss: 0.3375 Acc: 0.3981\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3559 Acc: 0.3931\n",
      "val Loss: 0.3745 Acc: 0.3908\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.2902 Acc: 0.3947\n",
      "val Loss: 0.3342 Acc: 0.3922\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3342 Acc: 0.3917\n",
      "val Loss: 0.3200 Acc: 0.3986\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2874 Acc: 0.3979\n",
      "val Loss: 0.3211 Acc: 0.4008\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2789 Acc: 0.3983\n",
      "val Loss: 0.2185 Acc: 0.3931\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2578 Acc: 0.3998\n",
      "val Loss: 0.2641 Acc: 0.3947\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2744 Acc: 0.3974\n",
      "val Loss: 0.2921 Acc: 0.3936\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2266 Acc: 0.4026\n",
      "val Loss: 0.2080 Acc: 0.3978\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.1888 Acc: 0.4044\n",
      "val Loss: 0.2015 Acc: 0.3953\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.1985 Acc: 0.4076\n",
      "val Loss: 0.1919 Acc: 0.4022\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2047 Acc: 0.4060\n",
      "val Loss: 0.1590 Acc: 0.3947\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.1920 Acc: 0.4078\n",
      "val Loss: 0.1995 Acc: 0.4008\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2038 Acc: 0.4095\n",
      "val Loss: 0.1483 Acc: 0.4000\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.1972 Acc: 0.4123\n",
      "val Loss: 0.2028 Acc: 0.3950\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2033 Acc: 0.4145\n",
      "val Loss: 0.1739 Acc: 0.3936\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.1725 Acc: 0.4176\n",
      "val Loss: 0.1207 Acc: 0.3992\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.1899 Acc: 0.4174\n",
      "val Loss: 0.1943 Acc: 0.3928\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2060 Acc: 0.4244\n",
      "val Loss: 0.1662 Acc: 0.3922\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2123 Acc: 0.4249\n",
      "val Loss: 0.2669 Acc: 0.3764\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1921 Acc: 0.4348\n",
      "val Loss: 0.1672 Acc: 0.3894\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1973 Acc: 0.4375\n",
      "val Loss: 0.2330 Acc: 0.3889\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1989 Acc: 0.4368\n",
      "val Loss: 0.1593 Acc: 0.3878\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1939 Acc: 0.4438\n",
      "val Loss: 0.1816 Acc: 0.3828\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.2027 Acc: 0.4419\n",
      "val Loss: 0.1592 Acc: 0.3911\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1862 Acc: 0.4497\n",
      "val Loss: 0.1824 Acc: 0.3800\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1917 Acc: 0.4484\n",
      "val Loss: 0.1379 Acc: 0.3925\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1868 Acc: 0.4603\n",
      "val Loss: 0.1855 Acc: 0.3789\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1770 Acc: 0.4611\n",
      "val Loss: 0.1953 Acc: 0.3819\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1776 Acc: 0.4619\n",
      "val Loss: 0.1953 Acc: 0.3653\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1685 Acc: 0.4680\n",
      "val Loss: 0.1576 Acc: 0.3778\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1754 Acc: 0.4706\n",
      "val Loss: 0.1318 Acc: 0.3919\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1633 Acc: 0.4778\n",
      "val Loss: 0.1582 Acc: 0.3797\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1701 Acc: 0.4806\n",
      "val Loss: 0.1357 Acc: 0.3833\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1630 Acc: 0.4820\n",
      "val Loss: 0.1412 Acc: 0.3814\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1591 Acc: 0.4896\n",
      "val Loss: 0.1442 Acc: 0.3792\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1546 Acc: 0.4972\n",
      "val Loss: 0.1524 Acc: 0.3814\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1479 Acc: 0.5035\n",
      "val Loss: 0.1412 Acc: 0.3808\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1462 Acc: 0.4976\n",
      "val Loss: 0.1443 Acc: 0.3747\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1413 Acc: 0.5100\n",
      "val Loss: 0.1330 Acc: 0.3708\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.5110\n",
      "val Loss: 0.1343 Acc: 0.3758\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1398 Acc: 0.5213\n",
      "val Loss: 0.1398 Acc: 0.3706\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1409 Acc: 0.5277\n",
      "val Loss: 0.1461 Acc: 0.3694\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.5292\n",
      "val Loss: 0.1347 Acc: 0.3653\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1311 Acc: 0.5365\n",
      "val Loss: 0.1313 Acc: 0.3717\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1289 Acc: 0.5478\n",
      "val Loss: 0.1603 Acc: 0.3628\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1226 Acc: 0.5499\n",
      "val Loss: 0.1304 Acc: 0.3678\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.5578\n",
      "val Loss: 0.1341 Acc: 0.3681\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1129 Acc: 0.5606\n",
      "val Loss: 0.1398 Acc: 0.3672\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.1158 Acc: 0.5633\n",
      "val Loss: 0.1189 Acc: 0.3697\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1072 Acc: 0.5683\n",
      "val Loss: 0.1066 Acc: 0.3725\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.1089 Acc: 0.5769\n",
      "val Loss: 0.1228 Acc: 0.3725\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.5792\n",
      "val Loss: 0.1023 Acc: 0.3708\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.1012 Acc: 0.5826\n",
      "val Loss: 0.1035 Acc: 0.3772\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0997 Acc: 0.5911\n",
      "val Loss: 0.1256 Acc: 0.3647\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.5884\n",
      "val Loss: 0.1232 Acc: 0.3697\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.5882\n",
      "val Loss: 0.1040 Acc: 0.3603\n",
      "\n",
      "Best val Acc: 0.402222\n",
      "Accuracy on test set using Importance Reweighting Method: 0.8586666666666667\n",
      "\n",
      "\n",
      "Training iteration: 2\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.5609 Acc: 0.3774\n",
      "val Loss: 0.4374 Acc: 0.4000\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4040 Acc: 0.3866\n",
      "val Loss: 0.3518 Acc: 0.3950\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.4029 Acc: 0.3869\n",
      "val Loss: 0.3410 Acc: 0.3819\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3452 Acc: 0.3920\n",
      "val Loss: 0.3218 Acc: 0.3969\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3159 Acc: 0.3915\n",
      "val Loss: 0.2996 Acc: 0.3986\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.2823 Acc: 0.3887\n",
      "val Loss: 0.2865 Acc: 0.3578\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.4173 Acc: 0.3819\n",
      "val Loss: 0.2913 Acc: 0.3986\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2994 Acc: 0.3928\n",
      "val Loss: 0.2645 Acc: 0.3953\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2786 Acc: 0.3942\n",
      "val Loss: 0.2365 Acc: 0.3989\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2878 Acc: 0.3918\n",
      "val Loss: 0.2495 Acc: 0.4003\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2688 Acc: 0.3941\n",
      "val Loss: 0.2752 Acc: 0.3928\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2669 Acc: 0.3937\n",
      "val Loss: 0.2787 Acc: 0.3903\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2703 Acc: 0.3933\n",
      "val Loss: 0.2243 Acc: 0.4031\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2345 Acc: 0.3992\n",
      "val Loss: 0.2605 Acc: 0.3922\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2351 Acc: 0.4012\n",
      "val Loss: 0.2621 Acc: 0.3969\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2281 Acc: 0.4004\n",
      "val Loss: 0.2553 Acc: 0.3831\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2480 Acc: 0.4015\n",
      "val Loss: 0.2312 Acc: 0.3903\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2468 Acc: 0.4008\n",
      "val Loss: 0.2072 Acc: 0.3994\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2525 Acc: 0.4056\n",
      "val Loss: 0.1803 Acc: 0.3986\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2365 Acc: 0.4042\n",
      "val Loss: 0.2514 Acc: 0.3936\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2263 Acc: 0.4087\n",
      "val Loss: 0.2293 Acc: 0.3967\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2251 Acc: 0.4101\n",
      "val Loss: 0.2001 Acc: 0.3894\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2016 Acc: 0.4111\n",
      "val Loss: 0.2316 Acc: 0.3983\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.2040 Acc: 0.4172\n",
      "val Loss: 0.1775 Acc: 0.3894\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1866 Acc: 0.4206\n",
      "val Loss: 0.1944 Acc: 0.3964\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2058 Acc: 0.4213\n",
      "val Loss: 0.1570 Acc: 0.3914\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1907 Acc: 0.4230\n",
      "val Loss: 0.1786 Acc: 0.3864\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1804 Acc: 0.4240\n",
      "val Loss: 0.1769 Acc: 0.3867\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.4344\n",
      "val Loss: 0.1811 Acc: 0.3897\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1604 Acc: 0.4355\n",
      "val Loss: 0.1811 Acc: 0.3778\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1684 Acc: 0.4328\n",
      "val Loss: 0.1873 Acc: 0.3897\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1864 Acc: 0.4372\n",
      "val Loss: 0.1405 Acc: 0.3839\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1799 Acc: 0.4451\n",
      "val Loss: 0.1744 Acc: 0.3817\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1617 Acc: 0.4524\n",
      "val Loss: 0.1469 Acc: 0.3842\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1697 Acc: 0.4535\n",
      "val Loss: 0.1566 Acc: 0.3781\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1695 Acc: 0.4592\n",
      "val Loss: 0.1587 Acc: 0.3764\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 0.4676\n",
      "val Loss: 0.1568 Acc: 0.3853\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.4695\n",
      "val Loss: 0.1288 Acc: 0.3836\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1542 Acc: 0.4706\n",
      "val Loss: 0.1444 Acc: 0.3711\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1501 Acc: 0.4853\n",
      "val Loss: 0.1233 Acc: 0.3756\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1495 Acc: 0.4878\n",
      "val Loss: 0.1390 Acc: 0.3689\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1357 Acc: 0.4925\n",
      "val Loss: 0.1337 Acc: 0.3756\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1402 Acc: 0.4966\n",
      "val Loss: 0.1398 Acc: 0.3639\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1419 Acc: 0.5008\n",
      "val Loss: 0.1538 Acc: 0.3667\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1299 Acc: 0.5110\n",
      "val Loss: 0.1430 Acc: 0.3664\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1253 Acc: 0.5210\n",
      "val Loss: 0.1248 Acc: 0.3786\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1295 Acc: 0.5203\n",
      "val Loss: 0.1427 Acc: 0.3642\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1191 Acc: 0.5262\n",
      "val Loss: 0.1162 Acc: 0.3761\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1112 Acc: 0.5324\n",
      "val Loss: 0.1305 Acc: 0.3611\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1100 Acc: 0.5351\n",
      "val Loss: 0.1065 Acc: 0.3644\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1119 Acc: 0.5439\n",
      "val Loss: 0.1145 Acc: 0.3753\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1057 Acc: 0.5496\n",
      "val Loss: 0.1175 Acc: 0.3644\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.1031 Acc: 0.5521\n",
      "val Loss: 0.1259 Acc: 0.3675\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1055 Acc: 0.5581\n",
      "val Loss: 0.1285 Acc: 0.3586\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.1031 Acc: 0.5676\n",
      "val Loss: 0.1210 Acc: 0.3608\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.5735\n",
      "val Loss: 0.0916 Acc: 0.3700\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.1003 Acc: 0.5734\n",
      "val Loss: 0.1084 Acc: 0.3656\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0976 Acc: 0.5766\n",
      "val Loss: 0.1075 Acc: 0.3539\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0943 Acc: 0.5819\n",
      "val Loss: 0.1124 Acc: 0.3586\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0909 Acc: 0.5842\n",
      "val Loss: 0.1025 Acc: 0.3647\n",
      "\n",
      "Best val Acc: 0.403056\n",
      "Accuracy on test set using Importance Reweighting Method: 0.8663333333333333\n",
      "\n",
      "\n",
      "Training iteration: 3\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6797 Acc: 0.3703\n",
      "val Loss: 0.5096 Acc: 0.3633\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4953 Acc: 0.3830\n",
      "val Loss: 0.3884 Acc: 0.3900\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3787 Acc: 0.3889\n",
      "val Loss: 0.3492 Acc: 0.3986\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3774 Acc: 0.3906\n",
      "val Loss: 0.3038 Acc: 0.3981\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3268 Acc: 0.3914\n",
      "val Loss: 0.2564 Acc: 0.3958\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3148 Acc: 0.3922\n",
      "val Loss: 0.3247 Acc: 0.3997\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3061 Acc: 0.3947\n",
      "val Loss: 0.2489 Acc: 0.3961\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2824 Acc: 0.3942\n",
      "val Loss: 0.2602 Acc: 0.3919\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2715 Acc: 0.3933\n",
      "val Loss: 0.2440 Acc: 0.3972\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2446 Acc: 0.3984\n",
      "val Loss: 0.2458 Acc: 0.3978\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2632 Acc: 0.3967\n",
      "val Loss: 0.3456 Acc: 0.3803\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.3251 Acc: 0.3962\n",
      "val Loss: 0.2696 Acc: 0.3897\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2504 Acc: 0.3993\n",
      "val Loss: 0.2328 Acc: 0.3958\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2470 Acc: 0.3975\n",
      "val Loss: 0.1959 Acc: 0.3969\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2479 Acc: 0.4043\n",
      "val Loss: 0.2437 Acc: 0.3864\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2731 Acc: 0.3987\n",
      "val Loss: 0.2113 Acc: 0.3911\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2234 Acc: 0.4026\n",
      "val Loss: 0.2291 Acc: 0.3925\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2294 Acc: 0.4050\n",
      "val Loss: 0.2473 Acc: 0.3928\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2410 Acc: 0.4048\n",
      "val Loss: 0.2291 Acc: 0.3964\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2118 Acc: 0.4115\n",
      "val Loss: 0.2234 Acc: 0.3933\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2327 Acc: 0.4131\n",
      "val Loss: 0.1825 Acc: 0.4008\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2173 Acc: 0.4128\n",
      "val Loss: 0.1884 Acc: 0.3975\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2022 Acc: 0.4217\n",
      "val Loss: 0.1857 Acc: 0.3956\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1976 Acc: 0.4221\n",
      "val Loss: 0.1587 Acc: 0.3856\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1902 Acc: 0.4215\n",
      "val Loss: 0.1566 Acc: 0.3956\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1915 Acc: 0.4267\n",
      "val Loss: 0.2035 Acc: 0.3817\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1902 Acc: 0.4288\n",
      "val Loss: 0.2050 Acc: 0.3819\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1810 Acc: 0.4301\n",
      "val Loss: 0.1668 Acc: 0.3950\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1764 Acc: 0.4397\n",
      "val Loss: 0.1752 Acc: 0.3919\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1713 Acc: 0.4392\n",
      "val Loss: 0.1639 Acc: 0.3864\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1649 Acc: 0.4467\n",
      "val Loss: 0.1801 Acc: 0.3864\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.4520\n",
      "val Loss: 0.1599 Acc: 0.3822\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1667 Acc: 0.4597\n",
      "val Loss: 0.1970 Acc: 0.3728\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1683 Acc: 0.4581\n",
      "val Loss: 0.2013 Acc: 0.3703\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.4623\n",
      "val Loss: 0.1477 Acc: 0.3858\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1636 Acc: 0.4658\n",
      "val Loss: 0.1194 Acc: 0.3811\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1427 Acc: 0.4723\n",
      "val Loss: 0.1492 Acc: 0.3842\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1614 Acc: 0.4769\n",
      "val Loss: 0.1583 Acc: 0.3808\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1617 Acc: 0.4844\n",
      "val Loss: 0.1691 Acc: 0.3789\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1524 Acc: 0.4857\n",
      "val Loss: 0.1402 Acc: 0.3714\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1532 Acc: 0.4992\n",
      "val Loss: 0.1511 Acc: 0.3817\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1462 Acc: 0.5021\n",
      "val Loss: 0.1486 Acc: 0.3753\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1391 Acc: 0.5120\n",
      "val Loss: 0.1302 Acc: 0.3733\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1298 Acc: 0.5097\n",
      "val Loss: 0.1489 Acc: 0.3728\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1333 Acc: 0.5137\n",
      "val Loss: 0.1530 Acc: 0.3792\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.5257\n",
      "val Loss: 0.1112 Acc: 0.3750\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1308 Acc: 0.5306\n",
      "val Loss: 0.1403 Acc: 0.3753\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1209 Acc: 0.5377\n",
      "val Loss: 0.1445 Acc: 0.3694\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1209 Acc: 0.5472\n",
      "val Loss: 0.1240 Acc: 0.3764\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1175 Acc: 0.5453\n",
      "val Loss: 0.1418 Acc: 0.3633\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1141 Acc: 0.5584\n",
      "val Loss: 0.1183 Acc: 0.3731\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1063 Acc: 0.5616\n",
      "val Loss: 0.1441 Acc: 0.3672\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.1100 Acc: 0.5655\n",
      "val Loss: 0.1359 Acc: 0.3733\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1081 Acc: 0.5710\n",
      "val Loss: 0.1149 Acc: 0.3772\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.1003 Acc: 0.5817\n",
      "val Loss: 0.1160 Acc: 0.3669\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.5803\n",
      "val Loss: 0.1161 Acc: 0.3689\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.5885\n",
      "val Loss: 0.1054 Acc: 0.3733\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.5887\n",
      "val Loss: 0.1228 Acc: 0.3686\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0925 Acc: 0.6012\n",
      "val Loss: 0.1042 Acc: 0.3575\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0850 Acc: 0.6080\n",
      "val Loss: 0.1140 Acc: 0.3756\n",
      "\n",
      "Best val Acc: 0.400833\n",
      "Accuracy on test set using Importance Reweighting Method: 0.842\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.5738 Acc: 0.3758\n",
      "val Loss: 0.4961 Acc: 0.3925\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4552 Acc: 0.3814\n",
      "val Loss: 0.4097 Acc: 0.3975\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3940 Acc: 0.3892\n",
      "val Loss: 0.4438 Acc: 0.3781\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3355 Acc: 0.3893\n",
      "val Loss: 0.4002 Acc: 0.3939\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3194 Acc: 0.3925\n",
      "val Loss: 0.2390 Acc: 0.3986\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.2768 Acc: 0.3918\n",
      "val Loss: 0.2334 Acc: 0.3958\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.2400 Acc: 0.3953\n",
      "val Loss: 0.2427 Acc: 0.4000\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2393 Acc: 0.3953\n",
      "val Loss: 0.2548 Acc: 0.3972\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2578 Acc: 0.3967\n",
      "val Loss: 0.3288 Acc: 0.3783\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2443 Acc: 0.3979\n",
      "val Loss: 0.1895 Acc: 0.3983\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2137 Acc: 0.3981\n",
      "val Loss: 0.1894 Acc: 0.3978\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2213 Acc: 0.3965\n",
      "val Loss: 0.2170 Acc: 0.3972\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2173 Acc: 0.3999\n",
      "val Loss: 0.2270 Acc: 0.3922\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2021 Acc: 0.4003\n",
      "val Loss: 0.2160 Acc: 0.4008\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2013 Acc: 0.4001\n",
      "val Loss: 0.1596 Acc: 0.3944\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.1998 Acc: 0.4023\n",
      "val Loss: 0.1913 Acc: 0.4031\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.1955 Acc: 0.4045\n",
      "val Loss: 0.1590 Acc: 0.3964\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.1884 Acc: 0.4035\n",
      "val Loss: 0.1760 Acc: 0.4014\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.1809 Acc: 0.4078\n",
      "val Loss: 0.2056 Acc: 0.3956\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.1690 Acc: 0.4124\n",
      "val Loss: 0.1638 Acc: 0.3964\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.1613 Acc: 0.4117\n",
      "val Loss: 0.1574 Acc: 0.3964\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1639 Acc: 0.4123\n",
      "val Loss: 0.1579 Acc: 0.3958\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1722 Acc: 0.4124\n",
      "val Loss: 0.2228 Acc: 0.3747\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1621 Acc: 0.4146\n",
      "val Loss: 0.1407 Acc: 0.3942\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1550 Acc: 0.4156\n",
      "val Loss: 0.1574 Acc: 0.3964\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1498 Acc: 0.4202\n",
      "val Loss: 0.1299 Acc: 0.4072\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.4208\n",
      "val Loss: 0.1664 Acc: 0.3942\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1471 Acc: 0.4221\n",
      "val Loss: 0.1186 Acc: 0.3886\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.4259\n",
      "val Loss: 0.1519 Acc: 0.3953\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1546 Acc: 0.4269\n",
      "val Loss: 0.1456 Acc: 0.3986\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1369 Acc: 0.4280\n",
      "val Loss: 0.1422 Acc: 0.3847\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1393 Acc: 0.4323\n",
      "val Loss: 0.1655 Acc: 0.3925\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1370 Acc: 0.4318\n",
      "val Loss: 0.1456 Acc: 0.3808\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.4388\n",
      "val Loss: 0.1184 Acc: 0.3967\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.4376\n",
      "val Loss: 0.1195 Acc: 0.3931\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.4455\n",
      "val Loss: 0.1470 Acc: 0.3958\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1312 Acc: 0.4514\n",
      "val Loss: 0.1079 Acc: 0.3964\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1329 Acc: 0.4513\n",
      "val Loss: 0.1357 Acc: 0.3878\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1259 Acc: 0.4485\n",
      "val Loss: 0.1276 Acc: 0.3919\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1206 Acc: 0.4582\n",
      "val Loss: 0.1243 Acc: 0.3975\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1266 Acc: 0.4592\n",
      "val Loss: 0.1331 Acc: 0.3861\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1223 Acc: 0.4644\n",
      "val Loss: 0.1277 Acc: 0.3886\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1187 Acc: 0.4705\n",
      "val Loss: 0.1246 Acc: 0.3906\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1257 Acc: 0.4695\n",
      "val Loss: 0.1193 Acc: 0.3928\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1128 Acc: 0.4753\n",
      "val Loss: 0.1182 Acc: 0.3986\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1122 Acc: 0.4763\n",
      "val Loss: 0.1157 Acc: 0.3869\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1152 Acc: 0.4858\n",
      "val Loss: 0.1239 Acc: 0.3783\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1067 Acc: 0.4907\n",
      "val Loss: 0.0938 Acc: 0.3942\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1105 Acc: 0.4944\n",
      "val Loss: 0.1391 Acc: 0.3861\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.4958\n",
      "val Loss: 0.0845 Acc: 0.3961\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1058 Acc: 0.4996\n",
      "val Loss: 0.0965 Acc: 0.3883\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.1094 Acc: 0.5053\n",
      "val Loss: 0.0968 Acc: 0.3819\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.1064 Acc: 0.5142\n",
      "val Loss: 0.1076 Acc: 0.3839\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.1045 Acc: 0.5180\n",
      "val Loss: 0.1017 Acc: 0.3886\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.5240\n",
      "val Loss: 0.1414 Acc: 0.3758\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.1074 Acc: 0.5259\n",
      "val Loss: 0.1097 Acc: 0.3756\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.1029 Acc: 0.5323\n",
      "val Loss: 0.1123 Acc: 0.3761\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.1038 Acc: 0.5342\n",
      "val Loss: 0.1118 Acc: 0.3742\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.1037 Acc: 0.5339\n",
      "val Loss: 0.1107 Acc: 0.3711\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.1039 Acc: 0.5435\n",
      "val Loss: 0.1285 Acc: 0.3744\n",
      "\n",
      "Best val Acc: 0.407222\n",
      "Accuracy on test set using Importance Reweighting Method: 0.844\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.5982 Acc: 0.3697\n",
      "val Loss: 0.4563 Acc: 0.3836\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4687 Acc: 0.3895\n",
      "val Loss: 0.4300 Acc: 0.3844\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.4003 Acc: 0.3861\n",
      "val Loss: 0.3162 Acc: 0.3997\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3530 Acc: 0.3923\n",
      "val Loss: 0.4069 Acc: 0.3817\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3486 Acc: 0.3925\n",
      "val Loss: 0.3390 Acc: 0.3839\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3269 Acc: 0.3916\n",
      "val Loss: 0.3795 Acc: 0.3961\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3605 Acc: 0.3912\n",
      "val Loss: 0.3468 Acc: 0.3961\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2878 Acc: 0.3926\n",
      "val Loss: 0.3293 Acc: 0.3886\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2906 Acc: 0.3956\n",
      "val Loss: 0.3469 Acc: 0.3931\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2935 Acc: 0.3983\n",
      "val Loss: 0.2503 Acc: 0.3919\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2710 Acc: 0.3992\n",
      "val Loss: 0.2462 Acc: 0.3858\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2801 Acc: 0.4001\n",
      "val Loss: 0.2449 Acc: 0.3986\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2701 Acc: 0.4016\n",
      "val Loss: 0.2249 Acc: 0.3978\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2351 Acc: 0.4019\n",
      "val Loss: 0.2096 Acc: 0.3917\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2570 Acc: 0.4033\n",
      "val Loss: 0.2300 Acc: 0.3922\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2359 Acc: 0.4076\n",
      "val Loss: 0.1960 Acc: 0.3833\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2488 Acc: 0.4060\n",
      "val Loss: 0.2327 Acc: 0.3928\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2329 Acc: 0.4144\n",
      "val Loss: 0.2518 Acc: 0.3881\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.1952 Acc: 0.4160\n",
      "val Loss: 0.1804 Acc: 0.3903\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2375 Acc: 0.4239\n",
      "val Loss: 0.2071 Acc: 0.3881\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2222 Acc: 0.4237\n",
      "val Loss: 0.2101 Acc: 0.3869\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2094 Acc: 0.4249\n",
      "val Loss: 0.1771 Acc: 0.3861\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1895 Acc: 0.4363\n",
      "val Loss: 0.2110 Acc: 0.3931\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.2202 Acc: 0.4416\n",
      "val Loss: 0.2099 Acc: 0.3861\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2043 Acc: 0.4469\n",
      "val Loss: 0.1528 Acc: 0.3881\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.4512\n",
      "val Loss: 0.1970 Acc: 0.3856\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1957 Acc: 0.4566\n",
      "val Loss: 0.1958 Acc: 0.3800\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1930 Acc: 0.4658\n",
      "val Loss: 0.1958 Acc: 0.3747\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1951 Acc: 0.4698\n",
      "val Loss: 0.1824 Acc: 0.3742\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1946 Acc: 0.4744\n",
      "val Loss: 0.2086 Acc: 0.3678\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1783 Acc: 0.4885\n",
      "val Loss: 0.1619 Acc: 0.3764\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1722 Acc: 0.4947\n",
      "val Loss: 0.1425 Acc: 0.3756\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1659 Acc: 0.5035\n",
      "val Loss: 0.1451 Acc: 0.3731\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1531 Acc: 0.5060\n",
      "val Loss: 0.1440 Acc: 0.3669\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1510 Acc: 0.5147\n",
      "val Loss: 0.1484 Acc: 0.3753\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1491 Acc: 0.5144\n",
      "val Loss: 0.1606 Acc: 0.3650\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1428 Acc: 0.5286\n",
      "val Loss: 0.1405 Acc: 0.3731\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1405 Acc: 0.5348\n",
      "val Loss: 0.1413 Acc: 0.3692\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1345 Acc: 0.5442\n",
      "val Loss: 0.1365 Acc: 0.3731\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1300 Acc: 0.5503\n",
      "val Loss: 0.1293 Acc: 0.3711\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1233 Acc: 0.5563\n",
      "val Loss: 0.1328 Acc: 0.3667\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1136 Acc: 0.5608\n",
      "val Loss: 0.1351 Acc: 0.3692\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1137 Acc: 0.5656\n",
      "val Loss: 0.1146 Acc: 0.3778\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1093 Acc: 0.5672\n",
      "val Loss: 0.1406 Acc: 0.3611\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1099 Acc: 0.5785\n",
      "val Loss: 0.1119 Acc: 0.3769\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1001 Acc: 0.5826\n",
      "val Loss: 0.1194 Acc: 0.3603\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.5809\n",
      "val Loss: 0.1198 Acc: 0.3575\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0993 Acc: 0.5922\n",
      "val Loss: 0.1388 Acc: 0.3475\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1046 Acc: 0.5962\n",
      "val Loss: 0.1068 Acc: 0.3503\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0982 Acc: 0.5983\n",
      "val Loss: 0.1171 Acc: 0.3606\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0947 Acc: 0.6068\n",
      "val Loss: 0.1244 Acc: 0.3511\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0947 Acc: 0.6115\n",
      "val Loss: 0.1144 Acc: 0.3628\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0927 Acc: 0.6175\n",
      "val Loss: 0.0968 Acc: 0.3689\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0912 Acc: 0.6176\n",
      "val Loss: 0.1369 Acc: 0.3494\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0849 Acc: 0.6235\n",
      "val Loss: 0.0920 Acc: 0.3639\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0852 Acc: 0.6249\n",
      "val Loss: 0.1134 Acc: 0.3544\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0883 Acc: 0.6329\n",
      "val Loss: 0.1067 Acc: 0.3572\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.6359\n",
      "val Loss: 0.1051 Acc: 0.3600\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0850 Acc: 0.6368\n",
      "val Loss: 0.1034 Acc: 0.3581\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.6412\n",
      "val Loss: 0.1086 Acc: 0.3547\n",
      "\n",
      "Best val Acc: 0.399722\n",
      "Accuracy on test set using Importance Reweighting Method: 0.8366666666666667\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6896 Acc: 0.3663\n",
      "val Loss: 0.4385 Acc: 0.3642\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4614 Acc: 0.3861\n",
      "val Loss: 0.4604 Acc: 0.3889\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3588 Acc: 0.3908\n",
      "val Loss: 0.3171 Acc: 0.3886\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3349 Acc: 0.3895\n",
      "val Loss: 0.3229 Acc: 0.3914\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3222 Acc: 0.3927\n",
      "val Loss: 0.3424 Acc: 0.3917\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3155 Acc: 0.3917\n",
      "val Loss: 0.3483 Acc: 0.3956\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3414 Acc: 0.3924\n",
      "val Loss: 0.2648 Acc: 0.3975\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2856 Acc: 0.3924\n",
      "val Loss: 0.2851 Acc: 0.3925\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2688 Acc: 0.3981\n",
      "val Loss: 0.2432 Acc: 0.3967\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2541 Acc: 0.3971\n",
      "val Loss: 0.2060 Acc: 0.3975\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2385 Acc: 0.3978\n",
      "val Loss: 0.2144 Acc: 0.3969\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2491 Acc: 0.4001\n",
      "val Loss: 0.1942 Acc: 0.3989\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2226 Acc: 0.3993\n",
      "val Loss: 0.1943 Acc: 0.4000\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2273 Acc: 0.4029\n",
      "val Loss: 0.2111 Acc: 0.3850\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2125 Acc: 0.4060\n",
      "val Loss: 0.1810 Acc: 0.3928\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2266 Acc: 0.4098\n",
      "val Loss: 0.1589 Acc: 0.3922\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.1951 Acc: 0.4097\n",
      "val Loss: 0.1701 Acc: 0.3853\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.1993 Acc: 0.4147\n",
      "val Loss: 0.2449 Acc: 0.3900\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.1951 Acc: 0.4160\n",
      "val Loss: 0.1846 Acc: 0.3897\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.1994 Acc: 0.4167\n",
      "val Loss: 0.2156 Acc: 0.3839\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.1929 Acc: 0.4230\n",
      "val Loss: 0.1460 Acc: 0.3928\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1697 Acc: 0.4328\n",
      "val Loss: 0.1520 Acc: 0.3922\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.4358\n",
      "val Loss: 0.1674 Acc: 0.3881\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1634 Acc: 0.4379\n",
      "val Loss: 0.1649 Acc: 0.3825\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1602 Acc: 0.4367\n",
      "val Loss: 0.1142 Acc: 0.3942\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1605 Acc: 0.4467\n",
      "val Loss: 0.1489 Acc: 0.3878\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1481 Acc: 0.4540\n",
      "val Loss: 0.1685 Acc: 0.3836\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1487 Acc: 0.4598\n",
      "val Loss: 0.1219 Acc: 0.3811\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1376 Acc: 0.4683\n",
      "val Loss: 0.1690 Acc: 0.3650\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1469 Acc: 0.4672\n",
      "val Loss: 0.1382 Acc: 0.3736\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1469 Acc: 0.4731\n",
      "val Loss: 0.1667 Acc: 0.3706\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1490 Acc: 0.4819\n",
      "val Loss: 0.1620 Acc: 0.3822\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1392 Acc: 0.4893\n",
      "val Loss: 0.1404 Acc: 0.3772\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.4965\n",
      "val Loss: 0.1297 Acc: 0.3839\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1386 Acc: 0.4983\n",
      "val Loss: 0.1240 Acc: 0.3731\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1336 Acc: 0.5052\n",
      "val Loss: 0.1581 Acc: 0.3700\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1327 Acc: 0.5143\n",
      "val Loss: 0.1419 Acc: 0.3728\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.5260\n",
      "val Loss: 0.1276 Acc: 0.3725\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1194 Acc: 0.5296\n",
      "val Loss: 0.1086 Acc: 0.3764\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1203 Acc: 0.5362\n",
      "val Loss: 0.1337 Acc: 0.3736\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.5376\n",
      "val Loss: 0.1290 Acc: 0.3731\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.5446\n",
      "val Loss: 0.1104 Acc: 0.3508\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1119 Acc: 0.5503\n",
      "val Loss: 0.1112 Acc: 0.3772\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1037 Acc: 0.5585\n",
      "val Loss: 0.1390 Acc: 0.3647\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.5661\n",
      "val Loss: 0.1125 Acc: 0.3639\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1066 Acc: 0.5669\n",
      "val Loss: 0.1343 Acc: 0.3603\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1073 Acc: 0.5707\n",
      "val Loss: 0.1129 Acc: 0.3589\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.0933 Acc: 0.5862\n",
      "val Loss: 0.1152 Acc: 0.3700\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0929 Acc: 0.5943\n",
      "val Loss: 0.1233 Acc: 0.3581\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.5930\n",
      "val Loss: 0.1098 Acc: 0.3747\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0897 Acc: 0.5994\n",
      "val Loss: 0.1064 Acc: 0.3692\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0844 Acc: 0.6090\n",
      "val Loss: 0.0979 Acc: 0.3761\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0798 Acc: 0.6203\n",
      "val Loss: 0.1213 Acc: 0.3653\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0834 Acc: 0.6184\n",
      "val Loss: 0.1165 Acc: 0.3653\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0802 Acc: 0.6310\n",
      "val Loss: 0.0918 Acc: 0.3678\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.6324\n",
      "val Loss: 0.1089 Acc: 0.3558\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0739 Acc: 0.6388\n",
      "val Loss: 0.1225 Acc: 0.3583\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0756 Acc: 0.6373\n",
      "val Loss: 0.0978 Acc: 0.3719\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.6447\n",
      "val Loss: 0.1068 Acc: 0.3617\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0733 Acc: 0.6472\n",
      "val Loss: 0.1020 Acc: 0.3692\n",
      "\n",
      "Best val Acc: 0.400000\n",
      "Accuracy on test set using Importance Reweighting Method: 0.8486666666666667\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.6140 Acc: 0.3740\n",
      "val Loss: 0.4951 Acc: 0.3906\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4215 Acc: 0.3899\n",
      "val Loss: 0.3186 Acc: 0.4031\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3818 Acc: 0.3881\n",
      "val Loss: 0.3182 Acc: 0.3986\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3781 Acc: 0.3901\n",
      "val Loss: 0.4123 Acc: 0.3822\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3193 Acc: 0.3932\n",
      "val Loss: 0.2530 Acc: 0.3919\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3130 Acc: 0.3929\n",
      "val Loss: 0.2538 Acc: 0.3903\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3128 Acc: 0.3933\n",
      "val Loss: 0.2895 Acc: 0.3911\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.3222 Acc: 0.3953\n",
      "val Loss: 0.2508 Acc: 0.3961\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.3473 Acc: 0.3910\n",
      "val Loss: 0.3374 Acc: 0.3869\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.3073 Acc: 0.3973\n",
      "val Loss: 0.2634 Acc: 0.3928\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2670 Acc: 0.4017\n",
      "val Loss: 0.2849 Acc: 0.3928\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2804 Acc: 0.3974\n",
      "val Loss: 0.2653 Acc: 0.3981\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2823 Acc: 0.4028\n",
      "val Loss: 0.3315 Acc: 0.3875\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2771 Acc: 0.4065\n",
      "val Loss: 0.2167 Acc: 0.3864\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2184 Acc: 0.4131\n",
      "val Loss: 0.2319 Acc: 0.3936\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2268 Acc: 0.4193\n",
      "val Loss: 0.2143 Acc: 0.3883\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2004 Acc: 0.4292\n",
      "val Loss: 0.1965 Acc: 0.3875\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.1968 Acc: 0.4283\n",
      "val Loss: 0.1976 Acc: 0.3958\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.1950 Acc: 0.4335\n",
      "val Loss: 0.2416 Acc: 0.3783\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.1899 Acc: 0.4389\n",
      "val Loss: 0.2313 Acc: 0.3725\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.1799 Acc: 0.4400\n",
      "val Loss: 0.1979 Acc: 0.3778\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.1767 Acc: 0.4447\n",
      "val Loss: 0.2021 Acc: 0.3811\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.4567\n",
      "val Loss: 0.1740 Acc: 0.3733\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1636 Acc: 0.4559\n",
      "val Loss: 0.1637 Acc: 0.3778\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1661 Acc: 0.4641\n",
      "val Loss: 0.1552 Acc: 0.3811\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1680 Acc: 0.4681\n",
      "val Loss: 0.1520 Acc: 0.3783\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1522 Acc: 0.4798\n",
      "val Loss: 0.1587 Acc: 0.3747\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1514 Acc: 0.4783\n",
      "val Loss: 0.1645 Acc: 0.3853\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1588 Acc: 0.4845\n",
      "val Loss: 0.1353 Acc: 0.3864\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1509 Acc: 0.4904\n",
      "val Loss: 0.1146 Acc: 0.3844\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1375 Acc: 0.4908\n",
      "val Loss: 0.1483 Acc: 0.3706\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1402 Acc: 0.4967\n",
      "val Loss: 0.1507 Acc: 0.3736\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1446 Acc: 0.4964\n",
      "val Loss: 0.1462 Acc: 0.3719\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1415 Acc: 0.5099\n",
      "val Loss: 0.1195 Acc: 0.3836\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.5160\n",
      "val Loss: 0.1401 Acc: 0.3794\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.5231\n",
      "val Loss: 0.1341 Acc: 0.3836\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1287 Acc: 0.5326\n",
      "val Loss: 0.1264 Acc: 0.3772\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1216 Acc: 0.5387\n",
      "val Loss: 0.1535 Acc: 0.3775\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1213 Acc: 0.5413\n",
      "val Loss: 0.1295 Acc: 0.3756\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.5483\n",
      "val Loss: 0.1281 Acc: 0.3717\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.5497\n",
      "val Loss: 0.1348 Acc: 0.3756\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.5567\n",
      "val Loss: 0.1286 Acc: 0.3711\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1133 Acc: 0.5537\n",
      "val Loss: 0.1062 Acc: 0.3769\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1101 Acc: 0.5684\n",
      "val Loss: 0.1105 Acc: 0.3811\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1037 Acc: 0.5712\n",
      "val Loss: 0.1150 Acc: 0.3792\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1056 Acc: 0.5817\n",
      "val Loss: 0.1403 Acc: 0.3706\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1084 Acc: 0.5847\n",
      "val Loss: 0.1176 Acc: 0.3731\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1050 Acc: 0.5877\n",
      "val Loss: 0.1169 Acc: 0.3650\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1003 Acc: 0.5985\n",
      "val Loss: 0.1221 Acc: 0.3647\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1002 Acc: 0.6005\n",
      "val Loss: 0.1179 Acc: 0.3781\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0996 Acc: 0.6008\n",
      "val Loss: 0.1141 Acc: 0.3708\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0974 Acc: 0.6044\n",
      "val Loss: 0.1201 Acc: 0.3656\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0915 Acc: 0.6191\n",
      "val Loss: 0.0956 Acc: 0.3731\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0844 Acc: 0.6258\n",
      "val Loss: 0.1044 Acc: 0.3703\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0841 Acc: 0.6297\n",
      "val Loss: 0.0964 Acc: 0.3661\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0814 Acc: 0.6315\n",
      "val Loss: 0.0960 Acc: 0.3769\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0807 Acc: 0.6352\n",
      "val Loss: 0.1054 Acc: 0.3711\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0801 Acc: 0.6431\n",
      "val Loss: 0.0914 Acc: 0.3700\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0811 Acc: 0.6430\n",
      "val Loss: 0.1041 Acc: 0.3675\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0877 Acc: 0.6412\n",
      "val Loss: 0.1214 Acc: 0.3697\n",
      "\n",
      "Best val Acc: 0.403056\n",
      "Accuracy on test set using Importance Reweighting Method: 0.858\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.3818\n",
      "val Loss: 0.4181 Acc: 0.3833\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.3994 Acc: 0.3892\n",
      "val Loss: 0.5281 Acc: 0.3889\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3968 Acc: 0.3871\n",
      "val Loss: 0.3131 Acc: 0.3956\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3567 Acc: 0.3900\n",
      "val Loss: 0.3176 Acc: 0.3972\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3149 Acc: 0.3923\n",
      "val Loss: 0.2924 Acc: 0.3956\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3409 Acc: 0.3922\n",
      "val Loss: 0.2861 Acc: 0.3981\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3151 Acc: 0.3941\n",
      "val Loss: 0.3189 Acc: 0.3975\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.3965\n",
      "val Loss: 0.2358 Acc: 0.3994\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.2549 Acc: 0.3978\n",
      "val Loss: 0.2123 Acc: 0.3928\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2882 Acc: 0.3965\n",
      "val Loss: 0.2758 Acc: 0.3756\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2667 Acc: 0.4013\n",
      "val Loss: 0.2140 Acc: 0.4000\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2547 Acc: 0.3996\n",
      "val Loss: 0.2685 Acc: 0.3806\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2558 Acc: 0.4067\n",
      "val Loss: 0.1932 Acc: 0.3956\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2390 Acc: 0.4029\n",
      "val Loss: 0.1944 Acc: 0.3922\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2286 Acc: 0.4067\n",
      "val Loss: 0.2659 Acc: 0.3847\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2456 Acc: 0.4139\n",
      "val Loss: 0.2379 Acc: 0.3828\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2279 Acc: 0.4167\n",
      "val Loss: 0.2026 Acc: 0.3922\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2331 Acc: 0.4186\n",
      "val Loss: 0.2415 Acc: 0.3808\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2229 Acc: 0.4195\n",
      "val Loss: 0.2430 Acc: 0.3881\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2275 Acc: 0.4276\n",
      "val Loss: 0.1940 Acc: 0.3944\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2018 Acc: 0.4342\n",
      "val Loss: 0.2366 Acc: 0.3711\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2058 Acc: 0.4338\n",
      "val Loss: 0.1962 Acc: 0.3953\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2109 Acc: 0.4422\n",
      "val Loss: 0.2084 Acc: 0.3803\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.1943 Acc: 0.4476\n",
      "val Loss: 0.2012 Acc: 0.3814\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.1879 Acc: 0.4533\n",
      "val Loss: 0.1968 Acc: 0.3850\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.1873 Acc: 0.4537\n",
      "val Loss: 0.1808 Acc: 0.3747\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.1674 Acc: 0.4622\n",
      "val Loss: 0.2071 Acc: 0.3736\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1756 Acc: 0.4715\n",
      "val Loss: 0.1716 Acc: 0.3794\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1681 Acc: 0.4774\n",
      "val Loss: 0.1673 Acc: 0.3792\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1660 Acc: 0.4766\n",
      "val Loss: 0.1511 Acc: 0.3831\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1660 Acc: 0.4890\n",
      "val Loss: 0.1558 Acc: 0.3819\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1463 Acc: 0.4954\n",
      "val Loss: 0.1717 Acc: 0.3672\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1488 Acc: 0.5044\n",
      "val Loss: 0.1482 Acc: 0.3681\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1453 Acc: 0.5051\n",
      "val Loss: 0.1563 Acc: 0.3672\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1480 Acc: 0.5136\n",
      "val Loss: 0.1436 Acc: 0.3786\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1364 Acc: 0.5128\n",
      "val Loss: 0.1397 Acc: 0.3672\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1289 Acc: 0.5247\n",
      "val Loss: 0.1380 Acc: 0.3733\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1385 Acc: 0.5288\n",
      "val Loss: 0.1358 Acc: 0.3769\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1309 Acc: 0.5369\n",
      "val Loss: 0.1410 Acc: 0.3744\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1255 Acc: 0.5455\n",
      "val Loss: 0.1094 Acc: 0.3769\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1168 Acc: 0.5556\n",
      "val Loss: 0.1294 Acc: 0.3633\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1209 Acc: 0.5543\n",
      "val Loss: 0.1267 Acc: 0.3622\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1163 Acc: 0.5649\n",
      "val Loss: 0.1249 Acc: 0.3650\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1059 Acc: 0.5719\n",
      "val Loss: 0.1082 Acc: 0.3658\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1096 Acc: 0.5788\n",
      "val Loss: 0.1228 Acc: 0.3683\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.5781\n",
      "val Loss: 0.1247 Acc: 0.3625\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1182 Acc: 0.5866\n",
      "val Loss: 0.1116 Acc: 0.3589\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1054 Acc: 0.5975\n",
      "val Loss: 0.1197 Acc: 0.3597\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.1040 Acc: 0.6005\n",
      "val Loss: 0.1513 Acc: 0.3539\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.1028 Acc: 0.6026\n",
      "val Loss: 0.1400 Acc: 0.3494\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.0997 Acc: 0.6077\n",
      "val Loss: 0.1286 Acc: 0.3581\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0985 Acc: 0.6156\n",
      "val Loss: 0.1244 Acc: 0.3478\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "train Loss: 0.0941 Acc: 0.6248\n",
      "val Loss: 0.1413 Acc: 0.3531\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "train Loss: 0.0930 Acc: 0.6285\n",
      "val Loss: 0.1142 Acc: 0.3533\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "train Loss: 0.0890 Acc: 0.6344\n",
      "val Loss: 0.1172 Acc: 0.3517\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "train Loss: 0.0896 Acc: 0.6365\n",
      "val Loss: 0.1109 Acc: 0.3494\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "train Loss: 0.0859 Acc: 0.6441\n",
      "val Loss: 0.1254 Acc: 0.3581\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "train Loss: 0.0914 Acc: 0.6443\n",
      "val Loss: 0.1193 Acc: 0.3478\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.6480\n",
      "val Loss: 0.0995 Acc: 0.3664\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "train Loss: 0.0850 Acc: 0.6540\n",
      "val Loss: 0.1166 Acc: 0.3447\n",
      "\n",
      "Best val Acc: 0.400000\n",
      "Accuracy on test set using Importance Reweighting Method: 0.8673333333333333\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Epoch 0/59\n",
      "----------\n",
      "train Loss: 0.5735 Acc: 0.3750\n",
      "val Loss: 0.5464 Acc: 0.3878\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "train Loss: 0.4559 Acc: 0.3890\n",
      "val Loss: 0.3402 Acc: 0.4008\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "train Loss: 0.3579 Acc: 0.3906\n",
      "val Loss: 0.3181 Acc: 0.3981\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "train Loss: 0.3855 Acc: 0.3858\n",
      "val Loss: 0.3843 Acc: 0.3992\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "train Loss: 0.3331 Acc: 0.3913\n",
      "val Loss: 0.3247 Acc: 0.3950\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "train Loss: 0.3357 Acc: 0.3917\n",
      "val Loss: 0.3733 Acc: 0.4014\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "train Loss: 0.3413 Acc: 0.3934\n",
      "val Loss: 0.3175 Acc: 0.3992\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "train Loss: 0.3048 Acc: 0.3976\n",
      "val Loss: 0.3623 Acc: 0.3917\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "train Loss: 0.3284 Acc: 0.3940\n",
      "val Loss: 0.3034 Acc: 0.3908\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "train Loss: 0.2977 Acc: 0.3963\n",
      "val Loss: 0.2998 Acc: 0.3850\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "train Loss: 0.2882 Acc: 0.4012\n",
      "val Loss: 0.2397 Acc: 0.3969\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "train Loss: 0.2613 Acc: 0.4014\n",
      "val Loss: 0.2573 Acc: 0.3942\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "train Loss: 0.2757 Acc: 0.4027\n",
      "val Loss: 0.3021 Acc: 0.3797\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "train Loss: 0.2800 Acc: 0.4033\n",
      "val Loss: 0.2548 Acc: 0.3903\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "train Loss: 0.2837 Acc: 0.4086\n",
      "val Loss: 0.2812 Acc: 0.3956\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "train Loss: 0.2960 Acc: 0.4135\n",
      "val Loss: 0.2472 Acc: 0.3908\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "train Loss: 0.2558 Acc: 0.4156\n",
      "val Loss: 0.2021 Acc: 0.3981\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "train Loss: 0.2419 Acc: 0.4183\n",
      "val Loss: 0.2208 Acc: 0.3906\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "train Loss: 0.2411 Acc: 0.4256\n",
      "val Loss: 0.2066 Acc: 0.3881\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "train Loss: 0.2489 Acc: 0.4306\n",
      "val Loss: 0.2325 Acc: 0.3861\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "train Loss: 0.2305 Acc: 0.4342\n",
      "val Loss: 0.2750 Acc: 0.3828\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "train Loss: 0.2328 Acc: 0.4403\n",
      "val Loss: 0.2454 Acc: 0.3758\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "train Loss: 0.2185 Acc: 0.4456\n",
      "val Loss: 0.2247 Acc: 0.3822\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.4554\n",
      "val Loss: 0.2108 Acc: 0.3767\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "train Loss: 0.2140 Acc: 0.4574\n",
      "val Loss: 0.2270 Acc: 0.3761\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "train Loss: 0.2055 Acc: 0.4647\n",
      "val Loss: 0.1741 Acc: 0.3839\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "train Loss: 0.2073 Acc: 0.4676\n",
      "val Loss: 0.2008 Acc: 0.3800\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "train Loss: 0.1959 Acc: 0.4767\n",
      "val Loss: 0.1950 Acc: 0.3725\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "train Loss: 0.1863 Acc: 0.4841\n",
      "val Loss: 0.1654 Acc: 0.3761\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "train Loss: 0.1775 Acc: 0.4858\n",
      "val Loss: 0.1795 Acc: 0.3742\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "train Loss: 0.1712 Acc: 0.4903\n",
      "val Loss: 0.1579 Acc: 0.3750\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "train Loss: 0.1737 Acc: 0.4944\n",
      "val Loss: 0.1658 Acc: 0.3697\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "train Loss: 0.1715 Acc: 0.5020\n",
      "val Loss: 0.1554 Acc: 0.3736\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "train Loss: 0.1610 Acc: 0.5083\n",
      "val Loss: 0.1621 Acc: 0.3789\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "train Loss: 0.1617 Acc: 0.5118\n",
      "val Loss: 0.1727 Acc: 0.3656\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.5228\n",
      "val Loss: 0.1724 Acc: 0.3800\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "train Loss: 0.1486 Acc: 0.5201\n",
      "val Loss: 0.1487 Acc: 0.3697\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.5333\n",
      "val Loss: 0.1347 Acc: 0.3794\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "train Loss: 0.1365 Acc: 0.5344\n",
      "val Loss: 0.1581 Acc: 0.3653\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "train Loss: 0.1331 Acc: 0.5507\n",
      "val Loss: 0.1359 Acc: 0.3797\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "train Loss: 0.1270 Acc: 0.5528\n",
      "val Loss: 0.1199 Acc: 0.3778\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "train Loss: 0.1237 Acc: 0.5531\n",
      "val Loss: 0.1606 Acc: 0.3597\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "train Loss: 0.1291 Acc: 0.5549\n",
      "val Loss: 0.1440 Acc: 0.3631\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n",
      "train Loss: 0.1197 Acc: 0.5691\n",
      "val Loss: 0.1241 Acc: 0.3714\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "train Loss: 0.1195 Acc: 0.5760\n",
      "val Loss: 0.1250 Acc: 0.3725\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.5790\n",
      "val Loss: 0.1257 Acc: 0.3719\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "train Loss: 0.1059 Acc: 0.5835\n",
      "val Loss: 0.1139 Acc: 0.3717\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "train Loss: 0.1122 Acc: 0.5867\n",
      "val Loss: 0.1136 Acc: 0.3700\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "train Loss: 0.0995 Acc: 0.5929\n",
      "val Loss: 0.1360 Acc: 0.3744\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "train Loss: 0.0986 Acc: 0.6015\n",
      "val Loss: 0.1282 Acc: 0.3639\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.5984\n",
      "val Loss: 0.1155 Acc: 0.3697\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "train Loss: 0.0960 Acc: 0.6067\n",
      "val Loss: 0.1114 Acc: 0.3600\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "# Training the model n_iters\n",
    "n_iters = 10\n",
    "\n",
    "# Set seed\n",
    "seed_torch()\n",
    "\n",
    "test_acc_impreweighting_results = {'cifar': [],\n",
    "                                   'fashionmnist0.5': [],\n",
    "                                   'fashionmnist0.6': []}\n",
    "\n",
    "# Iterate over each dataset\n",
    "for d in list(available_datasets.keys()):\n",
    "  dataset = d\n",
    "\n",
    "  # Load dataset\n",
    "  if dataset.lower() in available_datasets:\n",
    "    dir = available_datasets[dataset.lower()]\n",
    "    T_matrix = T_matrices[dataset.lower()]\n",
    "    X_train, y_train, X_test, y_test = load_data(dir)\n",
    "    print('-'*60)\n",
    "    print('-'*60)\n",
    "    print(f\"\\nDataset {dataset} loaded.\\n\")\n",
    "\n",
    "    print(f\"Shape Xtr: {X_train.shape}\")\n",
    "    print(f\"Shape Str: {y_train.shape}\")\n",
    "    print(f\"Shape Xts: {X_test.shape}\")\n",
    "    print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "    print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "    print('.'*30)\n",
    "  else:\n",
    "    print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")\n",
    "\n",
    "  # Detect if GPU or CPU\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "      \n",
    "  # Define parameters\n",
    "  if dataset.lower() == 'cifar':\n",
    "    n_channels = 3\n",
    "    n_filters = 8\n",
    "  else:\n",
    "    n_channels = 1\n",
    "    n_filters = 7\n",
    "\n",
    "  num_classes = 3\n",
    "  batch_size = 100\n",
    "  num_epochs = 60\n",
    "  learning_rate = 0.001\n",
    "  \n",
    "  for n in range(n_iters):\n",
    "    print(f\"\\nTraining iteration: {n}\")\n",
    "\n",
    "    # Clean cache each iteration\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # We did not set a seed for train_test_split thus, it would generate different samples each iteration\n",
    "    train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                    y_train=y_train, \n",
    "                                                    X_test=X_test, \n",
    "                                                    y_test=y_test, \n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "\n",
    "    # Initialize model\n",
    "    model_modified_resnet = ResNet(ResidualBlock, [2, 2, 2], \n",
    "                                   num_channels=n_channels, \n",
    "                                   num_filter=n_filters, \n",
    "                                   num_classes=num_classes).to(device)\n",
    "\n",
    "    # If GPU: send the model to GPU\n",
    "    model_modified_resnet = model_modified_resnet.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model_modified_resnet.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define Loss Function\n",
    "    criterion = ReweightingLoss()\n",
    "\n",
    "    # Train model - Revision FALSE\n",
    "    model_modified_resnet, hist = train_model_forward(model_modified_resnet, \n",
    "                                                      dataloaders_dict, \n",
    "                                                      criterion, \n",
    "                                                      optimizer, \n",
    "                                                      T_matrix, \n",
    "                                                      with_revision=False, \n",
    "                                                      num_epochs=num_epochs)\n",
    "\n",
    "    # Generate predictions on the test set\n",
    "    y_true, y_pred, acc, outputs = prediction(model_modified_resnet, test_loader, device)\n",
    "\n",
    "    test_acc_impreweighting_results[dataset].append(acc)\n",
    "    print(f\"Accuracy on test set using Importance Reweighting Method: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjqRcwNYMrlr"
   },
   "source": [
    "#### Results\n",
    "\n",
    "As we did previously, we can print the dictionary `test_acc_impreweighting_results` which has stored the results of the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXGN1FQpNDU5"
   },
   "outputs": [],
   "source": [
    "# Copy values\n",
    "test_acc_impreweighting_results_backup = test_acc_impreweighting_results \n",
    "\n",
    "# Create dataframe from dictionary\n",
    "df = pd.DataFrame(test_acc_impreweighting_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Fkk9kVpt4Hm"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "import pickle\n",
    "with open('test_acc_impreweighting_results.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_acc_impreweighting_results, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Adp6JqWt597"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('test_acc_impreweighting_results.pkl', 'rb') as fp:\n",
    "    test_load = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPNhb86rNJcj"
   },
   "outputs": [],
   "source": [
    "mean, std = process_results(test_acc_impreweighting_results, title=\"Importance Reweighting Method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Riia1Z1QD_w"
   },
   "source": [
    "### 4.4.- Baseline Results\n",
    "\n",
    "In this section, we train the ResNet model without any noisy label method and we will generate graphs to compare this baseline result with the results obtained in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24dniwdJIsRv"
   },
   "source": [
    "#### Generate Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64095,
     "status": "ok",
     "timestamp": 1605622494246,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "GOADDJC2F-P3",
    "outputId": "678b14fb-681d-4032-da2d-b8eab34f0226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset cifar loaded.\n",
      "\n",
      "Shape Xtr: (15000, 3, 32, 32)\n",
      "Shape Str: (15000,)\n",
      "Shape Xts: (3000, 3, 32, 32)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "None\n",
      "..............................\n",
      "\n",
      "Training iteration: 0\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1058 Acc: 0.3559\n",
      "val Loss: 1.1005 Acc: 0.3823\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0927 Acc: 0.3801\n",
      "val Loss: 1.0991 Acc: 0.3507\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0892 Acc: 0.3840\n",
      "val Loss: 1.1067 Acc: 0.3760\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0885 Acc: 0.3829\n",
      "val Loss: 1.0973 Acc: 0.3603\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0854 Acc: 0.3930\n",
      "val Loss: 1.0965 Acc: 0.3750\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0800 Acc: 0.4037\n",
      "val Loss: 1.1007 Acc: 0.3763\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0749 Acc: 0.4172\n",
      "val Loss: 1.0978 Acc: 0.3673\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0664 Acc: 0.4314\n",
      "val Loss: 1.1106 Acc: 0.3627\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0627 Acc: 0.4327\n",
      "val Loss: 1.1051 Acc: 0.3697\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0497 Acc: 0.4513\n",
      "val Loss: 1.1267 Acc: 0.3610\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0288 Acc: 0.4736\n",
      "val Loss: 1.1512 Acc: 0.3580\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0065 Acc: 0.4962\n",
      "val Loss: 1.1599 Acc: 0.3403\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9607 Acc: 0.5373\n",
      "val Loss: 1.2543 Acc: 0.3600\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9015 Acc: 0.5772\n",
      "val Loss: 1.4057 Acc: 0.3443\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8096 Acc: 0.6415\n",
      "val Loss: 1.4051 Acc: 0.3323\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.6967 Acc: 0.7013\n",
      "val Loss: 1.8594 Acc: 0.3560\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.5676 Acc: 0.7662\n",
      "val Loss: 2.0649 Acc: 0.3490\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.4250 Acc: 0.8367\n",
      "val Loss: 2.4853 Acc: 0.3427\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.3200 Acc: 0.8819\n",
      "val Loss: 2.8103 Acc: 0.3373\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.2278 Acc: 0.9183\n",
      "val Loss: 2.4456 Acc: 0.3447\n",
      "\n",
      "Best val Acc: 0.382333\n",
      "Accuracy on test set - Baseline: 0.5226666666666666\n",
      "\n",
      "\n",
      "Training iteration: 1\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1076 Acc: 0.3538\n",
      "val Loss: 1.1062 Acc: 0.3493\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0948 Acc: 0.3748\n",
      "val Loss: 1.0960 Acc: 0.3827\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0924 Acc: 0.3741\n",
      "val Loss: 1.0957 Acc: 0.3570\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0887 Acc: 0.3830\n",
      "val Loss: 1.0933 Acc: 0.3637\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0851 Acc: 0.3938\n",
      "val Loss: 1.0930 Acc: 0.3783\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0821 Acc: 0.3982\n",
      "val Loss: 1.1020 Acc: 0.3570\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0766 Acc: 0.4068\n",
      "val Loss: 1.1138 Acc: 0.3487\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0726 Acc: 0.4202\n",
      "val Loss: 1.1025 Acc: 0.3577\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0630 Acc: 0.4267\n",
      "val Loss: 1.1235 Acc: 0.3507\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0520 Acc: 0.4462\n",
      "val Loss: 1.1425 Acc: 0.3433\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0337 Acc: 0.4694\n",
      "val Loss: 1.1848 Acc: 0.3437\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0082 Acc: 0.4900\n",
      "val Loss: 1.1847 Acc: 0.3587\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9703 Acc: 0.5297\n",
      "val Loss: 1.5049 Acc: 0.3273\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9181 Acc: 0.5637\n",
      "val Loss: 1.2782 Acc: 0.3507\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8410 Acc: 0.6147\n",
      "val Loss: 1.5554 Acc: 0.3427\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.7388 Acc: 0.6770\n",
      "val Loss: 2.0539 Acc: 0.3467\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.6329 Acc: 0.7385\n",
      "val Loss: 1.7494 Acc: 0.3340\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.4996 Acc: 0.7987\n",
      "val Loss: 1.9379 Acc: 0.3570\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.3812 Acc: 0.8507\n",
      "val Loss: 2.2235 Acc: 0.3340\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.2872 Acc: 0.8944\n",
      "val Loss: 2.3644 Acc: 0.3337\n",
      "\n",
      "Best val Acc: 0.382667\n",
      "Accuracy on test set - Baseline: 0.5936666666666667\n",
      "\n",
      "\n",
      "Training iteration: 2\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1064 Acc: 0.3573\n",
      "val Loss: 1.1126 Acc: 0.3553\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0962 Acc: 0.3658\n",
      "val Loss: 1.0990 Acc: 0.3503\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0918 Acc: 0.3790\n",
      "val Loss: 1.0922 Acc: 0.3700\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0898 Acc: 0.3889\n",
      "val Loss: 1.0941 Acc: 0.3627\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0869 Acc: 0.3871\n",
      "val Loss: 1.0945 Acc: 0.3710\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0855 Acc: 0.3917\n",
      "val Loss: 1.1206 Acc: 0.3507\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0822 Acc: 0.3977\n",
      "val Loss: 1.1155 Acc: 0.3387\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0773 Acc: 0.4095\n",
      "val Loss: 1.0946 Acc: 0.3720\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0722 Acc: 0.4214\n",
      "val Loss: 1.1159 Acc: 0.3580\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0631 Acc: 0.4293\n",
      "val Loss: 1.1068 Acc: 0.3670\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0556 Acc: 0.4400\n",
      "val Loss: 1.1105 Acc: 0.3620\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0387 Acc: 0.4652\n",
      "val Loss: 1.1375 Acc: 0.3533\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0219 Acc: 0.4791\n",
      "val Loss: 1.1309 Acc: 0.3570\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9908 Acc: 0.5087\n",
      "val Loss: 1.1961 Acc: 0.3433\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9611 Acc: 0.5313\n",
      "val Loss: 1.2904 Acc: 0.3517\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9098 Acc: 0.5657\n",
      "val Loss: 1.2790 Acc: 0.3337\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8416 Acc: 0.6140\n",
      "val Loss: 1.3472 Acc: 0.3487\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7714 Acc: 0.6560\n",
      "val Loss: 1.4223 Acc: 0.3523\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6895 Acc: 0.7016\n",
      "val Loss: 1.6376 Acc: 0.3513\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6007 Acc: 0.7453\n",
      "val Loss: 1.8290 Acc: 0.3477\n",
      "\n",
      "Best val Acc: 0.372000\n",
      "Accuracy on test set - Baseline: 0.583\n",
      "\n",
      "\n",
      "Training iteration: 3\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1049 Acc: 0.3546\n",
      "val Loss: 1.1010 Acc: 0.3833\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0944 Acc: 0.3741\n",
      "val Loss: 1.0954 Acc: 0.3587\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0894 Acc: 0.3778\n",
      "val Loss: 1.1055 Acc: 0.3283\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0871 Acc: 0.3871\n",
      "val Loss: 1.0936 Acc: 0.3777\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0844 Acc: 0.3905\n",
      "val Loss: 1.0978 Acc: 0.3757\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0804 Acc: 0.4007\n",
      "val Loss: 1.1255 Acc: 0.3323\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0746 Acc: 0.4143\n",
      "val Loss: 1.1023 Acc: 0.3637\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0653 Acc: 0.4264\n",
      "val Loss: 1.1209 Acc: 0.3743\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0538 Acc: 0.4422\n",
      "val Loss: 1.1466 Acc: 0.3503\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0384 Acc: 0.4617\n",
      "val Loss: 1.1655 Acc: 0.3363\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0092 Acc: 0.4962\n",
      "val Loss: 1.1690 Acc: 0.3587\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9688 Acc: 0.5205\n",
      "val Loss: 1.1947 Acc: 0.3490\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.8979 Acc: 0.5810\n",
      "val Loss: 1.3118 Acc: 0.3513\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.8156 Acc: 0.6376\n",
      "val Loss: 1.3521 Acc: 0.3420\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.6954 Acc: 0.7035\n",
      "val Loss: 1.6549 Acc: 0.3440\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.5534 Acc: 0.7754\n",
      "val Loss: 2.3253 Acc: 0.3457\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.4294 Acc: 0.8347\n",
      "val Loss: 2.1390 Acc: 0.3363\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.3055 Acc: 0.8897\n",
      "val Loss: 2.3051 Acc: 0.3313\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.2282 Acc: 0.9186\n",
      "val Loss: 3.1054 Acc: 0.3293\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.1388 Acc: 0.9547\n",
      "val Loss: 3.4008 Acc: 0.3333\n",
      "\n",
      "Best val Acc: 0.383333\n",
      "Accuracy on test set - Baseline: 0.5123333333333333\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1070 Acc: 0.3523\n",
      "val Loss: 1.1019 Acc: 0.3690\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0975 Acc: 0.3616\n",
      "val Loss: 1.1126 Acc: 0.3363\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0906 Acc: 0.3786\n",
      "val Loss: 1.0916 Acc: 0.3750\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0865 Acc: 0.3934\n",
      "val Loss: 1.0928 Acc: 0.3853\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0839 Acc: 0.3952\n",
      "val Loss: 1.1095 Acc: 0.3580\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0806 Acc: 0.4016\n",
      "val Loss: 1.1004 Acc: 0.3513\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0772 Acc: 0.4002\n",
      "val Loss: 1.0944 Acc: 0.3743\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0727 Acc: 0.4145\n",
      "val Loss: 1.1070 Acc: 0.3543\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0622 Acc: 0.4321\n",
      "val Loss: 1.0963 Acc: 0.3807\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0520 Acc: 0.4445\n",
      "val Loss: 1.1267 Acc: 0.3650\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0376 Acc: 0.4620\n",
      "val Loss: 1.1358 Acc: 0.3677\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0090 Acc: 0.4872\n",
      "val Loss: 1.1533 Acc: 0.3687\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9787 Acc: 0.5199\n",
      "val Loss: 1.2887 Acc: 0.3583\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9280 Acc: 0.5593\n",
      "val Loss: 1.3820 Acc: 0.3380\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8613 Acc: 0.6024\n",
      "val Loss: 1.3841 Acc: 0.3773\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.7767 Acc: 0.6555\n",
      "val Loss: 1.5761 Acc: 0.3547\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.6832 Acc: 0.7053\n",
      "val Loss: 1.4809 Acc: 0.3457\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.5661 Acc: 0.7655\n",
      "val Loss: 2.0526 Acc: 0.3423\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.4632 Acc: 0.8160\n",
      "val Loss: 2.1350 Acc: 0.3637\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.3361 Acc: 0.8715\n",
      "val Loss: 2.7379 Acc: 0.3523\n",
      "\n",
      "Best val Acc: 0.385333\n",
      "Accuracy on test set - Baseline: 0.49766666666666665\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1074 Acc: 0.3508\n",
      "val Loss: 1.1029 Acc: 0.3453\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0968 Acc: 0.3696\n",
      "val Loss: 1.0966 Acc: 0.3587\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0907 Acc: 0.3725\n",
      "val Loss: 1.0917 Acc: 0.3827\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0870 Acc: 0.3904\n",
      "val Loss: 1.1109 Acc: 0.3507\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0847 Acc: 0.3984\n",
      "val Loss: 1.0915 Acc: 0.3817\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0798 Acc: 0.4010\n",
      "val Loss: 1.0931 Acc: 0.3810\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0760 Acc: 0.4095\n",
      "val Loss: 1.1028 Acc: 0.3780\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0686 Acc: 0.4181\n",
      "val Loss: 1.1033 Acc: 0.3597\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0611 Acc: 0.4360\n",
      "val Loss: 1.1026 Acc: 0.3727\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0451 Acc: 0.4498\n",
      "val Loss: 1.1259 Acc: 0.3590\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0207 Acc: 0.4839\n",
      "val Loss: 1.2864 Acc: 0.3433\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9905 Acc: 0.5108\n",
      "val Loss: 1.2282 Acc: 0.3610\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9376 Acc: 0.5532\n",
      "val Loss: 1.4017 Acc: 0.3703\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.8657 Acc: 0.6074\n",
      "val Loss: 1.2572 Acc: 0.3513\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.7637 Acc: 0.6658\n",
      "val Loss: 1.5825 Acc: 0.3520\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.6438 Acc: 0.7277\n",
      "val Loss: 1.8087 Acc: 0.3397\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.5204 Acc: 0.7895\n",
      "val Loss: 1.9002 Acc: 0.3427\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.3970 Acc: 0.8488\n",
      "val Loss: 2.4169 Acc: 0.3410\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.2795 Acc: 0.8954\n",
      "val Loss: 2.3711 Acc: 0.3443\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.1987 Acc: 0.9315\n",
      "val Loss: 2.4764 Acc: 0.3440\n",
      "\n",
      "Best val Acc: 0.382667\n",
      "Accuracy on test set - Baseline: 0.5853333333333334\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1057 Acc: 0.3497\n",
      "val Loss: 1.0915 Acc: 0.3777\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0935 Acc: 0.3727\n",
      "val Loss: 1.1165 Acc: 0.3540\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0922 Acc: 0.3720\n",
      "val Loss: 1.0965 Acc: 0.3710\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0871 Acc: 0.3902\n",
      "val Loss: 1.0951 Acc: 0.3573\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0855 Acc: 0.3910\n",
      "val Loss: 1.1006 Acc: 0.3377\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0803 Acc: 0.4118\n",
      "val Loss: 1.0997 Acc: 0.3523\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0769 Acc: 0.4027\n",
      "val Loss: 1.1347 Acc: 0.3427\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0697 Acc: 0.4202\n",
      "val Loss: 1.1446 Acc: 0.3380\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0603 Acc: 0.4274\n",
      "val Loss: 1.1089 Acc: 0.3560\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0447 Acc: 0.4565\n",
      "val Loss: 1.1215 Acc: 0.3723\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0217 Acc: 0.4766\n",
      "val Loss: 1.2632 Acc: 0.3613\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9909 Acc: 0.5070\n",
      "val Loss: 1.1744 Acc: 0.3557\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.5529\n",
      "val Loss: 1.2321 Acc: 0.3647\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.8638 Acc: 0.6052\n",
      "val Loss: 1.4222 Acc: 0.3257\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.7629 Acc: 0.6646\n",
      "val Loss: 1.6956 Acc: 0.3507\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.6447 Acc: 0.7312\n",
      "val Loss: 1.8399 Acc: 0.3537\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.5206 Acc: 0.7933\n",
      "val Loss: 1.8921 Acc: 0.3427\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.3826 Acc: 0.8530\n",
      "val Loss: 2.0400 Acc: 0.3387\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.2903 Acc: 0.8942\n",
      "val Loss: 2.4610 Acc: 0.3580\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.1987 Acc: 0.9297\n",
      "val Loss: 2.6419 Acc: 0.3473\n",
      "\n",
      "Best val Acc: 0.377667\n",
      "Accuracy on test set - Baseline: 0.5413333333333333\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1077 Acc: 0.3543\n",
      "val Loss: 1.1089 Acc: 0.3593\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0955 Acc: 0.3707\n",
      "val Loss: 1.0912 Acc: 0.3657\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0937 Acc: 0.3695\n",
      "val Loss: 1.0910 Acc: 0.3773\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0894 Acc: 0.3887\n",
      "val Loss: 1.0887 Acc: 0.3813\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0859 Acc: 0.3930\n",
      "val Loss: 1.1000 Acc: 0.3527\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0819 Acc: 0.3961\n",
      "val Loss: 1.0934 Acc: 0.3820\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0798 Acc: 0.4013\n",
      "val Loss: 1.0949 Acc: 0.3637\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0752 Acc: 0.4119\n",
      "val Loss: 1.0972 Acc: 0.3727\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0696 Acc: 0.4148\n",
      "val Loss: 1.1203 Acc: 0.3560\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0632 Acc: 0.4327\n",
      "val Loss: 1.1300 Acc: 0.3423\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0490 Acc: 0.4487\n",
      "val Loss: 1.1210 Acc: 0.3657\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0310 Acc: 0.4682\n",
      "val Loss: 1.1362 Acc: 0.3823\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0035 Acc: 0.4998\n",
      "val Loss: 1.1645 Acc: 0.3573\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9662 Acc: 0.5238\n",
      "val Loss: 1.2454 Acc: 0.3610\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9131 Acc: 0.5647\n",
      "val Loss: 1.2450 Acc: 0.3407\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8443 Acc: 0.6183\n",
      "val Loss: 2.2685 Acc: 0.3197\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7464 Acc: 0.6723\n",
      "val Loss: 1.7268 Acc: 0.3660\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.6370 Acc: 0.7306\n",
      "val Loss: 1.5357 Acc: 0.3550\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.5109 Acc: 0.7928\n",
      "val Loss: 2.2095 Acc: 0.3423\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.4040 Acc: 0.8424\n",
      "val Loss: 2.3254 Acc: 0.3547\n",
      "\n",
      "Best val Acc: 0.382333\n",
      "Accuracy on test set - Baseline: 0.5096666666666667\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1050 Acc: 0.3568\n",
      "val Loss: 1.1080 Acc: 0.3707\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0958 Acc: 0.3677\n",
      "val Loss: 1.1013 Acc: 0.3677\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0918 Acc: 0.3772\n",
      "val Loss: 1.1160 Acc: 0.3647\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0894 Acc: 0.3772\n",
      "val Loss: 1.1009 Acc: 0.3500\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0860 Acc: 0.3854\n",
      "val Loss: 1.0961 Acc: 0.3697\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0878 Acc: 0.3872\n",
      "val Loss: 1.0895 Acc: 0.3957\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0824 Acc: 0.3975\n",
      "val Loss: 1.1014 Acc: 0.3677\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0770 Acc: 0.4045\n",
      "val Loss: 1.1075 Acc: 0.3590\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0697 Acc: 0.4172\n",
      "val Loss: 1.1162 Acc: 0.3583\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0650 Acc: 0.4239\n",
      "val Loss: 1.1163 Acc: 0.3547\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0494 Acc: 0.4494\n",
      "val Loss: 1.1374 Acc: 0.3740\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0379 Acc: 0.4548\n",
      "val Loss: 1.1243 Acc: 0.3630\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0196 Acc: 0.4779\n",
      "val Loss: 1.1781 Acc: 0.3367\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9929 Acc: 0.5044\n",
      "val Loss: 1.1552 Acc: 0.3547\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.5362\n",
      "val Loss: 1.2579 Acc: 0.3447\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9094 Acc: 0.5640\n",
      "val Loss: 1.2361 Acc: 0.3347\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8512 Acc: 0.6007\n",
      "val Loss: 1.3297 Acc: 0.3437\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7824 Acc: 0.6479\n",
      "val Loss: 1.5955 Acc: 0.3520\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.7046 Acc: 0.6895\n",
      "val Loss: 2.1556 Acc: 0.3437\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6055 Acc: 0.7455\n",
      "val Loss: 2.0250 Acc: 0.3360\n",
      "\n",
      "Best val Acc: 0.395667\n",
      "Accuracy on test set - Baseline: 0.644\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1089 Acc: 0.3531\n",
      "val Loss: 1.1077 Acc: 0.3597\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0939 Acc: 0.3724\n",
      "val Loss: 1.0940 Acc: 0.3807\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0935 Acc: 0.3692\n",
      "val Loss: 1.0951 Acc: 0.3773\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0923 Acc: 0.3770\n",
      "val Loss: 1.0922 Acc: 0.3943\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0903 Acc: 0.3818\n",
      "val Loss: 1.0892 Acc: 0.3930\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0875 Acc: 0.3822\n",
      "val Loss: 1.0931 Acc: 0.3780\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0864 Acc: 0.3878\n",
      "val Loss: 1.0890 Acc: 0.3833\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0849 Acc: 0.3902\n",
      "val Loss: 1.1037 Acc: 0.3680\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0814 Acc: 0.3976\n",
      "val Loss: 1.0934 Acc: 0.3643\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0789 Acc: 0.4009\n",
      "val Loss: 1.0944 Acc: 0.3773\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0743 Acc: 0.4146\n",
      "val Loss: 1.0957 Acc: 0.3840\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0694 Acc: 0.4185\n",
      "val Loss: 1.1098 Acc: 0.3737\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0655 Acc: 0.4277\n",
      "val Loss: 1.1034 Acc: 0.3740\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0568 Acc: 0.4363\n",
      "val Loss: 1.1439 Acc: 0.3437\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0455 Acc: 0.4490\n",
      "val Loss: 1.1447 Acc: 0.3483\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0284 Acc: 0.4736\n",
      "val Loss: 1.1501 Acc: 0.3493\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0058 Acc: 0.4977\n",
      "val Loss: 1.1655 Acc: 0.3497\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.9731 Acc: 0.5271\n",
      "val Loss: 1.3012 Acc: 0.3527\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.9319 Acc: 0.5467\n",
      "val Loss: 1.2665 Acc: 0.3443\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.8800 Acc: 0.5850\n",
      "val Loss: 1.2947 Acc: 0.3397\n",
      "\n",
      "Best val Acc: 0.394333\n",
      "Accuracy on test set - Baseline: 0.6773333333333333\n",
      "\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset fashionmnist0.5 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "None\n",
      "..............................\n",
      "\n",
      "Training iteration: 0\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0706 Acc: 0.4456\n",
      "val Loss: 1.0437 Acc: 0.4794\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0522 Acc: 0.4722\n",
      "val Loss: 1.0526 Acc: 0.4672\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0493 Acc: 0.4744\n",
      "val Loss: 1.0544 Acc: 0.4692\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0464 Acc: 0.4789\n",
      "val Loss: 1.0479 Acc: 0.4789\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0450 Acc: 0.4791\n",
      "val Loss: 1.0456 Acc: 0.4850\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0401 Acc: 0.4833\n",
      "val Loss: 1.0397 Acc: 0.4828\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0390 Acc: 0.4849\n",
      "val Loss: 1.0506 Acc: 0.4708\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0327 Acc: 0.4870\n",
      "val Loss: 1.0422 Acc: 0.4811\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0271 Acc: 0.4899\n",
      "val Loss: 1.0846 Acc: 0.4639\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0217 Acc: 0.4951\n",
      "val Loss: 1.0557 Acc: 0.4569\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0103 Acc: 0.5041\n",
      "val Loss: 1.0780 Acc: 0.4519\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9942 Acc: 0.5149\n",
      "val Loss: 1.1141 Acc: 0.4422\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9727 Acc: 0.5240\n",
      "val Loss: 1.1002 Acc: 0.4531\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9401 Acc: 0.5509\n",
      "val Loss: 1.1420 Acc: 0.4239\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8916 Acc: 0.5820\n",
      "val Loss: 1.3083 Acc: 0.3889\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8461 Acc: 0.6101\n",
      "val Loss: 1.2394 Acc: 0.4036\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7793 Acc: 0.6496\n",
      "val Loss: 1.3269 Acc: 0.4222\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7135 Acc: 0.6819\n",
      "val Loss: 1.5690 Acc: 0.3783\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6380 Acc: 0.7166\n",
      "val Loss: 1.6436 Acc: 0.3992\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5797 Acc: 0.7497\n",
      "val Loss: 1.6481 Acc: 0.4136\n",
      "\n",
      "Best val Acc: 0.485000\n",
      "Accuracy on test set - Baseline: 0.9243333333333333\n",
      "\n",
      "\n",
      "Training iteration: 1\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0671 Acc: 0.4490\n",
      "val Loss: 1.0501 Acc: 0.4739\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0527 Acc: 0.4705\n",
      "val Loss: 1.0390 Acc: 0.4819\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0482 Acc: 0.4696\n",
      "val Loss: 1.0490 Acc: 0.4775\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0472 Acc: 0.4783\n",
      "val Loss: 1.0397 Acc: 0.4875\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0413 Acc: 0.4806\n",
      "val Loss: 1.0415 Acc: 0.4903\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0370 Acc: 0.4834\n",
      "val Loss: 1.0656 Acc: 0.4508\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0336 Acc: 0.4844\n",
      "val Loss: 1.0566 Acc: 0.4678\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0299 Acc: 0.4865\n",
      "val Loss: 1.0422 Acc: 0.4822\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0232 Acc: 0.4924\n",
      "val Loss: 1.0704 Acc: 0.4306\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0111 Acc: 0.4985\n",
      "val Loss: 1.0592 Acc: 0.4647\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0005 Acc: 0.5057\n",
      "val Loss: 1.0676 Acc: 0.4497\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9801 Acc: 0.5193\n",
      "val Loss: 1.1267 Acc: 0.4003\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9583 Acc: 0.5347\n",
      "val Loss: 1.1093 Acc: 0.4456\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9264 Acc: 0.5603\n",
      "val Loss: 1.1310 Acc: 0.4242\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8826 Acc: 0.5873\n",
      "val Loss: 1.2635 Acc: 0.4153\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8318 Acc: 0.6214\n",
      "val Loss: 1.2446 Acc: 0.4100\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7729 Acc: 0.6477\n",
      "val Loss: 1.3523 Acc: 0.3978\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7118 Acc: 0.6800\n",
      "val Loss: 1.4551 Acc: 0.4169\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6326 Acc: 0.7235\n",
      "val Loss: 1.8185 Acc: 0.3789\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5700 Acc: 0.7508\n",
      "val Loss: 1.8295 Acc: 0.4033\n",
      "\n",
      "Best val Acc: 0.490278\n",
      "Accuracy on test set - Baseline: 0.9176666666666666\n",
      "\n",
      "\n",
      "Training iteration: 2\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0702 Acc: 0.4474\n",
      "val Loss: 1.0467 Acc: 0.4753\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0538 Acc: 0.4699\n",
      "val Loss: 1.0573 Acc: 0.4689\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0489 Acc: 0.4722\n",
      "val Loss: 1.0681 Acc: 0.4222\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0465 Acc: 0.4769\n",
      "val Loss: 1.0454 Acc: 0.4781\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0454 Acc: 0.4803\n",
      "val Loss: 1.0405 Acc: 0.4828\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0407 Acc: 0.4836\n",
      "val Loss: 1.0585 Acc: 0.4622\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0405 Acc: 0.4803\n",
      "val Loss: 1.0423 Acc: 0.4875\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0360 Acc: 0.4831\n",
      "val Loss: 1.0438 Acc: 0.4761\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0291 Acc: 0.4865\n",
      "val Loss: 1.0626 Acc: 0.4483\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0255 Acc: 0.4869\n",
      "val Loss: 1.0640 Acc: 0.4669\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0173 Acc: 0.4965\n",
      "val Loss: 1.0487 Acc: 0.4733\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0078 Acc: 0.4999\n",
      "val Loss: 1.0677 Acc: 0.4644\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9885 Acc: 0.5128\n",
      "val Loss: 1.0885 Acc: 0.4456\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9678 Acc: 0.5243\n",
      "val Loss: 1.1114 Acc: 0.4431\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9366 Acc: 0.5494\n",
      "val Loss: 1.1127 Acc: 0.4331\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9001 Acc: 0.5697\n",
      "val Loss: 1.1277 Acc: 0.4508\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8484 Acc: 0.6000\n",
      "val Loss: 1.2838 Acc: 0.4183\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.8044 Acc: 0.6289\n",
      "val Loss: 1.3543 Acc: 0.4100\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.7257 Acc: 0.6722\n",
      "val Loss: 1.3287 Acc: 0.4067\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6645 Acc: 0.7045\n",
      "val Loss: 1.7499 Acc: 0.3869\n",
      "\n",
      "Best val Acc: 0.487500\n",
      "Accuracy on test set - Baseline: 0.9256666666666666\n",
      "\n",
      "\n",
      "Training iteration: 3\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0675 Acc: 0.4516\n",
      "val Loss: 1.0498 Acc: 0.4792\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0531 Acc: 0.4669\n",
      "val Loss: 1.0432 Acc: 0.4819\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0511 Acc: 0.4714\n",
      "val Loss: 1.0440 Acc: 0.4844\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0470 Acc: 0.4759\n",
      "val Loss: 1.0426 Acc: 0.4797\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0433 Acc: 0.4783\n",
      "val Loss: 1.0500 Acc: 0.4789\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0413 Acc: 0.4833\n",
      "val Loss: 1.0440 Acc: 0.4822\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0381 Acc: 0.4824\n",
      "val Loss: 1.0459 Acc: 0.4825\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0351 Acc: 0.4851\n",
      "val Loss: 1.0428 Acc: 0.4853\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0316 Acc: 0.4849\n",
      "val Loss: 1.0489 Acc: 0.4831\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0276 Acc: 0.4868\n",
      "val Loss: 1.0612 Acc: 0.4675\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0171 Acc: 0.4960\n",
      "val Loss: 1.0660 Acc: 0.4542\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0048 Acc: 0.5063\n",
      "val Loss: 1.0841 Acc: 0.4583\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9881 Acc: 0.5163\n",
      "val Loss: 1.0781 Acc: 0.4564\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9668 Acc: 0.5360\n",
      "val Loss: 1.1167 Acc: 0.4572\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9321 Acc: 0.5530\n",
      "val Loss: 1.1268 Acc: 0.4397\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8967 Acc: 0.5742\n",
      "val Loss: 1.1785 Acc: 0.4283\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8465 Acc: 0.6076\n",
      "val Loss: 1.2199 Acc: 0.4139\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7895 Acc: 0.6379\n",
      "val Loss: 1.4033 Acc: 0.4061\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.7358 Acc: 0.6646\n",
      "val Loss: 1.4901 Acc: 0.4147\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6729 Acc: 0.7035\n",
      "val Loss: 1.5622 Acc: 0.3961\n",
      "\n",
      "Best val Acc: 0.485278\n",
      "Accuracy on test set - Baseline: 0.8983333333333333\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0672 Acc: 0.4513\n",
      "val Loss: 1.0649 Acc: 0.4642\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0533 Acc: 0.4677\n",
      "val Loss: 1.0447 Acc: 0.4831\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0489 Acc: 0.4741\n",
      "val Loss: 1.0481 Acc: 0.4747\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0449 Acc: 0.4792\n",
      "val Loss: 1.0411 Acc: 0.4875\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0418 Acc: 0.4801\n",
      "val Loss: 1.0409 Acc: 0.4878\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0405 Acc: 0.4800\n",
      "val Loss: 1.0515 Acc: 0.4753\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0363 Acc: 0.4851\n",
      "val Loss: 1.0406 Acc: 0.4883\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0313 Acc: 0.4906\n",
      "val Loss: 1.0573 Acc: 0.4569\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0229 Acc: 0.4918\n",
      "val Loss: 1.0727 Acc: 0.4725\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0118 Acc: 0.5014\n",
      "val Loss: 1.0547 Acc: 0.4703\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.9977 Acc: 0.5060\n",
      "val Loss: 1.0796 Acc: 0.4456\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9738 Acc: 0.5277\n",
      "val Loss: 1.0824 Acc: 0.4369\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9426 Acc: 0.5485\n",
      "val Loss: 1.1537 Acc: 0.3917\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9016 Acc: 0.5707\n",
      "val Loss: 1.1758 Acc: 0.4044\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8412 Acc: 0.6098\n",
      "val Loss: 1.2804 Acc: 0.4336\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.7808 Acc: 0.6428\n",
      "val Loss: 1.3081 Acc: 0.4058\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7137 Acc: 0.6821\n",
      "val Loss: 1.6603 Acc: 0.3900\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.6402 Acc: 0.7160\n",
      "val Loss: 1.4851 Acc: 0.4181\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.5633 Acc: 0.7545\n",
      "val Loss: 1.9640 Acc: 0.4039\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.4995 Acc: 0.7898\n",
      "val Loss: 1.8587 Acc: 0.3842\n",
      "\n",
      "Best val Acc: 0.488333\n",
      "Accuracy on test set - Baseline: 0.93\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0668 Acc: 0.4514\n",
      "val Loss: 1.0522 Acc: 0.4694\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0515 Acc: 0.4685\n",
      "val Loss: 1.0454 Acc: 0.4822\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0491 Acc: 0.4759\n",
      "val Loss: 1.0453 Acc: 0.4828\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0448 Acc: 0.4764\n",
      "val Loss: 1.0465 Acc: 0.4828\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0440 Acc: 0.4826\n",
      "val Loss: 1.0452 Acc: 0.4864\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0414 Acc: 0.4812\n",
      "val Loss: 1.0491 Acc: 0.4761\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0380 Acc: 0.4819\n",
      "val Loss: 1.0466 Acc: 0.4806\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0331 Acc: 0.4881\n",
      "val Loss: 1.0474 Acc: 0.4825\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0294 Acc: 0.4872\n",
      "val Loss: 1.0465 Acc: 0.4828\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0223 Acc: 0.4925\n",
      "val Loss: 1.0755 Acc: 0.4417\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0182 Acc: 0.4960\n",
      "val Loss: 1.0652 Acc: 0.4597\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0045 Acc: 0.5046\n",
      "val Loss: 1.0912 Acc: 0.4461\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9872 Acc: 0.5228\n",
      "val Loss: 1.0911 Acc: 0.4594\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9613 Acc: 0.5371\n",
      "val Loss: 1.1593 Acc: 0.4567\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9271 Acc: 0.5574\n",
      "val Loss: 1.1623 Acc: 0.4197\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8803 Acc: 0.5872\n",
      "val Loss: 1.1968 Acc: 0.4267\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8228 Acc: 0.6205\n",
      "val Loss: 1.2482 Acc: 0.4000\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7540 Acc: 0.6597\n",
      "val Loss: 1.3334 Acc: 0.3864\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6894 Acc: 0.6976\n",
      "val Loss: 1.5029 Acc: 0.3886\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.7409\n",
      "val Loss: 1.6862 Acc: 0.3861\n",
      "\n",
      "Best val Acc: 0.486389\n",
      "Accuracy on test set - Baseline: 0.9056666666666666\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0685 Acc: 0.4467\n",
      "val Loss: 1.0610 Acc: 0.4564\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0519 Acc: 0.4709\n",
      "val Loss: 1.0484 Acc: 0.4744\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0485 Acc: 0.4750\n",
      "val Loss: 1.0615 Acc: 0.4414\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0439 Acc: 0.4788\n",
      "val Loss: 1.0455 Acc: 0.4847\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0434 Acc: 0.4817\n",
      "val Loss: 1.0447 Acc: 0.4825\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0395 Acc: 0.4816\n",
      "val Loss: 1.0438 Acc: 0.4858\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0355 Acc: 0.4874\n",
      "val Loss: 1.0451 Acc: 0.4789\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0300 Acc: 0.4858\n",
      "val Loss: 1.0563 Acc: 0.4658\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0257 Acc: 0.4894\n",
      "val Loss: 1.0506 Acc: 0.4744\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0157 Acc: 0.4947\n",
      "val Loss: 1.0688 Acc: 0.4717\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0018 Acc: 0.5063\n",
      "val Loss: 1.0743 Acc: 0.4506\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9858 Acc: 0.5151\n",
      "val Loss: 1.0812 Acc: 0.4439\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9628 Acc: 0.5324\n",
      "val Loss: 1.1194 Acc: 0.4325\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9296 Acc: 0.5565\n",
      "val Loss: 1.1434 Acc: 0.4200\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8890 Acc: 0.5768\n",
      "val Loss: 1.1987 Acc: 0.4169\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8371 Acc: 0.6117\n",
      "val Loss: 1.2986 Acc: 0.4025\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7866 Acc: 0.6365\n",
      "val Loss: 1.4200 Acc: 0.3989\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7228 Acc: 0.6730\n",
      "val Loss: 1.3938 Acc: 0.4133\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6593 Acc: 0.7058\n",
      "val Loss: 1.5120 Acc: 0.4117\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.7392\n",
      "val Loss: 1.7750 Acc: 0.3889\n",
      "\n",
      "Best val Acc: 0.485833\n",
      "Accuracy on test set - Baseline: 0.9223333333333333\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0692 Acc: 0.4474\n",
      "val Loss: 1.0458 Acc: 0.4772\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0551 Acc: 0.4676\n",
      "val Loss: 1.0564 Acc: 0.4469\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0492 Acc: 0.4765\n",
      "val Loss: 1.0472 Acc: 0.4794\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0485 Acc: 0.4741\n",
      "val Loss: 1.0596 Acc: 0.4497\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0457 Acc: 0.4754\n",
      "val Loss: 1.0381 Acc: 0.4906\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0421 Acc: 0.4786\n",
      "val Loss: 1.0447 Acc: 0.4817\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0397 Acc: 0.4847\n",
      "val Loss: 1.0364 Acc: 0.4911\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0352 Acc: 0.4847\n",
      "val Loss: 1.0538 Acc: 0.4778\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0315 Acc: 0.4876\n",
      "val Loss: 1.0444 Acc: 0.4847\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0267 Acc: 0.4881\n",
      "val Loss: 1.0493 Acc: 0.4872\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0179 Acc: 0.4959\n",
      "val Loss: 1.0606 Acc: 0.4586\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0089 Acc: 0.5001\n",
      "val Loss: 1.0656 Acc: 0.4464\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9958 Acc: 0.5100\n",
      "val Loss: 1.0710 Acc: 0.4569\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9762 Acc: 0.5248\n",
      "val Loss: 1.0905 Acc: 0.4489\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.5388\n",
      "val Loss: 1.1169 Acc: 0.4342\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9250 Acc: 0.5563\n",
      "val Loss: 1.1204 Acc: 0.4389\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8857 Acc: 0.5801\n",
      "val Loss: 1.2292 Acc: 0.4194\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.8479 Acc: 0.6030\n",
      "val Loss: 1.2025 Acc: 0.4167\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.7936 Acc: 0.6368\n",
      "val Loss: 1.4171 Acc: 0.4150\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.7541 Acc: 0.6588\n",
      "val Loss: 1.3173 Acc: 0.4247\n",
      "\n",
      "Best val Acc: 0.491111\n",
      "Accuracy on test set - Baseline: 0.923\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0688 Acc: 0.4421\n",
      "val Loss: 1.0543 Acc: 0.4706\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0526 Acc: 0.4701\n",
      "val Loss: 1.0467 Acc: 0.4792\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0475 Acc: 0.4751\n",
      "val Loss: 1.0440 Acc: 0.4775\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0470 Acc: 0.4792\n",
      "val Loss: 1.0392 Acc: 0.4861\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0423 Acc: 0.4819\n",
      "val Loss: 1.0429 Acc: 0.4872\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0420 Acc: 0.4790\n",
      "val Loss: 1.0609 Acc: 0.4794\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0343 Acc: 0.4858\n",
      "val Loss: 1.0429 Acc: 0.4844\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0311 Acc: 0.4898\n",
      "val Loss: 1.0467 Acc: 0.4756\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0249 Acc: 0.4921\n",
      "val Loss: 1.0456 Acc: 0.4789\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0184 Acc: 0.4982\n",
      "val Loss: 1.0477 Acc: 0.4750\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0069 Acc: 0.5035\n",
      "val Loss: 1.0694 Acc: 0.4697\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9864 Acc: 0.5194\n",
      "val Loss: 1.0775 Acc: 0.4636\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9597 Acc: 0.5385\n",
      "val Loss: 1.1126 Acc: 0.4519\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9284 Acc: 0.5566\n",
      "val Loss: 1.1620 Acc: 0.4583\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8849 Acc: 0.5847\n",
      "val Loss: 1.1917 Acc: 0.4217\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8253 Acc: 0.6218\n",
      "val Loss: 1.3022 Acc: 0.3933\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7643 Acc: 0.6499\n",
      "val Loss: 1.4879 Acc: 0.3964\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.6884 Acc: 0.6943\n",
      "val Loss: 1.5473 Acc: 0.4011\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.7299\n",
      "val Loss: 1.5376 Acc: 0.3875\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5486 Acc: 0.7638\n",
      "val Loss: 1.9232 Acc: 0.3958\n",
      "\n",
      "Best val Acc: 0.487222\n",
      "Accuracy on test set - Baseline: 0.903\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.0646 Acc: 0.4553\n",
      "val Loss: 1.0528 Acc: 0.4758\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0519 Acc: 0.4649\n",
      "val Loss: 1.0434 Acc: 0.4803\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0485 Acc: 0.4762\n",
      "val Loss: 1.0413 Acc: 0.4892\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0444 Acc: 0.4783\n",
      "val Loss: 1.0437 Acc: 0.4875\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0415 Acc: 0.4842\n",
      "val Loss: 1.0419 Acc: 0.4842\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0383 Acc: 0.4808\n",
      "val Loss: 1.0400 Acc: 0.4883\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0353 Acc: 0.4843\n",
      "val Loss: 1.0522 Acc: 0.4806\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0329 Acc: 0.4863\n",
      "val Loss: 1.0473 Acc: 0.4772\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0272 Acc: 0.4915\n",
      "val Loss: 1.0445 Acc: 0.4767\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0192 Acc: 0.4962\n",
      "val Loss: 1.0462 Acc: 0.4758\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0112 Acc: 0.4962\n",
      "val Loss: 1.0825 Acc: 0.4597\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.9960 Acc: 0.5094\n",
      "val Loss: 1.0789 Acc: 0.4564\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9818 Acc: 0.5203\n",
      "val Loss: 1.0834 Acc: 0.4544\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9535 Acc: 0.5377\n",
      "val Loss: 1.1282 Acc: 0.4478\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9239 Acc: 0.5553\n",
      "val Loss: 1.1217 Acc: 0.4533\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8825 Acc: 0.5818\n",
      "val Loss: 1.1746 Acc: 0.4250\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8395 Acc: 0.6115\n",
      "val Loss: 1.2291 Acc: 0.4192\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7790 Acc: 0.6460\n",
      "val Loss: 1.4001 Acc: 0.4217\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.7229 Acc: 0.6740\n",
      "val Loss: 1.4587 Acc: 0.3950\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6496 Acc: 0.7119\n",
      "val Loss: 1.5969 Acc: 0.3939\n",
      "\n",
      "Best val Acc: 0.489167\n",
      "Accuracy on test set - Baseline: 0.932\n",
      "\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset fashionmnist0.6 loaded.\n",
      "\n",
      "Shape Xtr: (18000, 1, 28, 28)\n",
      "Shape Str: (18000,)\n",
      "Shape Xts: (3000, 1, 28, 28)\n",
      "Shape Yts: (3000,)\n",
      "\n",
      "Transition Matrix: \n",
      "None\n",
      "..............................\n",
      "\n",
      "Training iteration: 0\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1030 Acc: 0.3611\n",
      "val Loss: 1.0996 Acc: 0.3656\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0938 Acc: 0.3848\n",
      "val Loss: 1.0951 Acc: 0.3844\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0945 Acc: 0.3832\n",
      "val Loss: 1.1020 Acc: 0.3506\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0925 Acc: 0.3874\n",
      "val Loss: 1.0966 Acc: 0.3706\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0920 Acc: 0.3891\n",
      "val Loss: 1.0942 Acc: 0.3817\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0905 Acc: 0.3906\n",
      "val Loss: 1.0987 Acc: 0.3694\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0887 Acc: 0.3925\n",
      "val Loss: 1.0971 Acc: 0.3767\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0868 Acc: 0.4008\n",
      "val Loss: 1.0957 Acc: 0.3667\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0837 Acc: 0.4006\n",
      "val Loss: 1.1006 Acc: 0.3592\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0802 Acc: 0.4098\n",
      "val Loss: 1.1016 Acc: 0.3647\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0745 Acc: 0.4200\n",
      "val Loss: 1.1122 Acc: 0.3542\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0634 Acc: 0.4354\n",
      "val Loss: 1.1199 Acc: 0.3433\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0513 Acc: 0.4485\n",
      "val Loss: 1.1319 Acc: 0.3767\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0287 Acc: 0.4674\n",
      "val Loss: 1.2084 Acc: 0.3472\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0051 Acc: 0.4920\n",
      "val Loss: 1.1590 Acc: 0.3472\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9629 Acc: 0.5210\n",
      "val Loss: 1.2605 Acc: 0.3494\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.9171 Acc: 0.5551\n",
      "val Loss: 1.2603 Acc: 0.3475\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.8662 Acc: 0.5928\n",
      "val Loss: 1.3192 Acc: 0.3483\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.7980 Acc: 0.6285\n",
      "val Loss: 1.4143 Acc: 0.3336\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.7310 Acc: 0.6693\n",
      "val Loss: 1.5912 Acc: 0.3536\n",
      "\n",
      "Best val Acc: 0.384444\n",
      "Accuracy on test set - Baseline: 0.7546666666666667\n",
      "\n",
      "\n",
      "Training iteration: 1\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1048 Acc: 0.3606\n",
      "val Loss: 1.1197 Acc: 0.3367\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0957 Acc: 0.3781\n",
      "val Loss: 1.0965 Acc: 0.3683\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0942 Acc: 0.3839\n",
      "val Loss: 1.0999 Acc: 0.3544\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0926 Acc: 0.3838\n",
      "val Loss: 1.0919 Acc: 0.3950\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0918 Acc: 0.3830\n",
      "val Loss: 1.0955 Acc: 0.3717\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0905 Acc: 0.3928\n",
      "val Loss: 1.0933 Acc: 0.3908\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0896 Acc: 0.3907\n",
      "val Loss: 1.0924 Acc: 0.3925\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0881 Acc: 0.3937\n",
      "val Loss: 1.0971 Acc: 0.3858\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0828 Acc: 0.4036\n",
      "val Loss: 1.1031 Acc: 0.3639\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0800 Acc: 0.4090\n",
      "val Loss: 1.0952 Acc: 0.3831\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0744 Acc: 0.4199\n",
      "val Loss: 1.1041 Acc: 0.3542\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0652 Acc: 0.4289\n",
      "val Loss: 1.1146 Acc: 0.3728\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0533 Acc: 0.4432\n",
      "val Loss: 1.1254 Acc: 0.3633\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0345 Acc: 0.4624\n",
      "val Loss: 1.1371 Acc: 0.3583\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0083 Acc: 0.4847\n",
      "val Loss: 1.1623 Acc: 0.3497\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9762 Acc: 0.5090\n",
      "val Loss: 1.2137 Acc: 0.3608\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.9302 Acc: 0.5412\n",
      "val Loss: 1.2531 Acc: 0.3489\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.8859 Acc: 0.5761\n",
      "val Loss: 1.3346 Acc: 0.3583\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.8236 Acc: 0.6123\n",
      "val Loss: 1.4863 Acc: 0.3494\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.7598 Acc: 0.6528\n",
      "val Loss: 1.4706 Acc: 0.3611\n",
      "\n",
      "Best val Acc: 0.395000\n",
      "Accuracy on test set - Baseline: 0.8636666666666667\n",
      "\n",
      "\n",
      "Training iteration: 2\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1028 Acc: 0.3665\n",
      "val Loss: 1.0952 Acc: 0.3639\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0963 Acc: 0.3746\n",
      "val Loss: 1.0976 Acc: 0.3633\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0932 Acc: 0.3881\n",
      "val Loss: 1.0956 Acc: 0.3858\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0910 Acc: 0.3890\n",
      "val Loss: 1.0959 Acc: 0.3783\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0897 Acc: 0.3931\n",
      "val Loss: 1.0946 Acc: 0.3819\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0873 Acc: 0.3972\n",
      "val Loss: 1.0953 Acc: 0.3758\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0839 Acc: 0.4036\n",
      "val Loss: 1.1019 Acc: 0.3744\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0791 Acc: 0.4076\n",
      "val Loss: 1.1025 Acc: 0.3700\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0706 Acc: 0.4194\n",
      "val Loss: 1.1097 Acc: 0.3581\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0609 Acc: 0.4363\n",
      "val Loss: 1.1072 Acc: 0.3639\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0415 Acc: 0.4576\n",
      "val Loss: 1.1267 Acc: 0.3539\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0204 Acc: 0.4776\n",
      "val Loss: 1.1325 Acc: 0.3583\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9885 Acc: 0.5015\n",
      "val Loss: 1.1629 Acc: 0.3600\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9484 Acc: 0.5262\n",
      "val Loss: 1.2146 Acc: 0.3586\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9047 Acc: 0.5622\n",
      "val Loss: 1.2971 Acc: 0.3539\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8435 Acc: 0.6022\n",
      "val Loss: 1.4644 Acc: 0.3458\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7734 Acc: 0.6454\n",
      "val Loss: 1.4780 Acc: 0.3450\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7048 Acc: 0.6804\n",
      "val Loss: 1.6136 Acc: 0.3439\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.7265\n",
      "val Loss: 1.8993 Acc: 0.3414\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5709 Acc: 0.7524\n",
      "val Loss: 1.9865 Acc: 0.3317\n",
      "\n",
      "Best val Acc: 0.385833\n",
      "Accuracy on test set - Baseline: 0.8213333333333334\n",
      "\n",
      "\n",
      "Training iteration: 3\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1022 Acc: 0.3700\n",
      "val Loss: 1.1046 Acc: 0.3500\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0962 Acc: 0.3760\n",
      "val Loss: 1.1009 Acc: 0.3483\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0939 Acc: 0.3829\n",
      "val Loss: 1.0980 Acc: 0.3736\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0930 Acc: 0.3855\n",
      "val Loss: 1.0942 Acc: 0.3947\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0906 Acc: 0.3885\n",
      "val Loss: 1.1029 Acc: 0.3714\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0892 Acc: 0.3903\n",
      "val Loss: 1.0988 Acc: 0.3606\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0864 Acc: 0.3930\n",
      "val Loss: 1.0931 Acc: 0.3817\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0840 Acc: 0.4027\n",
      "val Loss: 1.0969 Acc: 0.3781\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0814 Acc: 0.4047\n",
      "val Loss: 1.0965 Acc: 0.3886\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0756 Acc: 0.4106\n",
      "val Loss: 1.1062 Acc: 0.3717\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0671 Acc: 0.4259\n",
      "val Loss: 1.1042 Acc: 0.3825\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0557 Acc: 0.4394\n",
      "val Loss: 1.1113 Acc: 0.3706\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0396 Acc: 0.4614\n",
      "val Loss: 1.1354 Acc: 0.3594\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0146 Acc: 0.4821\n",
      "val Loss: 1.1580 Acc: 0.3628\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9819 Acc: 0.5067\n",
      "val Loss: 1.2584 Acc: 0.3450\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9400 Acc: 0.5348\n",
      "val Loss: 1.2849 Acc: 0.3383\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8801 Acc: 0.5772\n",
      "val Loss: 1.3125 Acc: 0.3531\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.8267 Acc: 0.6096\n",
      "val Loss: 1.3335 Acc: 0.3606\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.7517 Acc: 0.6542\n",
      "val Loss: 1.6314 Acc: 0.3531\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6732 Acc: 0.6969\n",
      "val Loss: 1.9208 Acc: 0.3497\n",
      "\n",
      "Best val Acc: 0.394722\n",
      "Accuracy on test set - Baseline: 0.8466666666666667\n",
      "\n",
      "\n",
      "Training iteration: 4\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1039 Acc: 0.3613\n",
      "val Loss: 1.0955 Acc: 0.3711\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0947 Acc: 0.3792\n",
      "val Loss: 1.0961 Acc: 0.3714\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0920 Acc: 0.3835\n",
      "val Loss: 1.0921 Acc: 0.3844\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0915 Acc: 0.3866\n",
      "val Loss: 1.0965 Acc: 0.3700\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0896 Acc: 0.3947\n",
      "val Loss: 1.0979 Acc: 0.3831\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0879 Acc: 0.3924\n",
      "val Loss: 1.1057 Acc: 0.3828\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0829 Acc: 0.3988\n",
      "val Loss: 1.0975 Acc: 0.3742\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0789 Acc: 0.4128\n",
      "val Loss: 1.1000 Acc: 0.3792\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0733 Acc: 0.4156\n",
      "val Loss: 1.1134 Acc: 0.3594\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0595 Acc: 0.4369\n",
      "val Loss: 1.1049 Acc: 0.3714\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0430 Acc: 0.4578\n",
      "val Loss: 1.1322 Acc: 0.3553\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0243 Acc: 0.4681\n",
      "val Loss: 1.1869 Acc: 0.3372\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9920 Acc: 0.5015\n",
      "val Loss: 1.1760 Acc: 0.3411\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.5348\n",
      "val Loss: 1.3328 Acc: 0.3528\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9007 Acc: 0.5657\n",
      "val Loss: 1.3430 Acc: 0.3389\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8371 Acc: 0.6004\n",
      "val Loss: 1.5097 Acc: 0.3419\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7699 Acc: 0.6467\n",
      "val Loss: 1.4164 Acc: 0.3492\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7055 Acc: 0.6831\n",
      "val Loss: 1.6385 Acc: 0.3469\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6408 Acc: 0.7144\n",
      "val Loss: 1.7852 Acc: 0.3528\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5639 Acc: 0.7562\n",
      "val Loss: 1.8328 Acc: 0.3494\n",
      "\n",
      "Best val Acc: 0.384444\n",
      "Accuracy on test set - Baseline: 0.8293333333333334\n",
      "\n",
      "\n",
      "Training iteration: 5\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1027 Acc: 0.3716\n",
      "val Loss: 1.0967 Acc: 0.3692\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0949 Acc: 0.3762\n",
      "val Loss: 1.0982 Acc: 0.3869\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0947 Acc: 0.3819\n",
      "val Loss: 1.0935 Acc: 0.3714\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0915 Acc: 0.3901\n",
      "val Loss: 1.0970 Acc: 0.3653\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0913 Acc: 0.3891\n",
      "val Loss: 1.0932 Acc: 0.3822\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0880 Acc: 0.3962\n",
      "val Loss: 1.0954 Acc: 0.3700\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0850 Acc: 0.3988\n",
      "val Loss: 1.0970 Acc: 0.3861\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0835 Acc: 0.4032\n",
      "val Loss: 1.1009 Acc: 0.3689\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0808 Acc: 0.4066\n",
      "val Loss: 1.0993 Acc: 0.3800\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0734 Acc: 0.4178\n",
      "val Loss: 1.1062 Acc: 0.3725\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0651 Acc: 0.4238\n",
      "val Loss: 1.1083 Acc: 0.3819\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0547 Acc: 0.4379\n",
      "val Loss: 1.1170 Acc: 0.3797\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0422 Acc: 0.4449\n",
      "val Loss: 1.1385 Acc: 0.3456\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0290 Acc: 0.4550\n",
      "val Loss: 1.1374 Acc: 0.3706\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0056 Acc: 0.4769\n",
      "val Loss: 1.1862 Acc: 0.3628\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.9799 Acc: 0.4969\n",
      "val Loss: 1.2060 Acc: 0.3661\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.9525 Acc: 0.5185\n",
      "val Loss: 1.2860 Acc: 0.3722\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.9276 Acc: 0.5331\n",
      "val Loss: 1.2134 Acc: 0.3544\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.8862 Acc: 0.5628\n",
      "val Loss: 1.3793 Acc: 0.3583\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.8461 Acc: 0.5873\n",
      "val Loss: 1.4859 Acc: 0.3519\n",
      "\n",
      "Best val Acc: 0.386944\n",
      "Accuracy on test set - Baseline: 0.782\n",
      "\n",
      "\n",
      "Training iteration: 6\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1033 Acc: 0.3599\n",
      "val Loss: 1.0948 Acc: 0.3917\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0958 Acc: 0.3760\n",
      "val Loss: 1.0949 Acc: 0.3742\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0926 Acc: 0.3883\n",
      "val Loss: 1.0984 Acc: 0.3603\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0898 Acc: 0.3881\n",
      "val Loss: 1.0900 Acc: 0.4008\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0888 Acc: 0.3928\n",
      "val Loss: 1.1033 Acc: 0.3481\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0857 Acc: 0.3976\n",
      "val Loss: 1.1042 Acc: 0.3658\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0825 Acc: 0.3958\n",
      "val Loss: 1.0973 Acc: 0.3708\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0770 Acc: 0.4138\n",
      "val Loss: 1.1164 Acc: 0.3606\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0688 Acc: 0.4248\n",
      "val Loss: 1.1243 Acc: 0.3575\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0590 Acc: 0.4378\n",
      "val Loss: 1.1145 Acc: 0.3656\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0416 Acc: 0.4608\n",
      "val Loss: 1.1371 Acc: 0.3689\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0173 Acc: 0.4831\n",
      "val Loss: 1.1733 Acc: 0.3353\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9801 Acc: 0.5122\n",
      "val Loss: 1.1964 Acc: 0.3553\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9396 Acc: 0.5451\n",
      "val Loss: 1.2079 Acc: 0.3639\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8760 Acc: 0.5857\n",
      "val Loss: 1.4570 Acc: 0.3444\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8035 Acc: 0.6268\n",
      "val Loss: 1.6633 Acc: 0.3483\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7281 Acc: 0.6737\n",
      "val Loss: 1.8073 Acc: 0.3431\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.6494 Acc: 0.7122\n",
      "val Loss: 1.6042 Acc: 0.3503\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.5724 Acc: 0.7513\n",
      "val Loss: 2.0308 Acc: 0.3394\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5001 Acc: 0.7872\n",
      "val Loss: 1.9417 Acc: 0.3369\n",
      "\n",
      "Best val Acc: 0.400833\n",
      "Accuracy on test set - Baseline: 0.8973333333333333\n",
      "\n",
      "\n",
      "Training iteration: 7\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1025 Acc: 0.3596\n",
      "val Loss: 1.0956 Acc: 0.3831\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0954 Acc: 0.3770\n",
      "val Loss: 1.0950 Acc: 0.3808\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0930 Acc: 0.3835\n",
      "val Loss: 1.0934 Acc: 0.3861\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0918 Acc: 0.3847\n",
      "val Loss: 1.1011 Acc: 0.3783\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0891 Acc: 0.3919\n",
      "val Loss: 1.0963 Acc: 0.3750\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0870 Acc: 0.3962\n",
      "val Loss: 1.1003 Acc: 0.3803\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0845 Acc: 0.3995\n",
      "val Loss: 1.0985 Acc: 0.3867\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0823 Acc: 0.4048\n",
      "val Loss: 1.0938 Acc: 0.3731\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0763 Acc: 0.4120\n",
      "val Loss: 1.1072 Acc: 0.3756\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0696 Acc: 0.4229\n",
      "val Loss: 1.1030 Acc: 0.3636\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0536 Acc: 0.4402\n",
      "val Loss: 1.1160 Acc: 0.3806\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0360 Acc: 0.4663\n",
      "val Loss: 1.1524 Acc: 0.3553\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0120 Acc: 0.4801\n",
      "val Loss: 1.1626 Acc: 0.3542\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9743 Acc: 0.5108\n",
      "val Loss: 1.2115 Acc: 0.3608\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.9358 Acc: 0.5410\n",
      "val Loss: 1.2080 Acc: 0.3575\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8816 Acc: 0.5763\n",
      "val Loss: 1.5510 Acc: 0.3422\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8191 Acc: 0.6207\n",
      "val Loss: 1.4705 Acc: 0.3442\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.7550 Acc: 0.6552\n",
      "val Loss: 1.5676 Acc: 0.3489\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6828 Acc: 0.6957\n",
      "val Loss: 1.6300 Acc: 0.3486\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6101 Acc: 0.7344\n",
      "val Loss: 1.7565 Acc: 0.3625\n",
      "\n",
      "Best val Acc: 0.386667\n",
      "Accuracy on test set - Baseline: 0.763\n",
      "\n",
      "\n",
      "Training iteration: 8\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1042 Acc: 0.3615\n",
      "val Loss: 1.0938 Acc: 0.3903\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0948 Acc: 0.3808\n",
      "val Loss: 1.0960 Acc: 0.3778\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0934 Acc: 0.3785\n",
      "val Loss: 1.0946 Acc: 0.3817\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0934 Acc: 0.3815\n",
      "val Loss: 1.0930 Acc: 0.3956\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0931 Acc: 0.3876\n",
      "val Loss: 1.0917 Acc: 0.3911\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0911 Acc: 0.3887\n",
      "val Loss: 1.0962 Acc: 0.3600\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0897 Acc: 0.3941\n",
      "val Loss: 1.0923 Acc: 0.3939\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0903 Acc: 0.3895\n",
      "val Loss: 1.0922 Acc: 0.3994\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0879 Acc: 0.3972\n",
      "val Loss: 1.0941 Acc: 0.3831\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0866 Acc: 0.3946\n",
      "val Loss: 1.0995 Acc: 0.3919\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0834 Acc: 0.4040\n",
      "val Loss: 1.0980 Acc: 0.3731\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0788 Acc: 0.4099\n",
      "val Loss: 1.0992 Acc: 0.3853\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0736 Acc: 0.4205\n",
      "val Loss: 1.1182 Acc: 0.3600\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0658 Acc: 0.4283\n",
      "val Loss: 1.1102 Acc: 0.3725\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0518 Acc: 0.4431\n",
      "val Loss: 1.1122 Acc: 0.3636\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0309 Acc: 0.4653\n",
      "val Loss: 1.1838 Acc: 0.3650\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0106 Acc: 0.4840\n",
      "val Loss: 1.1310 Acc: 0.3800\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.9712 Acc: 0.5122\n",
      "val Loss: 1.1937 Acc: 0.3589\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.9208 Acc: 0.5542\n",
      "val Loss: 1.2585 Acc: 0.3511\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.8626 Acc: 0.5859\n",
      "val Loss: 1.3643 Acc: 0.3467\n",
      "\n",
      "Best val Acc: 0.399444\n",
      "Accuracy on test set - Baseline: 0.841\n",
      "\n",
      "\n",
      "Training iteration: 9\n",
      "Validation Split: 0.2\n",
      "Transition Matrix: \n",
      "None\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.1032 Acc: 0.3616\n",
      "val Loss: 1.0974 Acc: 0.3697\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0946 Acc: 0.3796\n",
      "val Loss: 1.0934 Acc: 0.3939\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0926 Acc: 0.3848\n",
      "val Loss: 1.1023 Acc: 0.3617\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0909 Acc: 0.3887\n",
      "val Loss: 1.0975 Acc: 0.3761\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0872 Acc: 0.3977\n",
      "val Loss: 1.1002 Acc: 0.3789\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0855 Acc: 0.3991\n",
      "val Loss: 1.0974 Acc: 0.3839\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0817 Acc: 0.4076\n",
      "val Loss: 1.1073 Acc: 0.3567\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.0777 Acc: 0.4062\n",
      "val Loss: 1.1081 Acc: 0.3625\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0659 Acc: 0.4239\n",
      "val Loss: 1.1081 Acc: 0.3800\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.0521 Acc: 0.4432\n",
      "val Loss: 1.1256 Acc: 0.3683\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0326 Acc: 0.4623\n",
      "val Loss: 1.1252 Acc: 0.3786\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0057 Acc: 0.4868\n",
      "val Loss: 1.1777 Acc: 0.3414\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.9625 Acc: 0.5280\n",
      "val Loss: 1.2547 Acc: 0.3450\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.9093 Acc: 0.5650\n",
      "val Loss: 1.2451 Acc: 0.3492\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8460 Acc: 0.6038\n",
      "val Loss: 1.4213 Acc: 0.3533\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.7736 Acc: 0.6457\n",
      "val Loss: 1.4545 Acc: 0.3436\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.6868 Acc: 0.6951\n",
      "val Loss: 1.6737 Acc: 0.3536\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.6055 Acc: 0.7355\n",
      "val Loss: 1.7771 Acc: 0.3472\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.5294 Acc: 0.7742\n",
      "val Loss: 1.9630 Acc: 0.3478\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.4651 Acc: 0.8060\n",
      "val Loss: 2.0799 Acc: 0.3547\n",
      "\n",
      "Best val Acc: 0.393889\n",
      "Accuracy on test set - Baseline: 0.836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "available_datasets = {'cifar': '/content/drive/My Drive/comp5328-assignment-2/data/CIFAR.npz',\n",
    "                      'fashionmnist0.5': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.5.npz',\n",
    "                      'fashionmnist0.6': '/content/drive/My Drive/comp5328-assignment-2/data/FashionMNIST0.6.npz'}\n",
    "\n",
    "# Training the model n_iters\n",
    "n_iters = 10\n",
    "\n",
    "# Set seed\n",
    "seed_torch()\n",
    "\n",
    "test_acc_baseline_results = {'cifar': [],\n",
    "                             'fashionmnist0.5': [],\n",
    "                             'fashionmnist0.6': []}\n",
    "# Iterate over each dataset\n",
    "for d in list(available_datasets.keys()):\n",
    "  dataset = d\n",
    "\n",
    "  # Load dataset\n",
    "  if dataset.lower() in available_datasets:\n",
    "    dir = available_datasets[dataset.lower()]\n",
    "    # In this case we do not want to use the transition matrix\n",
    "    # because we want to get a baseline results\n",
    "    T_matrix = None\n",
    "    X_train, y_train, X_test, y_test = load_data(dir)\n",
    "    print('-'*60)\n",
    "    print('-'*60)\n",
    "    print(f\"\\nDataset {dataset} loaded.\\n\")\n",
    "\n",
    "    print(f\"Shape Xtr: {X_train.shape}\")\n",
    "    print(f\"Shape Str: {y_train.shape}\")\n",
    "    print(f\"Shape Xts: {X_test.shape}\")\n",
    "    print(f\"Shape Yts: {y_test.shape}\")\n",
    "\n",
    "    print(f\"\\nTransition Matrix: \\n{T_matrix}\")\n",
    "    print('.'*30)\n",
    "  else:\n",
    "    print(\"Dataset not valid. The dataset available are: cifar, fashionmnist0.5 and fashionmnist0.6\")\n",
    "\n",
    "  # Detect if GPU or CPU\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "      \n",
    "  # Define parameters\n",
    "  if dataset.lower() == 'cifar':\n",
    "    n_channels = 3\n",
    "    n_filters = 8\n",
    "  else:\n",
    "    n_channels = 1\n",
    "    n_filters = 7\n",
    "\n",
    "  num_classes = 3\n",
    "  batch_size = 100\n",
    "  num_epochs = 20\n",
    "  learning_rate = 0.001\n",
    "\n",
    "  for n in range(n_iters):\n",
    "    print(f\"\\nTraining iteration: {n}\")\n",
    "\n",
    "    # Clean cache each iteration\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # We did not set a seed for train_test_split thus, it would generate different samples each iteration\n",
    "    train_loader, val_loader, test_loader = get_loader(X_train=X_train, \n",
    "                                                    y_train=y_train, \n",
    "                                                    X_test=X_test, \n",
    "                                                    y_test=y_test, \n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "\n",
    "    # Initialize model\n",
    "    model_modified_resnet = ResNet(ResidualBlock, [2, 2, 2], \n",
    "                                    num_channels=n_channels, \n",
    "                                    num_filter=n_filters, \n",
    "                                    num_classes=num_classes).to(device)\n",
    "\n",
    "    # If GPU: send the model to GPU\n",
    "    model_modified_resnet = model_modified_resnet.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model_modified_resnet.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define Loss Function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train model\n",
    "    model_modified_resnet, hist = train_model(model_modified_resnet, \n",
    "                                              dataloaders_dict, \n",
    "                                              criterion, \n",
    "                                              optimizer, \n",
    "                                              num_epochs=num_epochs, \n",
    "                                              T=None)\n",
    "    \n",
    "    # Generate predictions on the test set\n",
    "    y_true, y_pred, acc, outputs = prediction(model_modified_resnet, test_loader, device)\n",
    "\n",
    "    test_acc_baseline_results[dataset].append(acc)\n",
    "    print(f\"Accuracy on test set - Baseline: {acc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1605622494248,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "lChszx9WGEnY",
    "outputId": "058ee5f1-2538-4876-a106-4712cddebe25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifar</th>\n",
       "      <th>fashionmnist0.5</th>\n",
       "      <th>fashionmnist0.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.522667</td>\n",
       "      <td>0.924333</td>\n",
       "      <td>0.754667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.593667</td>\n",
       "      <td>0.917667</td>\n",
       "      <td>0.863667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.925667</td>\n",
       "      <td>0.821333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.512333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.846667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.497667</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.829333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.585333</td>\n",
       "      <td>0.905667</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.541333</td>\n",
       "      <td>0.922333</td>\n",
       "      <td>0.897333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.509667</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.677333</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.836000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cifar  fashionmnist0.5  fashionmnist0.6\n",
       "0  0.522667         0.924333         0.754667\n",
       "1  0.593667         0.917667         0.863667\n",
       "2  0.583000         0.925667         0.821333\n",
       "3  0.512333         0.898333         0.846667\n",
       "4  0.497667         0.930000         0.829333\n",
       "5  0.585333         0.905667         0.782000\n",
       "6  0.541333         0.922333         0.897333\n",
       "7  0.509667         0.923000         0.763000\n",
       "8  0.644000         0.903000         0.841000\n",
       "9  0.677333         0.932000         0.836000"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy values\n",
    "test_acc_baseline_results_backup = test_acc_baseline_results \n",
    "\n",
    "# Create dataframe from dictionary\n",
    "df = pd.DataFrame(test_acc_baseline_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQAaoSeeIhyD"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "import pickle\n",
    "with open('test_acc_baseline_results.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_acc_baseline_results, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5cMqyQ6Ik6w"
   },
   "outputs": [],
   "source": [
    "with open('test_acc_baseline_results.pkl', 'rb') as fp:\n",
    "    test_load = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehB38nKJIoXP"
   },
   "source": [
    "#### Combining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmbCgVKnO4uY"
   },
   "source": [
    "**Cifar10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zvkY_B0QMGu"
   },
   "outputs": [],
   "source": [
    "cifar_baseline = test_acc_baseline_results['cifar']\n",
    "cifar_forward = test_acc_forward_results['cifar']\n",
    "cifar_trevision = test_acc_trevision_results['cifar']\n",
    "cifar_impreweighting = test_acc_impreweighting_results['cifar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L59us7HtFM6u"
   },
   "outputs": [],
   "source": [
    "# Calculate mean and std\n",
    "cifar_mean = [np.mean(cifar_baseline), np.mean(cifar_forward), np.mean(cifar_impreweighting), np.mean(cifar_trevision)]\n",
    "cifar_std = [np.std(cifar_baseline),np.std(cifar_forward), np.std(cifar_impreweighting), np.std(cifar_trevision)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1605622495560,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "ohZjBPh2Eqv1",
    "outputId": "a4849a9b-fee0-45e8-8fe6-01b96edd464d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4), dpi=200)\n",
    "plt.title('Average Accuracy and Standard Deviation - Cifar10')\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.errorbar(['Baseline', 'Forward', 'Importance Reweighting', 'T-Revision'], \n",
    "             cifar_mean, \n",
    "             cifar_std, \n",
    "             ecolor=[\"red\", \"orange\", \"orange\", \"orange\"], \n",
    "             linestyle='None', \n",
    "             marker='^')\n",
    "plt.grid(alpha=0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8dYgGBgPA7V"
   },
   "source": [
    "**FashionMNIST0.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEYs68-fO_5M"
   },
   "outputs": [],
   "source": [
    "# Get values\n",
    "fashion05_baseline = test_acc_baseline_results['fashionmnist0.5']\n",
    "fashion05_forward = test_acc_forward_results['fashionmnist0.5']\n",
    "fashion05_trevision = test_acc_trevision_results['fashionmnist0.5']\n",
    "fashion05_impreweighting = test_acc_impreweighting_results['fashionmnist0.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLt1K9ssO_5U"
   },
   "outputs": [],
   "source": [
    "# Calculate mean and std\n",
    "fashion05_mean = [np.mean(fashion05_baseline), np.mean(fashion05_forward), np.mean(fashion05_impreweighting), np.mean(fashion05_trevision)]\n",
    "fashion05_std = [np.std(fashion05_baseline),np.std(fashion05_forward), np.std(fashion05_impreweighting), np.std(fashion05_trevision)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1605622495562,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "jzp3lb4EO_5a",
    "outputId": "02eb18ca-266d-4e9d-9606-1a1be2c814db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4), dpi=200)\n",
    "plt.title('Average Accuracy and Standard Deviation - FashionMNIST0.5')\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.errorbar(['Baseline', 'Forward', 'Importance Reweighting', 'T-Revision'], \n",
    "             fashion05_mean, \n",
    "             fashion05_std, \n",
    "             ecolor=[\"red\", \"orange\", \"orange\", \"orange\"], \n",
    "             linestyle='None', \n",
    "             marker='^')\n",
    "plt.grid(alpha=0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cS-rf-f0PqPH"
   },
   "source": [
    "**FashionMNIST0.6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMKkMBEEPqPI"
   },
   "outputs": [],
   "source": [
    "# Get values\n",
    "fashion06_baseline = test_acc_baseline_results['fashionmnist0.6']\n",
    "fashion06_forward = test_acc_forward_results['fashionmnist0.6']\n",
    "fashion06_trevision = test_acc_trevision_results['fashionmnist0.6']\n",
    "fashion06_impreweighting = test_acc_impreweighting_results['fashionmnist0.6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_Qv3QnbPqPN"
   },
   "outputs": [],
   "source": [
    "# Calculate mean and std\n",
    "fashion06_mean = [np.mean(fashion06_baseline), np.mean(fashion06_forward), np.mean(fashion06_impreweighting), np.mean(fashion06_trevision)]\n",
    "fashion06_std = [np.std(fashion06_baseline),np.std(fashion06_forward), np.std(fashion06_impreweighting), np.std(fashion06_trevision)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "executionInfo": {
     "elapsed": 1845,
     "status": "ok",
     "timestamp": 1605622497390,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "umMQCf0XPqPU",
    "outputId": "b8722967-5786-4edc-b753-def3dd4746ea"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4), dpi=200)\n",
    "plt.title('Average Accuracy and Standard Deviation - FashionMNIST0.6')\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.errorbar(['Baseline', 'Forward', 'Importance Reweighting', 'T-Revision'], \n",
    "             fashion06_mean, \n",
    "             fashion06_std, \n",
    "             ecolor=[\"red\", \"orange\", \"orange\", \"orange\"], \n",
    "             linestyle='None', \n",
    "             marker='^')\n",
    "plt.grid(alpha=0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qAgvDZiDRUA"
   },
   "source": [
    "## 5.- Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM79KV0NDYnJ"
   },
   "source": [
    "This section contains code used to generate graphs for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1605622497396,
     "user": {
      "displayName": "Alejandro Diaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhKUBA8AamxrqLNO9Bfr3yJL6_7GLp-EZQ-JUeQGUk=s64",
      "userId": "13299696405271868031"
     },
     "user_tz": -600
    },
    "id": "hmeIUh7UlolK",
    "outputId": "2ec5868f-fc26-441e-ca66-75007f068c40"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "flip = np.random.choice([True, False], len(y), p=[0.05, 0.95])\n",
    "y_flip = np.random.choice([0,1,2], len(y), p=[0.4, 0.3, 0.3])\n",
    "z = np.copy(y)\n",
    "z[flip] = y_flip[flip]\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "plt.figure(2, figsize=(8, 4))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the true points\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n",
    "            edgecolor='k')\n",
    "plt.xlabel('W1')\n",
    "plt.ylabel('W2')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# Plot the noisy points\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=z, cmap=plt.cm.Set1,edgecolor='k')\n",
    "plt.scatter(X[flip, 0], X[flip, 1], s=90, facecolors='none', edgecolors='black')\n",
    "plt.xlabel('W1') \n",
    "plt.ylabel('W2')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Analysis_of_classifiers_robust_to_noisy_labels.ipynb",
   "provenance": [
    {
     "file_id": "1ZSm2-A5OCbyYhTebIlEVZTVmi_7YSQhD",
     "timestamp": 1603786544949
    },
    {
     "file_id": "1w1w8j-oJFbiZsc0agPr0MV7_GdfnAcFc",
     "timestamp": 1603287545326
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
